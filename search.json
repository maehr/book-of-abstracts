[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Security"
    ]
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "",
    "section": "Reporting a Vulnerability",
    "text": "Reporting a Vulnerability\n\nTo report a security issue, please email digital-history-2024@unibas.ch with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Abstracts",
      "About",
      "Security"
    ]
  },
  {
    "objectID": "fonts/LICENSE-OFL.html",
    "href": "fonts/LICENSE-OFL.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nCopyright 2020 The Jost Project Authors (https://github.com/indestructible-type/Jost)\nThis Font Software is licensed under the SIL Open Font License, Version 1.1. This license is available with a FAQ at: https://scripts.sil.org/OFL\n\nSIL OPEN FONT LICENSE\nVersion 1.1 - 26 February 2007\nPREAMBLE The goals of the Open Font License (OFL) are to stimulate worldwide development of collaborative font projects, to support the font creation efforts of academic and linguistic communities, and to provide a free and open framework in which fonts may be shared and improved in partnership with others.\nThe OFL allows the licensed fonts to be used, studied, modified and redistributed freely as long as they are not sold by themselves. The fonts, including any derivative works, can be bundled, embedded, redistributed and/or sold with any software provided that any reserved names are not used by derivative works. The fonts and derivatives, however, cannot be released under any other type of license. The requirement for fonts to remain under this license does not apply to any document created using the fonts or their derivatives.\nDEFINITIONS “Font Software” refers to the set of files released by the Copyright Holder(s) under this license and clearly marked as such. This may include source files, build scripts and documentation.\n“Original Version” refers to the collection of Font Software components as distributed by the Copyright Holder(s).\n“Modified Version” refers to any derivative made by adding to, deleting, or substituting — in part or in whole — any of the components of the Original Version, by changing formats or by porting the Font Software to a new environment.\n“Author” refers to any designer, engineer, programmer, technical writer or other person who contributed to the Font Software.\nPERMISSION & CONDITIONS Permission is hereby granted, free of charge, to any person obtaining a copy of the Font Software, to use, study, copy, merge, embed, modify, redistribute, and sell modified and unmodified copies of the Font Software, subject to the following conditions:\n\nNeither the Font Software nor any of its individual components, in Original or Modified Versions, may be sold by itself.\nOriginal or Modified Versions of the Font Software may be bundled, redistributed and/or sold with any software, provided that each copy contains the above copyright notice and this license. These can be included either as stand-alone text files, human-readable headers or in the appropriate machine-readable metadata fields within text or binary files as long as those fields can be easily viewed by the user.\nThe name(s) of the Copyright Holder(s) or the Author(s) of the Font Software shall not be used to promote, endorse or advertise any Modified Version, except to acknowledge the contribution(s) of the Copyright Holder(s) and the Author(s) or with their explicit written permission.\nThe Font Software, modified or unmodified, in part or in whole, must be distributed entirely under this license, and must not be distributed under any other license. The requirement for fonts to remain under this license does not apply to any document created using the Font Software.\n\nTERMINATION This license becomes null and void if any of the above conditions are not met.\nDISCLAIMER THE FONT SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, INCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM OTHER DEALINGS IN THE FONT SOFTWARE.\n\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "submissions/poster/472/index.html",
    "href": "submissions/poster/472/index.html",
    "title": "Discuss Data – an Open Repository for Research and Data Communities",
    "section": "",
    "text": "In this poster, we show how the Discuss Data research data platform is being expanded to include a “community space” for the digital humanities (DH). Discuss Data enables and promotes contextualized discussion about the quality and sustainability of research data directly on the object.\nCurrent standards and the digitization of existing processes require structures to enable sustainable development models. This applies in particular to the quality of research data, which is becoming increasingly important in the academic debate.\nDiscuss Data offers a platform for this. In addition to the information technology management, archiving and provision of data, Discuss Data also contextualizes data through curated discussion. The platform addresses individual communities and offers them a subject-specific discussion space and, in the long term, community-specific tools. Communities are not to be equated with disciplines, but are rather interest groups on specific issues or data materials.\nFollowing the introduction of the first community space for the research community on Eastern Europe, South Caucasus and Central Asia in 2020, 121 datasets were published and 141 users have registered (as of 28.11.23). However, the discussion function provided by Discuss Data has been used comparatively little so far. This discussion culture, which is quite common at conferences and reviews and is extremely important from a technical perspective, has not yet become established, despite the positive attitude towards it.\nDigital method and source criticism has become one of the central challenges of the digital humanities. Until now, research data has generally been published on institutional repositories or platforms such as Zenodo, but without the kind of quality control that is customary for journal articles. As a result, datasets often remain unused for further processing because it remains unclear what quality the research data has and what it might be suitable for.\nFrom the experience of the first funding phase of Discuss Data, it has become clear that more energy must be put into attracting data curators in order to ensure that the community spaces are supported by the community in the long term. Positive examples are needed for this. For example, the integration of discussions as micropublications could help to demonstrate the individual added value.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@misc{kahlert2024,\n  author = {Kahlert, Torsten and Kurzawe, Daniel},\n  editor = {Baudry, Jérôme and Burkart, Lucas and Joyeux-Prunel,\n    Béatrice and Kurmann, Eliane and Mähr, Moritz and Natale, Enrico and\n    Sibille, Christiane and Twente, Moritz},\n  title = {Discuss {Data} -\\/- an {Open} {Repository} for {Research} and\n    {Data} {Communities}},\n  date = {2024-08-28},\n  url = {https://digihistch24.github.io/book-of-abstracts/submissions/poster/472/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKahlert, Torsten, and Daniel Kurzawe. 2024. “Discuss Data -- an\nOpen Repository for Research and Data Communities.” Edited by\nJérôme Baudry, Lucas Burkart, Béatrice Joyeux-Prunel, Eliane Kurmann,\nMoritz Mähr, Enrico Natale, Christiane Sibille, and Moritz Twente.\nDigital History Switzerland 2024: Book of Abstracts. https://digihistch24.github.io/book-of-abstracts/submissions/poster/472/."
  },
  {
    "objectID": "submissions/poster/484/index.html",
    "href": "submissions/poster/484/index.html",
    "title": "Swiss Google Books for Research",
    "section": "",
    "text": "The UB Bern, ZHB Lucerne, ZB Zurich and UB Basel are digitizing a large part of their holdings from the 18th and 19th centuries in collaboration with Google Books. This digital collection, which is accessible in full text, is intended to offer new possibilities for digital and data-driven research and teaching, e.g. in the context of text and data mining and distant reading.\nDue to its size (90 million pages), the collection offers many opportunities, but also presents libraries and researchers with new challenges. Google’s algorithms are responsible for image processing, book composition and full-text recognition. Continuous data improvement/changes must therefore be expected when changed algorithms deliver new data versions. This helps to continuously improve quality, but represents a black box that makes it complicated to make transparent statements about the data production processes.\nThe four partner libraries are currently working on a project (“Google Books for Research”):\n\nResearch and teaching requirements for large digital historical text collections\nState of the art solutions for research-orientated accessibility of large historical text collections\nData quality and enrichment\nInfrastructure solutions\n\nThe central question is how libraries, as cultural and memory institutions, can offer relatively generic infrastructure in the digital space and keep it stable while still being able to use it flexibly enough for very specific research questions and methods.\nAs part of the poster session, we will present the results of the preliminary project and would like to explore these further with the audience.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@misc{reisacher2024,\n  author = {Reisacher, Martin and Dubey, Eric},\n  editor = {Baudry, Jérôme and Burkart, Lucas and Joyeux-Prunel,\n    Béatrice and Kurmann, Eliane and Mähr, Moritz and Natale, Enrico and\n    Sibille, Christiane and Twente, Moritz},\n  title = {Swiss {Google} {Books} for {Research}},\n  date = {2024-08-29},\n  url = {https://digihistch24.github.io/book-of-abstracts/submissions/poster/484/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nReisacher, Martin, and Eric Dubey. 2024. “Swiss Google Books for\nResearch.” Edited by Jérôme Baudry, Lucas Burkart, Béatrice\nJoyeux-Prunel, Eliane Kurmann, Moritz Mähr, Enrico Natale, Christiane\nSibille, and Moritz Twente. Digital History Switzerland 2024: Book\nof Abstracts. https://digihistch24.github.io/book-of-abstracts/submissions/poster/484/."
  },
  {
    "objectID": "submissions/poster/440/index.html",
    "href": "submissions/poster/440/index.html",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "",
    "text": "Access to archival records has been an integral part of the mission of the Swiss Federal Archives (SFA)1 since the founding of the Helvetic Republic in 1798, and represents a commitment to preserving and providing access to the administrative records of federal authorities such as the government, parliament and the administration. As custodian of Switzerland’s historical documentation, the SFA plays a vital role in facilitating access to these records for researchers and the public. In response to the constant evolution of digital technologies and the emergence of artificial intelligence (AI), the SFA has adopted innovative approaches to improve the accessibility and usability of its extensive holdings.\nOne significant advancement is the application of Automated Handwritten Text Recognition (AHTR) on the website housing the Minutes of the Federal Council (1848-1972)2 (Hodel et al. 2021). This initiative exemplifies the SFA’s commitment to leveraging AI to improve access and searchability of historical documents, enabling researchers to explore and analyse these crucial records more efficiently. Moreover, the SFA has collaborated with the Linguistic Research Infrastructure (LiRI) of the University of Zurich on a natural language processing (NLP)-based anonymiser for archival metadata3. This collaboration underscores the SFA’s proactive approach to improving privacy measures while facilitating broader access and use of records.\nSimilar efforts in other archives have demonstrated the efficacy of Machine Learning (ML), Linked Open Data (LOD), and community-led application programming interfaces (APIs) like the International Image Interoperability Framework (IIIF) in enhancing archival accessibility and research capabilities. LOD facilitates the interconnectedness of datasets, allowing seamless integration and discovery across archival collections. Meanwhile, IIIF standardises the delivery of image-based content, enabling sophisticated visual analysis and comparison of digital objects (Cornut, Raemy, and Spiess 2023).\nIn 2022, the SFA furthered its digital transformation efforts by implementing Archipanion, powered by vitrivr, a sophisticated content-based multimedia information retrieval system (Spiess and Stauffiger 2023). This strategic decision aims to revolutionise access to the SFA’s digital film collection, “Filmwochenschau (1940-1975)”. By integrating AI-driven technologies, Archipanion enhances the discoverability of over 200 hours of historical videos, previously indexed using traditional methods.\nThe structure of this paper begins with an overview of the Schweizer Filmwochenschau and the rationale behind digitising and improving access to this collection through AI technologies. The technological framework of vitrivr and Archipanion is then described, their capabilities and the integration process at the SFA. It then examines the implications of AI for archival research, focusing on the opportunities and challenges presented by predictive capabilities and the need for rigorous validation. Finally, it concludes with perspectives on ongoing efforts at the SFA to optimise AI tools, emphasising improved access to archival materials. This initiative extends beyond the GLAM (Galleries, Libraries, Archives, and Museums) sector to encompass methodological approaches that cater to the needs of Digital Humanities (DH) and Data Science practitioners."
  },
  {
    "objectID": "submissions/poster/440/index.html#introduction",
    "href": "submissions/poster/440/index.html#introduction",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "",
    "text": "Access to archival records has been an integral part of the mission of the Swiss Federal Archives (SFA)1 since the founding of the Helvetic Republic in 1798, and represents a commitment to preserving and providing access to the administrative records of federal authorities such as the government, parliament and the administration. As custodian of Switzerland’s historical documentation, the SFA plays a vital role in facilitating access to these records for researchers and the public. In response to the constant evolution of digital technologies and the emergence of artificial intelligence (AI), the SFA has adopted innovative approaches to improve the accessibility and usability of its extensive holdings.\nOne significant advancement is the application of Automated Handwritten Text Recognition (AHTR) on the website housing the Minutes of the Federal Council (1848-1972)2 (Hodel et al. 2021). This initiative exemplifies the SFA’s commitment to leveraging AI to improve access and searchability of historical documents, enabling researchers to explore and analyse these crucial records more efficiently. Moreover, the SFA has collaborated with the Linguistic Research Infrastructure (LiRI) of the University of Zurich on a natural language processing (NLP)-based anonymiser for archival metadata3. This collaboration underscores the SFA’s proactive approach to improving privacy measures while facilitating broader access and use of records.\nSimilar efforts in other archives have demonstrated the efficacy of Machine Learning (ML), Linked Open Data (LOD), and community-led application programming interfaces (APIs) like the International Image Interoperability Framework (IIIF) in enhancing archival accessibility and research capabilities. LOD facilitates the interconnectedness of datasets, allowing seamless integration and discovery across archival collections. Meanwhile, IIIF standardises the delivery of image-based content, enabling sophisticated visual analysis and comparison of digital objects (Cornut, Raemy, and Spiess 2023).\nIn 2022, the SFA furthered its digital transformation efforts by implementing Archipanion, powered by vitrivr, a sophisticated content-based multimedia information retrieval system (Spiess and Stauffiger 2023). This strategic decision aims to revolutionise access to the SFA’s digital film collection, “Filmwochenschau (1940-1975)”. By integrating AI-driven technologies, Archipanion enhances the discoverability of over 200 hours of historical videos, previously indexed using traditional methods.\nThe structure of this paper begins with an overview of the Schweizer Filmwochenschau and the rationale behind digitising and improving access to this collection through AI technologies. The technological framework of vitrivr and Archipanion is then described, their capabilities and the integration process at the SFA. It then examines the implications of AI for archival research, focusing on the opportunities and challenges presented by predictive capabilities and the need for rigorous validation. Finally, it concludes with perspectives on ongoing efforts at the SFA to optimise AI tools, emphasising improved access to archival materials. This initiative extends beyond the GLAM (Galleries, Libraries, Archives, and Museums) sector to encompass methodological approaches that cater to the needs of Digital Humanities (DH) and Data Science practitioners."
  },
  {
    "objectID": "submissions/poster/440/index.html#schweizer-filmwochenschau",
    "href": "submissions/poster/440/index.html#schweizer-filmwochenschau",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "Schweizer Filmwochenschau",
    "text": "Schweizer Filmwochenschau\nThe Schweizer Filmwochenschau was shown every week in the supporting programme of Swiss cinemas between 1940 and 1975 in the three national languages German, French and Italian. The programmes were commissioned by the Federal Council and featured news from the worlds of politics, culture, social and sport. In 2015, the Swiss Film Archive (Cinémathèque suisse)4, Memoriav5 and the SFA made the films available in the three languages as part of a joint project6. The digitisation made it possible to preserve analogue films consisting of 35mm nitrate and acetate elements, positives and negatives in digital formats, as well as to provide a new access. The digitised film collection J2.143*#20 with 1,651 editions and a total running time of approximately 200 hours is part of the Federal Archives’ online access platform7. Depending on their access authorisation, users can search for a variety of metatada and primary data according to their access authorisation or download the films directly via video streaming. Like all other formats, this film collection is described according to the International Standard Archival Description and indexed down to document level in the archive tree.\nGiven the rapidly growing volume of analogue and digital born archival content8, in-depth exploration and indexing conducted solely by humans is hardly conceivable for the SFA. In this context, the SFA see automation and AI as resource-efficient ways of complementing traditional archiving work and offering innovative services to users. The new Archipanion Filmwochenschau platform9 extends the conventional search methods to include content-based discovery in the Schweizer Filmwochenschau. Archipanion is a service provided by 4eyes GmbH10 and is implemented on top of vitrivr11 (Sauter et al. 2024)."
  },
  {
    "objectID": "submissions/poster/440/index.html#the-deployment-of-vitrivr-and-archipanion",
    "href": "submissions/poster/440/index.html#the-deployment-of-vitrivr-and-archipanion",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "The deployment of vitrivr and Archipanion",
    "text": "The deployment of vitrivr and Archipanion\nVitrivr is an open-source retrieval stack that supports multiple query modes for searching multimedia collections (Rossetto et al. 2016). It has been developed by the Database and Information Systems Group (DBIS) at the University of Basel, which notably carries out research in the field of multimedia information retrieval. The system enables the automated search and access to multimedia data by combining various ML techniques to analyse video, image, and audio files. Unlike conventional multimedia search solutions, it is not limited to metadata browsing, but is able to search for and retrieve multimedia content based on similarity to a user’s query. It excels by leveraging “object detection method for search filtering and annotation, a co-embedding method used for content-based text-to-image and image-to-image retrieval” (Cornut, Raemy, and Spiess 2023), as well as Contrastive Language-Image Pre-Training (CLIP), a versatile model that bridges the gap between computer vision and NLP, enabling seamless search and retrieval of multimedia content based on semantic similarity.\nAt its core, vitrivr transforms both images and textual content into a unified semantic space, enabling efficient query processing regardless of the input type. Each query generates a unified vector in a multidimensional space where similarity metrics determine the most relevant results (Spiess et al. 2022). The effectiveness of this model depends on the quality of the training data. In particular, vitrivr uses a carefully selected dataset for its text co-embedding model, including English-only datasets such as Flickr30k (31,000 images of everyday scenes with captions), Microsoft COCO (over one million images focused on object recognition), MSRVTT (10,000 video clips with linguistic annotations linking video content to words), TextCaps and TGIF (specialising in understanding complex captions and animated GIFs), VaTeX (41,250 multilingual video clips enhancing the language capabilities of our AI) and ImageNet (categorising around 50 million images based on the hierarchical structure provided by WordNet). The pretrained visual and textual feature encoders remain fixed, significantly reducing the required training resources. During training, only the visual and textual embedding networks were trained on a mixture of captioned video and image datasets. These datasets ensure the model’s training is thorough and enhances retrieval accuracy (Waltenpül 2023).\nArchipanion uses elements of vitrivr to revolutionise access and content-based search within multimedia content for GLAM. Its web user interface supports multiple query types in a multilingual environment (German, French, Italian and English) and automates the enhancement of search content effortlessly. Archipanion harnesses various AI technologies, including the DeepL API12, to translate queries into English because the text co-embedding model is not multilingual. This translation ensures that queries in different languages are understood and processed effectively, delivering pertinent results.\nWith a wide range of query modes, Archipanion facilitates scene searches by allowing users to enter descriptive queries, such as ‘a football player lying in front of the goal’, to locate relevant images from the Schweizer Filmwochenschau. Text search capabilities allow the discovery of specific content within films, such as the name of an airline (as seen in Figure 1) or a place name sign, through automatic text recognition. In addition, Archipanion supports exploration through similarity searches based on displayed images, helping users to discover previously unseen relationships within collections. Each search result offers the option to download sequences for use and re-use in different contexts, alongside a link to the object in the online archival arrangement.\n\n\n\n\n\n\nText search for “Swissair” on the website Schweizerisches Bundesarchiv Filmwochenschauen.\n\n\n\n\nFigure 1\n\n\n\nThe integration of vitrivr and Archipanion has significantly improved accessibility to the Filmwochenschau archive. Previously indexed using conventional methods with limited content utilisation, the collection is now effortlessly searchable through AI-driven, content-based processing. This technological advancement converts digitised information into actionable knowledge, providing historians and humanities researchers unprecedented access to this collection, facilitating research and exploration anytime, anywhere."
  },
  {
    "objectID": "submissions/poster/440/index.html#perspectives",
    "href": "submissions/poster/440/index.html#perspectives",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "Perspectives",
    "text": "Perspectives\nAI introduces both opportunities and challenges in research. Archipanion, specialised in content-based multimedia retrieval, exemplifies this transformation. It empowers researchers by automatically sifting through vast datasets, extracting specific content details, and uncovering connections that would be difficult or impractical for human analysis alone. However, the outcomes of these AI models provide a simplified representation of reality, enabling understanding of complex relationships and predictive capabilities within the multidimensional vector space.\nDespite the transformative promise of predictive capabilities in AI, their effectiveness depends on fundamental factors such as the quality of training data, the accuracy of feature extraction methods, and the appropriateness of similarity metrics. Tools such as vitrivr have demonstrated progress in retrieval tasks, yet there remains a critical need for rigorous research to fully understand AI mechanisms and validate their practical applications. Human expertise remains essential in interpreting and contextualising AI-generated insights to ensure that query results are not only useful but also meaningful in advancing research agendas.\nThe integration of AI into archival research presents both innovative methodologies and potential limitations, as highlighted by Jaillant and Aske (2023). Their research underscores the importance of critical engagement with AI tools, and advocates for interdisciplinary collaboration to effectively harness the capabilities of AI while navigating its inherent biases and limitations. They argue that while AI facilitates improved access to and analysis of archival materials, including textual and visual content, its use requires ongoing scrutiny and refinement to ensure scholarly integrity and accuracy. This perspective calls for an ongoing dialogue between AI practitioners, humanities scholars, and archival professionals to optimise AI technologies.\nCollaborative efforts are essential not only to refine AI applications in archival settings, but also to maximise their societal benefits and relevance. It is crucial to address various risks associated with AI, such as potential biases in algorithmic decision-making, privacy and security issues, and challenges in maintaining the long-term accessibility and usability of digital archives."
  },
  {
    "objectID": "submissions/poster/440/index.html#conclusion",
    "href": "submissions/poster/440/index.html#conclusion",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "Conclusion",
    "text": "Conclusion\nUltimately, the SFA remains committed to improving the accessibility and usability of its archival collections for both users and staff. The implementation of Archipanion represents a significant leap forward in improving access to the Filmwochenschau and other digital holdings. By harnessing AI technologies, the SFA not only facilitates the exploration and retrieval of historical materials, but also provides researchers with advanced tools for in-depth analysis and discovery. Looking ahead, the SFA continues to explore innovative ways to enhance search capabilities and ensure that its vast archival resources remain accessible and relevant in the digital age. Collaborative efforts are essential not only to refine AI applications in archival settings but also to amplify their societal benefits and relevance, thus preserving historical records and advancing scholarly research."
  },
  {
    "objectID": "submissions/poster/440/index.html#references",
    "href": "submissions/poster/440/index.html#references",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "References",
    "text": "References\n\n\nCornut, Murielle, Julien Antoine Raemy, and Florian Spiess. 2023. “Annotations as Knowledge Practices in Image Archives: Application of Linked Open Usable Data and Machine Learning.” Journal on Computing and Cultural Heritage, Applying Innovative Technologies to Digitised and Born-Digital Archives, 16 (4): 1–19. https://doi.org/10.1145/3625301.\n\n\nHodel, Tobias, David Schoch, Christa Schneider, and Jake Purcell. 2021. “General Models for Handwritten Text Recognition: Feasibility and State-of-the Art. German Kurrent as an Example.” Journal of Open Humanities Data 7 (July): 13. https://doi.org/10.5334/johd.46.\n\n\nJaillant, Lise, and Katherine Aske. 2023. “Are Users of Digital Archives Ready for the AI Era? Obstacles to the Application of Computational Research Methods and New Opportunities.” Journal on Computing and Cultural Heritage 16 (4): 1–16. https://doi.org/10.1145/3631125.\n\n\nRossetto, Luca, Ivan Giangreco, Claudiu Tanase, and Heiko Schuldt. 2016. “Vitrivr: A Flexible Retrieval Stack Supporting Multiple Query Modes for Searching in Multimedia Collections.” In Proceedings of the 24th ACM International Conference on Multimedia, 1183–86. Amsterdam The Netherlands: ACM. https://doi.org/10.1145/2964284.2973797.\n\n\nSauter, Loris, Ralph Gasser, Laura Rettig, Heiko Schuldt, and Luca Rossetto. 2024. “General Purpose Multimedia Retrieval with Vitrivr at LSC’24.” In Proceedings of the 7th Annual ACM Workshop on the Lifelog Search Challenge, 47–52. Phuket Thailand: ACM. https://doi.org/10.1145/3643489.3661120.\n\n\nSpiess, Florian, Ralph Gasser, Silvan Heller, Mahnaz Parian-Scherb, Luca Rossetto, Loris Sauter, and Heiko Schuldt. 2022. “Multi-Modal Video Retrieval in Virtual Reality with Vitrivr-VR.” In MultiMedia Modeling, edited by Björn Þór Jónsson, Cathal Gurrin, Minh-Triet Tran, Duc-Tien Dang-Nguyen, Anita Min-Chun Hu, Binh Huynh Thi Thanh, and Benoit Huet, 13142:499–504. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-98355-0_45.\n\n\nSpiess, Florian, and Markus Stauffiger. 2023. “Forschung Und Archive: Erschliessung Und Zugänglichkeit Neu Gedacht.” Arbido 2023 (1). https://arbido.ch/de/ausgaben-artikel/2023/archiv-der-zukunft/forschung-und-archive-erschliessung-und-zugaenglichkeit-neu-gedacht.\n\n\nWaltenpül, Raphael. 2023. “Summary of Visual-Text Co Embedding Models.” Archipanion Project. Basel, Switzerland: University of Basel."
  },
  {
    "objectID": "submissions/poster/440/index.html#footnotes",
    "href": "submissions/poster/440/index.html#footnotes",
    "title": "Multimodal UI for Video Retrieval at the Swiss Federal Archives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSwiss Federal Archives: https://www.bar.admin.ch/↩︎\nMinutes of the Federal Council (1848-1972): https://www.chgov.bar.admin.ch/↩︎\ntcc-metadata-anonymization: https://github.com/SwissFederalArchives/tcc-metadata-anonymization↩︎\nNational Film Archive: https://www.cinematheque.ch/↩︎\nMemoriav: https://memoriav.ch↩︎\nFilmbestand Schweizer Filmwochenschau (1940-1975): https://memoriav.ch/de/projects/schweizer-filmwochenschau-1940-1975/↩︎\nSchweizer Filmwochenschau, 1940-1975 (Series): https://www.recherche.bar.admin.ch/recherche/link/en/archive/unit/21677483↩︎\nSwiss Federal Archives’ facts and figures: https://www.bar.admin.ch/bar/en/home/about-us/the-federal-archives/facts-and-figures.html↩︎\nSchweizerisches Bundesarchiv Filmwochenschauen: https://wochenschau.archipanion.com/↩︎\n4eyes: https://www.4eyes.ch/↩︎\nvitrivr: https://vitrivr.org/↩︎\nDeepL API: https://developers.deepl.com/docs↩︎"
  },
  {
    "objectID": "submissions/457/index.html",
    "href": "submissions/457/index.html",
    "title": "Towards Computational Historiographical Modeling",
    "section": "",
    "text": "When we look for epistemological differences between “traditional” and digital history, corpora—stand out. Of course, historians have always created and studied collections of traces, in particular documents, but sometimes also other artifacts, and have built their narratives on the basis of these collections. This is a significant aspect of scholarship and in some sense constitutes the difference between historical and literary narratives: historical narratives are supposed to be grounded (in some way) in the historical facts represented by the respective corpus.\nNevertheless, the relation between such a corpus and the narrative is traditionally rather unclear. Not only is the corpus necessarily incomplete (and uncertain), but it’s typically only “virtual.” As Mayaffre (2006, 20) puts it, in the humanities corpora traditionally tend to be potentialities rather than realities: one could go and consult a certain document in some archive, but this may only be rarely done, and the corpus may thus have never been anything but an “intellectual object.” \nMachine-readable digital corpora—that is, what we mean by corpora today—have brought about major changes. Most of the time, it is their practical advantages that are highlighted: they are easier to store, they are (at least potentially) accessible from anywhere at any time, and they can be processed automatically. This, in turn, enables us to apply new types of analysis and thus to ask and study new research questions. What tends to be overlooked, though, is the epistemological impact of machine-readable corpora in history. The notion of corpus in digital history (and in digital humanities in general) is heavily influenced by the notion of corpus in computational linguistics: a large but finite collection of digital texts. Mayaffre (2006, 20) hints at the epistemological impact when he notes that, on the one hand, digitization dematerializes the text in that it is lifted from its previous support, but on the other hand, materializes the corpus more rigorously than before.\nThis is, of course, a precondition for more rigorous types of analysis, notably computational analyses, and—eventually—the construction of computational historical models. However, this raises a number of epistemological and methodological questions. In computational linguistics, a corpus is essentially considered a statistical sample of language. Historical corpora typically differ from linguistic corpora, both in its relation to the research objects, the research questions, and to the expected research findings. They also differ in the way they are constructed.\nWhile there is much discussion of individual methods and their appropriateness—and many common definitions as well as a large part of the criticism of DH are related to these methods—there is surprisingly little theoretical discussion of corpora. In a typical DH paper (or project proposal), just a few words are said about the corpus that was used, and most of it tends to concern its size and composition (n items of class X, m items of class Y, and so on) and the technical aspects of its construction (e.g., how it was scraped), if the authors did not use an existing corpus. The methods (algorithms, tools, etc.) used and the results achieved (and their interpretation and visualization) are typically discussed extensively, though.\nGiven the central role of corpora in digital history, I think we need to study them and the roles they play in order to avoid the production of research that is formally rigorous but historically meaningless (or even nonsensical).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#introduction",
    "href": "submissions/457/index.html#introduction",
    "title": "Towards Computational Historiographical Modeling",
    "section": "",
    "text": "When we look for epistemological differences between “traditional” and digital history, corpora—stand out. Of course, historians have always created and studied collections of traces, in particular documents, but sometimes also other artifacts, and have built their narratives on the basis of these collections. This is a significant aspect of scholarship and in some sense constitutes the difference between historical and literary narratives: historical narratives are supposed to be grounded (in some way) in the historical facts represented by the respective corpus.\nNevertheless, the relation between such a corpus and the narrative is traditionally rather unclear. Not only is the corpus necessarily incomplete (and uncertain), but it’s typically only “virtual.” As Mayaffre (2006, 20) puts it, in the humanities corpora traditionally tend to be potentialities rather than realities: one could go and consult a certain document in some archive, but this may only be rarely done, and the corpus may thus have never been anything but an “intellectual object.” \nMachine-readable digital corpora—that is, what we mean by corpora today—have brought about major changes. Most of the time, it is their practical advantages that are highlighted: they are easier to store, they are (at least potentially) accessible from anywhere at any time, and they can be processed automatically. This, in turn, enables us to apply new types of analysis and thus to ask and study new research questions. What tends to be overlooked, though, is the epistemological impact of machine-readable corpora in history. The notion of corpus in digital history (and in digital humanities in general) is heavily influenced by the notion of corpus in computational linguistics: a large but finite collection of digital texts. Mayaffre (2006, 20) hints at the epistemological impact when he notes that, on the one hand, digitization dematerializes the text in that it is lifted from its previous support, but on the other hand, materializes the corpus more rigorously than before.\nThis is, of course, a precondition for more rigorous types of analysis, notably computational analyses, and—eventually—the construction of computational historical models. However, this raises a number of epistemological and methodological questions. In computational linguistics, a corpus is essentially considered a statistical sample of language. Historical corpora typically differ from linguistic corpora, both in its relation to the research objects, the research questions, and to the expected research findings. They also differ in the way they are constructed.\nWhile there is much discussion of individual methods and their appropriateness—and many common definitions as well as a large part of the criticism of DH are related to these methods—there is surprisingly little theoretical discussion of corpora. In a typical DH paper (or project proposal), just a few words are said about the corpus that was used, and most of it tends to concern its size and composition (n items of class X, m items of class Y, and so on) and the technical aspects of its construction (e.g., how it was scraped), if the authors did not use an existing corpus. The methods (algorithms, tools, etc.) used and the results achieved (and their interpretation and visualization) are typically discussed extensively, though.\nGiven the central role of corpora in digital history, I think we need to study them and the roles they play in order to avoid the production of research that is formally rigorous but historically meaningless (or even nonsensical).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#corpora-as-models",
    "href": "submissions/457/index.html#corpora-as-models",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Corpora as Models",
    "text": "Corpora as Models\nAs Granger (1967) notes, the goal of any science (natural or other) is to build coherent and effective models of the phenomena they study.\nThus, and as I have argued before (Piotrowski 2019), a corpus should be considered a model in the sense of Leo Apostel, who asserted that “any subject using a system A that is neither directly nor indirectly interacting with a system B to obtain information about the system B, is using A as a model for B” (Apostel 1961, 36, emphasis in original). Creating a corpus thus means constructing a model, and modelers consequently have to answer questions such as: What is it that I am trying to model? In what respects is the model a reduction of it? And for whom and for what purpose am I creating the model?\nThese are not new questions: every time historians select sources, they construct models, even before any detailed analysis. However, machine-readable corpora are not only potentially much larger than any material collection of sources—which is already not inconsequential—but also have important epistemological consequences. The larger and the more “complete” a corpus is, the greater the danger to succumb to an “implicit essentialism” (Mothon 2010, 19) and to mistake the model for the original, a fallacy that can frequently be observed in the field of cultoromics (Michel et al. 2011), when arguments are being made on the basis of the Google Books Ngram Corpus.\nThe same then goes for any analysis of a corpus: if the corpus is “true,” so must be the results of the analysis; if there is no evidence of something in the corpus, it did not exist. This allure is even greater when the analysis is done automatically and in particular using opaque quantitative methods: as the computational analysis is assumed to be completely objective, there seems to be no reason to question the results—they merely need to be interpreted, which leads us to some kind of “digital positivism.” To rephrase Fustel de Coulanges (Monod 1889, 278), “Ne m’applaudissez pas, ce n’est pas moi qui vous parle ; ce sont les données qui parlent par mes courbes.”\nHowever, as Korzybski (1933, 58) famously remarked, “A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which accounts for its usefulness.” An analysis of a corpus will always yield results; the crucial question is whether these can tell us anything about the original phenomenon it aims to model. So, the crucial point is that corpora are not naturally occurring but intentionally constructed. A corpus is already a model and thus not epistemologically neutral. A good starting point for dealing with this seems to be Gaston Bachelard’s notion of phenomenotechnique (Bachelard 1968).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#corpora-as-phenomenotechnical-devices",
    "href": "submissions/457/index.html#corpora-as-phenomenotechnical-devices",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Corpora as Phenomenotechnical Devices",
    "text": "Corpora as Phenomenotechnical Devices\nBachelard originally developed this notion, which treats scientific instruments as “materialized theories,” as a way to study the epistemology of modern physics, which goes far beyond what is directly observable. The humanities also and even primarily deal with phenomena that are not directly observable, but only through artifacts, in particular texts. They thus have also always constructed the objects of their studies through, for example, the categorization and selection of sources and the hermeneutic postulation and affirmation of phenomena.\nHowever, only the praxis has been codified to some extent as “best practices,” such as source criticism. Theories—or perhaps better: models and metamodels, as the term “theory” has a somewhat different meaning in the humanities than in the sciences—are not formalized and are only suggested by the (natural language) narrative. What history (and the humanities in general) traditionally do not have is something that corresponds to the scientific instrument.\nThis changes with digitalization and datafication: phenomena are now constructed and modeled through data and code, and (like in the sciences), the computational model takes on the role of the instrument and “sits in the center of the epistemic ensemble” (Rheinberger 2005, 320). Corpora are then, methodologically speaking, phenomenotechnical devices and form the basis and influence how we build, understand, and research higher-level concepts—which at the same time underly the construction of the corpus. In short: a corpus produces the phenomenon to be studied. As a model, it has Stachowiak’s three characteristics of models, the characteristic of mapping, the characteristic of shortening, and the characteristic of pragmatical model-function (Stachowiak 1973, 131–33). Note also that while a model does not have all properties of its corresponding original (the characteristic of shortening), it has abundant attributes (Stachowiak 1973, 155), i.e., attributes that are not present in the original.\nStatistics provide us with means to formally describe and analyze a specific subclass of models that are able to represent originals that have particular properties. However, the phenomena studied by the humanities generally do not have these properties, and we thus still lack adequate formal methods to describe them.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#conclusion",
    "href": "submissions/457/index.html#conclusion",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Conclusion",
    "text": "Conclusion\nI have tried to outline some of the background and the motivation for the project Towards Computational Historiographical Modeling: Corpora and Concepts, which is part of a larger research program.\nSo far, digital history (and digital humanities more generally) has largely contented itself with borrowing methods from other fields and has developed little methodology of its own. The focus on “methods and tools” represents a major obstacle towards the construction of computational models that could help us to obtain new insights into humanities research questions rather than just automate primarily quantitative processing—which is, without doubt, useful, but inherently limited, given that the research questions are ultimately qualitative.\nRegardless of the application domain, digital humanities research tends to rely heavily on corpora, i.e., curated collections of texts, images, music, or other types of data. However, both the epistemological foundations—the underlying concepts—and the epistemological implications have so far been largely ignored. I have proposed to consider corpora as phenomenotechnical devices (Bachelard 1968), like scientific instruments: corpora are, on the one hand, models of the phenomenon under study; on the other hand, the phenomenon is constructed through the corpus.\nWe therefore need to study corpora as models to answer questions such as: How do corpora model and produce phenomena? What are commonalities and differences between different types of corpora? How can corpora-as-models be formally described in order to take their properties into account for research that makes use of them?\nThe overall goal of the project is to contribute to theory formation in digital history and digital humanities, and to help us move from project-specific, often ad hoc, solutions to particular problems to a more general understanding of the issues at stake.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#acknowledgements",
    "href": "submissions/457/index.html#acknowledgements",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis research was supported by the Swiss National Science Foundation (SNSF) under grant no. 105211_204305.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#references",
    "href": "submissions/457/index.html#references",
    "title": "Towards Computational Historiographical Modeling",
    "section": "References",
    "text": "References\n\n\nApostel, Leo. 1961. “Towards the Formal Study of Models in the Non-Formal Sciences.” In The Concept and the Role of the Model in Mathematics and Natural and Social Sciences, edited by Hans Freudenthal, 1–37. Dordrecht: Reidel. https://doi.org/10.1007/978-94-010-3667-2_1.\n\n\nBachelard, Gaston. 1968. Le nouvel esprit scientifique. 10th ed. Paris: Les Presses universitaires de France.\n\n\nGranger, Gilles-Gaston. 1967. Pensée formelle et sciences de l’homme. Nouvelle éd. augmentée d’une préface. Paris: Aubier-Montaigne.\n\n\nKorzybski, Alfred. 1933. Science and Sanity: An Introduction to Non-Aristotelian Systems and General Semantics. Lancaster, PA: International Non-Aristotelian Library Publishing Company. https://n2t.net/ark:/13960/t6c261n93.\n\n\nMayaffre, Damon. 2006. “Philologie et/ou herméneutique numérique: nouveaux concepts pour de nouvelles pratiques?” In Corpus en lettres et sciences sociales: des documents numériques à l’interprétation. Actes du XXVIIe Colloque d’Albi “Langages et signification”, edited by François Rastier and Michel Ballabriga, 15–25. CALS-CPST. https://hal.science/hal-00551477.\n\n\nMichel, Jean-Baptiste, Yuan K. Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. https://doi.org/10.1126/science.1199644.\n\n\nMonod, Gabriel. 1889. “M. Fustel de Coulanges.” Revue historique 42 (2): 277–85. https://www.jstor.org/stable/40938008.\n\n\nMothon, Bernard. 2010. Modélisation et vérité. Paris: Archétype82.\n\n\nPiotrowski, Michael. 2019. “Historical Models and Serial Sources.” Journal of European Periodical Studies 4 (1): 8–18. https://doi.org/10.21825/jeps.v4i1.10226.\n\n\nRheinberger, Hans-Jörg. 2005. “Gaston Bachelard and the Notion of ‘Phenomenotechnique’.” Perspectives on Science 13 (3): 313–28. https://doi.org/10.1162/106361405774288026.\n\n\nStachowiak, Herbert. 1973. Allgemeine Modelltheorie. Wien, New York: Springer.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/450/index.html",
    "href": "submissions/450/index.html",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "",
    "text": "GIS (Geographic Information Systems) have become increasingly valuable in spatial history research since the mid-1990s, and is particularly useful for analyzing socio-spatial dynamics in historical contexts (Kemp 2009, 16; Gregory and Ell 2007, 1). My PhD research applies GIS to examine and compare the development of public urban green spaces, namely public parks and playgrounds, in the port cities of Hamburg and Marseille, between post-WWII urban reconstruction and the First Oil Shock in 1973. The management and processing of data concerning green space evolution in GIS allow visualization of when and where parks were created, and how these reflect socio-spatial differentiations. This layering of information offers ways to evaluate historical data and construct arguments, while also helping communicate the project to a wider audience. To critically assess the application of GIS in historical research, I will use the SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis framework. This popular business consultancy approach (Minsky and Aron 2021) serves here as a structure for systematic reflection on how digital methods can enhance historical research and where caution is needed. The goal is to provoke critical thinking about when using GIS genuinely support research beyond producing impressive visuals, and to explore the balance between close and distant reading of historical data.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#introduction",
    "href": "submissions/450/index.html#introduction",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "",
    "text": "GIS (Geographic Information Systems) have become increasingly valuable in spatial history research since the mid-1990s, and is particularly useful for analyzing socio-spatial dynamics in historical contexts (Kemp 2009, 16; Gregory and Ell 2007, 1). My PhD research applies GIS to examine and compare the development of public urban green spaces, namely public parks and playgrounds, in the port cities of Hamburg and Marseille, between post-WWII urban reconstruction and the First Oil Shock in 1973. The management and processing of data concerning green space evolution in GIS allow visualization of when and where parks were created, and how these reflect socio-spatial differentiations. This layering of information offers ways to evaluate historical data and construct arguments, while also helping communicate the project to a wider audience. To critically assess the application of GIS in historical research, I will use the SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis framework. This popular business consultancy approach (Minsky and Aron 2021) serves here as a structure for systematic reflection on how digital methods can enhance historical research and where caution is needed. The goal is to provoke critical thinking about when using GIS genuinely support research beyond producing impressive visuals, and to explore the balance between close and distant reading of historical data.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#strengths",
    "href": "submissions/450/index.html#strengths",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Strengths",
    "text": "Strengths\nGIS are composed of layers of data sets and are mainly used for mapping, georeferencing, and data analysis (e.g. spatial analysis). The data can be varied in what it represents but must be linked to spatial parameters for it to be positioned in a visualization for analysis of spatial information (Wheatley and Gillings 2005, 1, 8). GIS layers for historical studies can be viewed as models of data sets. They represent specific topics in time and space simplistically, and spark reflection (Van Ruymbeke 2021, 8). The screen shot of my QGIS workspace (Figure 1) shows the city of Marseille with parks marked in various stages of planning in the early 1970s. This was the time when longstanding Mayor Gaston Defferre launched the large-scale greening initiative Mille Points Verts pour Marseille (Gassier 1971). Defferre and his team’s goal was to react to a growing ecological awareness and increase the number of green spaces for a more livable city (Chelini 1971). They also organized events to include and educate citizens and to garner their support for the upcoming elections (Anonymous 1971). The majority of parks created within Mille Points Verts remain until today, with only a handful of additional parks added after the mid-1970s. This is visible when the layer with parks and gardens from a 2018 dataset provided by the government of Marseille is selected (Figure 2). The strength of GIS layering is evident when we apply distant reading techniques: skimming over the model we see a display of spatial relations of park distribution and location as well as their connection to time.\nIn order for this visualization to take shape, I produced and assembled data. Specifically, I selected data and went through the process of closely reading my historical sources, learning to understand them and to think through their meaning. In this way maps are social documents. By themselves, they do not reveal anything yet. But by superimposing visualizations, GIS can reveal thinking processes of the data curators and map creators (cf. for example C. Jones 2021).\n\n\n\n\n\n\nPlanned parks (1970-71) within the ‘Mille Points Verts’ initiative.\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\nThis image can be deceiving as it seems that many parks do not overlap. Figure 1 shows approximate planning locations. The 2018 state of parks therefore shows the current locations of many of the planned ones from 1970-71.\n\n\n\n\nFigure 2",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#weaknesses",
    "href": "submissions/450/index.html#weaknesses",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Weaknesses",
    "text": "Weaknesses\nThe curation of data, although an important and empowering step for the historian and GIS researcher, also reveals the weaknesses of GIS: the mismatch between GIS requirements (in terms of data structuring and quality) and the imperfection of historical data. GIS software is created for geographers, not historians. Everything in GIS is structured data and therefore cannot handle ambiguities natural to historical sources. Sources in whatever form they are collected by the historian first must be organized, selected, tabularized, geocoded and/or georeferenced (Kemp 2009, 16–17). The caveat here is that a historian’s data is hardly ever complete. Missing records shape both what we can and cannot analyze – especially when working with GIS. In historical narration on text the researcher can explain gaps and postulate why this may be the case. GIS do not allow for gaps and thus we can only produce models and visualizations with the numerical evidence available.\nThe visualization presented here helps to model different states of an object: the park. As my data is not complete, discrepancies between the mapped data and the on-the-ground reality occur, especially since the planned parks had vague names sometimes only matching the name of an entire neighborhood. This raises the question of how to capture temporality. How can the aspect of time appear on a two-dimensional visualization? Rendering the time layer onto the spatial one demands creativity and an awareness that time is something constructed (Massey speaks of “implicit imaginations of time and space” Massey 2005, 22).\nFrom the map making perspective, time significantly impacts the creation process. GIS work is time-consuming and labor-intensive. It involves meticulous manual searching, assembling, and layering of data. However, linking to the overarching topic of this conference, AI may offer new possibilities. Tools such as Transkribus allow users to apply machine learning to filter specific elements from document sets. LLMs can then process this information into CSV files for GIS software. While not yet revolutionary, as these tools evolve, AI could become useful in extracting numerical evidence from textual sources. For geocoding of places, AI would greatly aid efficiency and relieve the researcher of tedious manual work. However, at this point, LLMs such as Claude AI and ChatGPT still hallucinate considerably.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#opportunities",
    "href": "submissions/450/index.html#opportunities",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Opportunities",
    "text": "Opportunities\nAI-assisted data extraction presents a gateway to think about opportunities. Researchers could focus more on experimenting with design and layering by automating time-consuming tasks. For example, mapping supports spatial thinking and perception by integrating the crucial ‘where’ element: Where are specific features located? How near or far is one place from another? Depending on what obstacles or facilitators are in place a park may be close in measured distance but far in terms of accessibility if there is no bridge or tunnel to e.g. cross a motorway, or water element. Therefore, how are different locations related? How can we perceive and understand distance?\nThe screenshot here shows a handful of ‘wheres’ (Figure 3). They reveal where the majority of parks are located, their proximity to neighborhoods, the types of surrounding communities, and their connections to amenities and public infrastructure. This approach enables comparisons across different scales. For instance, I can compare park distribution between Hamburg and Marseille and track their development over time.\nThese questions prompted by the use of GIS direct both user and observer back to the original sources for close reading. A GIS model can spark interest in a topic and motivate the researcher to dig deeper on what these layers mean and how they were created. Ideally GIS should be used as a starting point for in depth analysis. In the case of Marseille and Hamburg, the development of public urban green spaces was what inspired me to look more closely at the historical circumstances. Hamburg, for instance, has a long history of creating expansive green areas with the support of private patrons. Marseille does not have a comparable patronage system. Instead, municipal expropriation rendered private villas and their gardens public.\nGIS are a powerful tool that serve multiple functions in research (Wheatley and Gillings 2005, 8). They “can play a role in generating ideas and hypotheses at the beginning of a project” and serve as valuable instruments for analysis and evaluation (Brewer 2006, S36). By modeling research hypotheses and findings, e.g. maps can be used to effectively communicate to diverse audiences – from the general public to specialized groups such as urban planners and municipal governments, relevant to my field of historical urban planning research.\nA particularly compelling aspect of GIS is their ability to visually represent power relations (Figure 4). This feature bridges the gap between historical analysis and contemporary urban planning, making it an invaluable tool in understanding the evolution of urban spaces. The visualization of Marseille reveals that the majority of parks are located towards the center and south of the city and does not necessarily correspond to the population density. The south of Marseille is where villas abound and thus the upper and upper-middle class live. The majority of the HLM (housing at moderate rent) are located towards the north, where living conditions are condensed, and political representation is low. What is more, if I select the layers showing where most immigrants and workers live today, a lack of green spaces is visible (Figure 5) (Figure 6) (Figure 7).\nConnecting this once more to close reading of the sources: when Mille Points Verts was launched, planners scavenged locations for green space creation. The HLM neighborhoods were marked as unsuitable for participation in this program: People living in social housing would “misuse” the parks by playing soccer on them or walking across the grass (Anonymous 1970). This shows complexity of space perception and power imbalance (Van Ruymbeke 2021, 7).\n\n\n\n\n\n\nZoom in on the harbor area where also the “pénétrante nord” is located (built in the late 1960s). Along this main road many parks were planned but not built.\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\nPopulation 2012 layer turned on (natural breaks). This visualization presents a caveat: the northern part does not appear densely populated. This is not the case as the northern neighborhoods are very hilly, thus HLM apartment blocks house a large number of people in a small space.\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\nZoom in on harbor area. Population 2012 (natural breaks).\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\nZoom in on harbor area. Immigrants 2012 (natural breaks).\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\nZoom in on harbor area. Workers 2012 (natural breaks).\n\n\n\n\nFigure 7",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#threats",
    "href": "submissions/450/index.html#threats",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Threats",
    "text": "Threats\nYet all these opportunities are ambiguous and “entrusting machines with the memory of human activity can be frightening”. The last element of the SWOT analysis, threats, rounds off these reflections. Although it is crucial to encourage critical thinking through the mapping of, for example, political representation and wealth distribution of a city it also shows my personal convictions. I wish to demonstrate which voices where not heard in the planning of these spaces, which people were not considered when decisions were made. I am biased when I start with the premise that there is inequality. The map, objective as it may seem, never is. The book How to Lie with Maps provocatively shows the power of maps to create a strong, and perhaps deceiving, narrative:\n“Map users generally are a trusting lot: they understand the need to distort geometry and suppress features, and they believe the cartographer really does know where to draw the line, figuratively as well as literally. […] Yet cartographers are not licensed, and many mapmakers competent in commercial art or the use of computer workstations have never studied cartography. Map users seldom, if ever, question these authorities, and they often fail to appreciate the map’s power as a tool of deliberate falsification or subtle propaganda” (Monmonier 1996, 1).\n\nPeople working with GIS can have all kinds of skill levels and interests. I, for example, am not a GIS specialist and relatively new to using the tool. Still, I can easily manipulate my model to paint various pictures, if I wish to do so. I can turn on different layers and focus on the number of immigrants per neighborhood, I can change the classification for the choropleth map and create entirely different impressions, or I can simply change the basemap and take away the context of terrain, transportation systems, etc. (cf. Figure 4). The quote speaks of an almost blind trust in maps, which shows once more that we must always be critical observers of the things we consume and historians should always want to be curios fact checkers.\nA map is a series of decisions and it reflects the biography of both the maker and the observer. It is the responsibility of the historian working with GIS to be as transparent as possible regarding the choices made to display a historical development or state. It is the responsibility of the observer to use the map as a starting point for close reading, interpretation and analysis rather than the end point and a fact. We must remember “80% of GIS is about transforming, manipulating and managing spatial data”(C. E. Jones and Schiel 2022).",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#conclusion",
    "href": "submissions/450/index.html#conclusion",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion the use of GIS in historical research and analysis requires the researcher to stay true to the principles of the craft of the historian: source criticism, the ability to subsume information, create a strong narrative, document the process of source manipulation and proper source citation. As historians we should be aware of the power of storytelling – no matter which medium we use. An audience’s spatial understanding can be enhanced via GIS models, serving as a support system of sorts. All the more reason why GIS in historical analysis must be used and consumed critically and consciously. By embracing this complexity, we can use GIS for historical reflections, enhancing our understanding of spatial and temporal dynamics in historical contexts.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#references",
    "href": "submissions/450/index.html#references",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "References",
    "text": "References\n\n\nAnonymous. 1970. “Aménagement d’une Bande de Terrain Longeant l’avenue Jean Compadieu à BOIS LEMAITRE. COMPTE RENDU DE VISITE Suite à La Demande de M. P. CERMOLACCE.” 554W6, Archives de la Ville de Marseille.\n\n\n———. 1971. “La Semaine Verte.” 483W278, Archives de la Ville de Marseille.\n\n\nBrewer, Cynthia A. 2006. “Basic Mapping Principles for Visualizing Cancer Data Using Geographic Information Systems (GIS).” American Journal of Preventive Medicine 30 (2): S25–36. https://doi.org/10.1016/j.amepre.2005.09.007.\n\n\nChelini, Prof. J. 1971. “Letter to Monsieur Jacques Mazel.” 483W278, Archives de la Ville de Marseille.\n\n\nGassier, G. 1971. “Envoi de Circulaires Sur Les ’Espaces Verts’.” 554W6, Archives de la Ville de Marseille.\n\n\nGregory, Ian, and Paul S. Ell. 2007. Historical GIS: Technologies, Methodologies, and Scholarship. Cambridge Studies in Historical Geography 39. Cambridge ; New York: Cambridge University Press.\n\n\nJones, Catherine. 2021. “Mapping Citizens’ Reflections and Perceptions of Place-Based Experiences in the Time of COVID-19.” In. 29th GISRUK Conference 2021. https://doi.org/10.5281/ZENODO.4664529.\n\n\nJones, Catherine E., and Kerry Schiel. 2022. “Introduction to GIS, Lesson 1 ‘Mapping Elements and My First Map’.” Université du Luxembourg.\n\n\nKemp, Karen K. 2009. “What Can GIS Offer History?” International Journal of Humanities and Arts Computing 3 (1-2): 15–19. https://doi.org/10.3366/ijhac.2009.0006.\n\n\nMassey, Doreen B. 2005. For Space. ProQuest Ebook Central: SAGE Publications.\n\n\nMinsky, Laurence, and David Aron. 2021. “Are You Doing the SWOT Analysis Backwards?” Harvard Business Review, February. https://hbr.org/2021/02/are-you-doing-the-swot-analysis-backwards.\n\n\nMonmonier, Mark. 1996. How to Lie with Maps. 2nd ed. Chicago: University of Chicago Press.\n\n\nVan Ruymbeke, Muriel. 2021. “Modéliser l’information Archéologique à l’ère Du Web Sémantique. Relecture 2.0 Des Données Archéologiques Antiques Et Alto-Médiévales de La Commune de Theux (B.).” Thèse de Doctorat, Liège: Université de Liège.\n\n\nWheatley, David, and Mark Gillings. 2005. Spatial Technology and Archaeology: The Archeaological Applications of GIS. London: Taylor & Francis.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/456/index.html",
    "href": "submissions/456/index.html",
    "title": "Theory and Practice of Historical Data Versioning",
    "section": "",
    "text": "Among digital approaches to historical data, we can identify various types of scholarship and methodologies practiced in creating transcription corpora of documents that are valuable for research, specifically quantitative analyses. Demographic, cadastral, and geographic sources are particularly notable for their transcripts, which are instrumental in generating datasets that can be used or reused across different disciplinary contexts.\nFrom a disciplinary perspective, it is essential to observe certain trends that have fundamentally transformed the methodology of making historical data accessible and studying them. These trends extend the usability of historical data beyond the individual scholar’s interpretation to a broader community. The creation of datasets that can then serve again to different communities of study is increasingly practiced by scholars; the data, before their interpretations, acquire an autonomous value, an important authorial legitimacy. Just as is already currently practiced in the natural sciences, the provision of datasets becomes crucial to history as well. Computational approaches, however, follow trends common to other fields, and the increasing use of artificial intelligence to extract, refine, realign, and understand historical data compels the integration of clear and well-described protocols to inform the datasets that are created.",
    "crumbs": [
      "Abstracts",
      "Theory and Practice of Historical Data Versioning"
    ]
  },
  {
    "objectID": "submissions/456/index.html#introduction",
    "href": "submissions/456/index.html#introduction",
    "title": "Theory and Practice of Historical Data Versioning",
    "section": "",
    "text": "Among digital approaches to historical data, we can identify various types of scholarship and methodologies practiced in creating transcription corpora of documents that are valuable for research, specifically quantitative analyses. Demographic, cadastral, and geographic sources are particularly notable for their transcripts, which are instrumental in generating datasets that can be used or reused across different disciplinary contexts.\nFrom a disciplinary perspective, it is essential to observe certain trends that have fundamentally transformed the methodology of making historical data accessible and studying them. These trends extend the usability of historical data beyond the individual scholar’s interpretation to a broader community. The creation of datasets that can then serve again to different communities of study is increasingly practiced by scholars; the data, before their interpretations, acquire an autonomous value, an important authorial legitimacy. Just as is already currently practiced in the natural sciences, the provision of datasets becomes crucial to history as well. Computational approaches, however, follow trends common to other fields, and the increasing use of artificial intelligence to extract, refine, realign, and understand historical data compels the integration of clear and well-described protocols to inform the datasets that are created.",
    "crumbs": [
      "Abstracts",
      "Theory and Practice of Historical Data Versioning"
    ]
  },
  {
    "objectID": "submissions/456/index.html#lausanne-and-venice-census-datasets",
    "href": "submissions/456/index.html#lausanne-and-venice-census-datasets",
    "title": "Theory and Practice of Historical Data Versioning",
    "section": "Lausanne and Venice Census Datasets",
    "text": "Lausanne and Venice Census Datasets\nThe first example are the IIIF (International Image Interoperability Framework) protocols. These protocols allow the community to employ computational approaches to use these sources as a foundation for refining techniques to extract the embedded information. Open sources are made possible by heritage institutions that recognize the added value of studying their collections with computational approaches, thus transforming them into indispensable objects of research for interdisciplinary and ever-expanding community. Another significant trend is the creation of datasets through the extraction of information contained in sources by diverse communities from various disciplinary fields. These communities might be interested in the computational methodologies, extraction techniques, or the historical content of the extracted data. The field of Digital Humanities, addressing the latter aspect, has effectively highlighted that each element extracted from historical documents should, whenever possible, be directly referenced to the document from which it was taken. This can be achieved through IIIF-compatible datasets. Moreover, the methods for extracting, processing, and normalizing the data must always be thoroughly explained and are subject to scholarly critique and evaluation. Emphasizing the importance of maintaining the link between extracted data and original documents ensures transparency and accuracy, thus enhancing the credibility and usability for digital historical research.\nWhen publishing a historical dataset, adhering to a methodology that consistently maintains the link to the original source is crucial. Wherever possible, the source document itself should be made accessible. Additionally, the methods used to extract historical information must be clearly stated. This transparency allows for subsequent corrections, refinements, and improvements to the dataset by other researchers, even a posteriori (Maryl 2023). This means that datasets can be corrected and refined, particularly when normalizing entity names such as places or people. In this context, we are moving beyond the traditional reliance on the author and their citation. Every piece of information valuable for historical reconstruction should be validated by tracing its publication genesis: from the original source, through its interpretation, to its final dataset form (Traub 2020).\nSpecifying the version of the data released and the one being worked on for improvements is essential. When these datasets are studied with quantitative analysis approaches, any profound transformation of the dataset—such as an improved version through OCR correction—can lead to a re-evaluation of all existing interpretations (Bürgermeister 2020). This ensures that the resulting historical narratives can be re-examined and updated accordingly.\nMany projects that handle historical data, such as datasets containing information about people or places, follow established pipelines. These pipelines typically involve an initial transcription phase, which can be manual, semiautomatic, or automatic (using machine learning or artificial intelligence), followed by OCR (Optical Character Recognition) verifications and, finally, entity alignment with existing dictionaries or the creation of specific ontologies for the extracted entities.\nThe new potentials afforded by the latest computational techniques —whether based on manual annotation, automatic machine learning, or OCR extraction through AI— position us on the brink of what we may soon call the data deluge of information drawn from historical sources (Kaplan and Lenardo 2017). Pioneering approaches to harnessing this deluge are already proliferating (Gibbs and Owens 2013). This vast quantity of mined data makes manual exploration impractical, necessitating an approach where computational methods facilitate asking questions, analyzing answers, and reformulating inquiries.\nThese new methods will partially reshape historical approaches, requiring engagement with extensive datasets of varied origins. The validation of data extraction will become a critical task within both human and computational pipelines. This raises the question of the accuracy of information in the extracted datasets, making the extraction and correction process itself crucial.\nSeveral key elements emerge in this context. First, there is a distinction between creating datasets accurate enough to locate results—such as names or places—in specialized search engines and creating datasets for precise theoretical and historical analysis. While these elements are related, they operate on different temporal scales.\nProcessing and extracting millions of transcript segments is a computationally intensive task. In the case of the “Names of Lausanne” project (EPFL-CROSS project n. 20212), part of the larger Lausanne Time Machine initiative, dealing with census sources containing millions of text segments, after an initial phase of manual annotation training, it is essential to rely on computational approaches. Although the results do not always perfectly reflect the original text and may contain spelling errors, they are often sufficient for an elastic search or distance-approximative navigation.\nRESTful search and analytics engine capable of addressing a growing number of use cases is particularly well suited for managing and querying large datasets (Gormley and Tong 2015). It enables full-text search, structured search, and analysis, providing a robust framework for handling large amounts of historical data (Neudecker and Antonacopoulos 2016). The flexibility of elastic search means that it can handle complex queries and provide fast search results, even in datasets that contain inconsistencies or errors. This capability is essential to enable initial navigation and interaction with the huge volumes of data generated by historical sources, facilitating further detailed analysis, and possibly enabling collaborative approaches to correction.\nThis means that millions of units of information can be made “searchable,” even if they do not correspond to a fully curated dataset. Through further approaches –such as contributory appeals for correction or progressive improvements in transcription techniques– these datasets can eventually produce “structured” and verified datasets, as understood in the historical sciences. Consequently, it will become increasingly essential to trace the computational origin of extracted items. This involves documenting the methods by which the data were processed and specifying the version of the dataset to which they correspond, recognizing that these versions are subject to continuous improvement.\nBy applying these principles to historical datasets, we can ensure that the data extracted is not only accessible and traceable, but also continuously refined and validated through collaborative efforts and methodological advances.\nIn “Names of Lausanne”, Petitpierre et al. (2023) automatically extracted 72 census records of Lausanne. The complete dataset covers a century of historical demography in Lausanne (1805-1898), corresponding to 18,831 pages, and nearly 6 million cells (Figure 1).\n\n\n\n\n\n\nFigure 1: Sample page from the “Recensements de la ville de Lausanne”; (AVL) Archives de la Ville de Lausanne\n\n\n\nThe structure and informational wealth of censuses have also provided an opportunity to develop automatic methods for processing structured documents. The processing of censuses includes several steps, from identifying text segments to restructuring information as digital tabular data through Handwritten Text Recognition and the automatic segmentation of the layout using neural networks (Petitpierre, Kramer, and Rappo 2023). The data are structured in rows and columns, where each row corresponds to a distinct household.\nThe historical information is particularly rich, including the composition of the household, children, place names, servants, housemates, as well as the occupations, municipalities of origin, and dates of birth of all persons mentioned. The documents present considerable reading difficulties, and the liability of the first version of the dataset extracted, combining Handwritten Text Recognition (HTR) with pre-processing and post-correction strategies based on supervised language models, achieved a character error rate of 3.44%. These improvements result in high-quality data suitable for research in demographic and social history. The data are already searchable, and can later be queried using string distance heuristics, or statistical methodologies to deal with noise and uncertainty (Figure 2). The goal is to make these transcribed datasets, produced using automatic approaches, available in a collaborative annotation tool that will allow a progressively larger number of mentions to be corrected manually, and use the corrections to progressively reprocess and improve the corpus.\n\n\n\n\n\n\nFigure 2: Screenshot of the search interface. In the example, the filter search targeted the surname “Rochat” in the dataset of census records for the year 1832.\n\n\n\nIn the case of “Parcels of Venice” (FNS project - 185060), land records from the Napoleonic land register (1808) were analyzed (Figure 3). They contain a parcel number with some associated attributes, such as owner, function, and area, which are linked to plans on which the geometries of the parcels are drawn, and can be reassembled on a map of Venice (Lenardo et al. 2021).\nSome 23,428 lines of documents were manually extracted and then analyzed, which included mentions of owners, functions, and house numbers, as well as other correspondences. In this case, the mentions will also be entered into a text search engine that can filter and combine different fields, but there was a need to standardize some items, for example, family surnames and functions, in order to proceed with quantitative historical analyses. The proprietary text entries in the dataset are too noisy for reliable analysis, due to the lack of standard delimiters, missing surnames, frequent use of the term “above” (“as above”), and other inconsistencies. Additional information, such as family relationships or origin details, complicates entries. No general algorithm can accurately correct and standardize these entries; therefore, a manual standardization approach was used. This involved manually editing all 23,428 entries, aided by a customizable Jupyter Notebook tool, to apply specific corrections across the dataset (Musso 2023). The goal was different; it was not only to make the data searchable but also to make it analyzable, particularly regarding ownership. The classification of owners into three types —City of Venice, Institution, and Person— was carried out in a three-step process, resolving 23,395 entries. The unresolved entries did not match any of the predefined categories.\n\n\n\n\n\n\nFigure 3: Sample page from the “Sommarioni” (Land Registers) of Venitian Cadaster ab.1808; (ASVE) Archivio di Stato di Venezia\n\n\n\nFor named entity recognition, a “people” dataset was created, linking individuals with their parcels based on disambiguated person mentions from the Sommarioni records. A merging protocol was employed to consolidate entries with the same name and additional matching attributes into unique person objects, resulting in a comprehensive and standardized dataset ready for detailed analysis. The dataset navigation interface thus indicates not only the authorship of the manual transcription, where present, but also whether the displayed data correspond to a semi- or fully automatic extraction or entrusted even partially to the language models (Figure 4). It will also be possible, via GitHub, to trace the extraction production Jupyter notebook 1. This will allow the user, whether the general public or scholars to clarify the genesis of the data represented.\n\n\n\n\n\n\nFigure 4: Screenshot of the search interface for the city of Venice. In the example, the filter search targeted the surname “Pasqualigo” in the dataset of Napoleonic cadaster (1808); Under the dataset name, the nature of the data extraction is specified.\n\n\n\nAn initial version of this dataset will be published and will already allow some key aspects of land tenure to be studied, however, not all fields of information have been standardized, which is why we can build on this version later to improve machine reading and produce new corrections.\nThese projects underscore the transformative impact of OCR and HTR technologies, coupled with language models, on the extraction and correction processes of historical documents. The challenge lies in consistently documenting the computational process origins within datasets, ensuring users can evaluate the reliability of the data. As the quality of data extraction and transcription improves, new historical narratives may emerge, emphasizing the critical need to track data versions and correct older datasets to prevent potential inaccuracies.",
    "crumbs": [
      "Abstracts",
      "Theory and Practice of Historical Data Versioning"
    ]
  },
  {
    "objectID": "submissions/456/index.html#references",
    "href": "submissions/456/index.html#references",
    "title": "Theory and Practice of Historical Data Versioning",
    "section": "References",
    "text": "References\n\n\nBürgermeister, Martina. 2020. “Enabling the Scholarly Discourse of the Future: Versioning RDF Data in the Digital Humanities.” In Graph Technologies in the Humanities - Proceedings 2020, edited by Tara Andrews, Franziska Diehr, Thomas Efer, Andreas Kuczera, and Joris van Zundert, 3110:1. CEUR Workshop Proceedings. Vienna, Austria: CEUR. https://ceur-ws.org/Vol-3110/#paper1.\n\n\nGibbs, Fred, and Trevor Owens. 2013. “The Hermeneutics of Data and Historical Writing.” In Writing History in the Digital Age, edited by Jack Dougherty and Kristen Nawrotzki, 159–70. University of Michigan Press. https://doi.org/10.2307/j.ctv65sx57.18.\n\n\nGormley, Clinton, and Zachary Tong. 2015. Elasticsearch: The Definitive Guide: A Distributed Real-Time Search and Analytics Engine. 1st edition. Beijing ; Sebastopol, CA: O’Reilly Media.\n\n\nKaplan, Frédéric, and Isabella di Lenardo. 2017. “Big Data of the Past.” Frontiers in Digital Humanities 4 (May). https://doi.org/10.3389/fdigh.2017.00012.\n\n\nLenardo, Isabella di, Raphaël Barman, Federica Pardini, and Frédéric Kaplan. 2021. “Une Approche Computationnelle Du Cadastre Napoléonien de Venise.” Humanités Numériques 3 (May). https://doi.org/0.4000/revuehn.1786.\n\n\nMaryl, Maciej. 2023. “Recognition and Assessment of Digital Scholarly Outputs in the Humanities.” Septentrio Conference Series, no. 1 (September). https://doi.org/10.7557/5.7151.\n\n\nMusso, Carlo. 2023. “Standardising Ownership Information from the Napoleonic Cadastre of 1808 Venice: Methods and Findings in the First Database Creation.” Master's project, Lausanne, Switzerland: EPFL.\n\n\nNeudecker, Clemens, and Apostolos Antonacopoulos. 2016. “Making Europe’s Historical Newspapers Searchable.” In 2016 12th IAPR Workshop on Document Analysis Systems (DAS), 405–10. https://doi.org/10.1109/DAS.2016.83.\n\n\nPetitpierre, Remi, Marion Kramer, and Lucas Rappo. 2023. “An End-to-End Pipeline for Historical Censuses Processing.” International Journal on Document Analysis and Recognition (IJDAR), March. https://doi.org/10.1007/s10032-023-00428-9.\n\n\nPetitpierre, Remi, Marion Kramer, Lucas Rappo, and Isabella di Lenardo. 2023. “1805-1898 Census Records of Lausanne : A Long Digital Dataset for Demographic History.” https://doi.org/10.5281/zenodo.7711640.\n\n\nTraub, Myriam Christine. 2020. “Measuring Tool Bias & Improving Data Quality for Digital Humanities Research.” SIKS Dissertation Series, no. 2020-09 (May). https://dspace.library.uu.nl/handle/1874/396185.",
    "crumbs": [
      "Abstracts",
      "Theory and Practice of Historical Data Versioning"
    ]
  },
  {
    "objectID": "submissions/456/index.html#footnotes",
    "href": "submissions/456/index.html#footnotes",
    "title": "Theory and Practice of Historical Data Versioning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/dhlab-epfl/venice-owners-1808↩︎",
    "crumbs": [
      "Abstracts",
      "Theory and Practice of Historical Data Versioning"
    ]
  },
  {
    "objectID": "submissions/460/index.html",
    "href": "submissions/460/index.html",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "",
    "text": "As part of the author’s PhD project, ‘Glass and its makers in Estonia, c. 1550–1950: an archaeological study,’ the genealogical data about 1,248 migrant glassworkers and their family members working in Estonia from the 16th–19th century were collected using archival records and newspapers. The goal was to use information about key life events to trace the life histories of the glassworkers and their families from childhood to old age to gain an understanding of the community and the industry through one of its most important aspects – the workforce. It was hoped that the data will also assist in identifying the locations and names of glassworks during the period under study. In this paper, the author reflects on the process of this documentary archaeology research. The data collection, storage, and visualisation process are described, followed by the results of the study which have been included in a doctoral dissertation (Reppo 2024) and a research article (Reppo 2023b).",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#introduction",
    "href": "submissions/460/index.html#introduction",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "",
    "text": "As part of the author’s PhD project, ‘Glass and its makers in Estonia, c. 1550–1950: an archaeological study,’ the genealogical data about 1,248 migrant glassworkers and their family members working in Estonia from the 16th–19th century were collected using archival records and newspapers. The goal was to use information about key life events to trace the life histories of the glassworkers and their families from childhood to old age to gain an understanding of the community and the industry through one of its most important aspects – the workforce. It was hoped that the data will also assist in identifying the locations and names of glassworks during the period under study. In this paper, the author reflects on the process of this documentary archaeology research. The data collection, storage, and visualisation process are described, followed by the results of the study which have been included in a doctoral dissertation (Reppo 2024) and a research article (Reppo 2023b).",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#data-collection",
    "href": "submissions/460/index.html#data-collection",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "Data collection",
    "text": "Data collection\nThe aim of this part of the PhD project was to collate, visualise, and publish data on the key life events of migrant glassworkers in post-medieval Estonia. Information on 1,248 individuals was obtained who are mostly of German origin. This list is in no way complete but provides information workers and their family members connected with the glass industry from the 16th century until the 1840s–1860s when the reliance on foreign workers started to lessen due to the abolishment of serfdom in Estonia which allowed locals access to skilled professions previously inaccessible to them (Reppo 2024, 52).\nThe data were collected, tabulated, and made Open Access via DataDOI (“DataDOI” 2024) as a raw dataset (Reppo 2023a). The following life events were considered – birth, baptism, marriage(s), and death. Both the date and place were included where possible to identify migration routes to and within Estonia. With baptisms, the number of godparents as well as names of all the godparents in the order listed in the church records were included. In total, the dataset has 1,249 rows and 22 columns. But how to find, access, and organise data about more than 1,200 individuals at this scale?\nIn addition to previously published sources and some additional archival information, this study mainly used records kept and digitised by the National Archives of Estonia (NAE) and the National Library of Estonia (NLE). During the period under study, the area of modern day Estonia was under the rule of the Swedish Kingdom (1561–1710) and the Russian Czardom (1710–1918). Due to the political history of the area, official business, including church records were kept in German well into the 19th century but also both Swedish and Russian during the respective periods. The newspapers considered in this study were also published in German and Russian as a result but there are also sources compiled in Estonian that were used in this study. This means the raw data could be in any of these languages.\nAs the dissertation and most of the articles connected to this thesis was written in English, all collected data was translated into English. For many of the entries on the dataset, the place name in the original source was in German. The currently used name is given first with the German version in brackets, for example, ‘Latvia, Suntaži (Sunzel).’ For Estonian place names, the German version is mostly not given but can be found in the Dictionary of Estonian Place Names (KNR; “Dictionary of Estonian Place Names” (2017)). For the workers’ profession, the translated version is given first with the title from the original source, for example, ‘Hollow glass maker (Hohlgläser).’ For surnames, there is some change from German to Russian to Estonian and from church warden to another. The most common variations of a surname are given in brackets – for example, ‘Kilias (Kihlgas).’ This translation is not included for the glassworks as all used names and other details such as coordinates, operation dates, owners, and so on are given in another dataset (Reppo 2023b).",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#national-archives-of-estonia",
    "href": "submissions/460/index.html#national-archives-of-estonia",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "National Archives of Estonia",
    "text": "National Archives of Estonia\nFrom the NAE, data were collected by identifying records using the Archival Information System (Rahvusarhiiv 2024a), the name register for the Lutheran congregations (“Luteri Koguduste Personaalraamatute Nimeregister” 2024), and Saaga (“Saaga” 2024). With AIS and Saaga, it was possible to find references to records only available as paper copies at the NAE reading rooms in Tartu and Tallinn but also access digitised records, most of which were church books. NAE has estimated that around 34 million images of their physical records have been made available online which is roughly 5% of their collection (“National Archives of Estonia” 2024). NAE adopted Transkribus, an AI-powered platform developed to transcribe and recognise historical handwritten documents and text in October 2022 (Rahvusarhiiv 2024b) but a limited number of records are searchable through this feature at present.\nUnfortunately, none of the records related to the glassworkers life events under consideration in this study have been added yet. To test the employability of Transkribus as a non-expert user, a handful of 17th-century documents in Swedish were run through Transkribus (Transkribus 2024) by the author to identify the location of a glassworks in Pärnu, Estonia. These records did not yield results that were hoped for but using Transkribus did speed up the process, even if the transcribed text needed corrections.\nDespite the current lack of records related to the key life events of the glassworkers via the Transkribus engine on the NAE homepage, the archive has used family name indexes compiled in the 1960s–1980s at the present-day Estonian Ministry of the Interior’s IT and Development Centre Department of Population Services based on church books which were kept until the 1940s. Although many congregations have preserved church records already from the 18th century and some even from the 17th century, the church law legislated keeping church books only from 1834 onwards so the coverage varies across Estonia (Puss 2024). For this study, the focus was on the Kärevere-Laeva region which housed the largest number of Estonian glassworks from the mid-18th until the 20th century (Reppo 2024, 35). This means studying the church books from this area – Kursi and Kolga-Jaani parishes – was predicted to be the most advantageous exercise.\nThe indexes mentioned above are based on these records and list the last name with the relevant church book page numbers. Their digitisation was started in 2005 by the Estonian Association of Genealogists, taking advantage of researchers’ strong interest in this material (Puss 2024). Members and other volunteers thus digitised these indexes but also added their own indexes to this collection. The NAE complemented these surname indexes with a search engine which allows searching by date, parish, and last name. Over the years, the system has been developed to allow users to add image numbers which direct researchers to the correct image (page) in the digitised church book. Maiden names have also been partially indexed. The archive has now upscaled the use of this external help, crowdsourcing the indexing for specific thematic projects occasionally.\nAlthough the crowdsourced indexes allowed identifying the records which included the glassworkers, and most of these were indeed digitised, the use of records from NAE during this study was certainly affected by the need to use traditional research methods to retrieve the information. Thus, thousands of pages of church books were combed through to compile the raw dataset after identifying the parishes with the highest number of glassworks. With further help from transcription services, the process of collecting basic data about key life events of the glassworkers and their family members could be streamlined further. Whilst some 17th-century records were uploaded to Transkribus for transcription to speed up the process of collecting very straightforward data for the individuals – dates and locations of key life events – future studies would certainly be facilitated by the built-in Transkribus engine on NAE.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#national-library-of-estonia",
    "href": "submissions/460/index.html#national-library-of-estonia",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "National Library of Estonia",
    "text": "National Library of Estonia\nFurther information about the glassworkers and their family members was collected from the Digital archive of Estonian newspapers (“DIGAR Eesti Artiklid” 2024) which is managed by the NLE. As the newspapers available via this database were published from 1811 with some earlier exceptions. Unlike NAE, this collection employs Optical Character Recognition (OCR). The use of OCR for these records did significantly speed up the process of research. There were obviously errors, for example where OCR was unable to detect the layout of the text or where the print ink had bled. The database allows corrections from users. As the author of this study did correct the errors in recognised characters in the sources used for this study, future searches for other researchers should be less error-prone.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#publication",
    "href": "submissions/460/index.html#publication",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "Publication",
    "text": "Publication\nPublication of raw datasets in Estonian archaeology is a new phenomenon and has been particularly rare for material culture studies which this study was a part of (Reppo 2024, 38). In addition to adhering to FAIR principles, the publication of this dataset is tied to an unusual situation – the author is the only archaeologists in Estonia studying post-medieval glass. In fact, three large datasets were published as part of this dissertation – one on archaeological finds (Reppo 2023a), another on the workers (Reppo 2023a), and a third one the glassworks themselves (Reppo 2023b) to avoid research monopoly and encourage other researchers to study the post-medieval glass industry in Estonia.\nThe raw dataset was published Open Access under a CC-BY 4.0 licence via DataDOI, a free data repository which is managed by the University of Tartu library which provides the dataset with a persistent interoperable identifier. As mentioned above, the dataset of life events is tabulated and has 22 columns and 1,249 lines. It is accompanied by a metadata file which includes details on the project, the references, and other information relevant to the raw data.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#visualisation",
    "href": "submissions/460/index.html#visualisation",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "Visualisation",
    "text": "Visualisation\nOne of the goals of this study were to visualise the data to provide easily legible images (charts, models, drawings) which encompass the entirety of the collected data. The data were visualised using Gephi, an open-source visualisation program by extracting the raw data using pivot tables in Microsoft Excel and wrangling the data to remove unnecessary details and columns. This proved that the data is mutable and suitable for network analysis. For Gephi, this data needed to be sorted into nodes and edges which allows visualising the connections between several points of data by means of lines. After cleaning the data, the format was transformed from a Microsoft Excel table (.XLSX) to a .CSV file to run the model. In the model, the node (point) size is representative of the number of connections to the place or family. Glassworks are differentiated from birth, marriage, and death locations by the ‘GW’ (glassworks) in the name.\nIn this model, marriages between families and the connections of those families to places are plotted based on their places of origin, birth, baptism, marriage, and death. With further data wrangling it would be possible to show the connections of the glassworkers and their family members within the larger community beyond marriages by analysing the connections of those individuals who appear as godparents.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#results",
    "href": "submissions/460/index.html#results",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "Results",
    "text": "Results\nThis study explored the network of connections between 1,248 migrant glassworkers and their family members working in Estonia from the 16th–19th century, using Transkribus, OCR, and Gephi as the main tools. A complete list of workers during this period was not the goal of this study. The raw dataset was published via DataDOI, an Open Access repository managed by the University of Tartu library in accordance to FAIR principles. The data shows that a key factor in building and maintaining the glass community was godparenting and marriages between the families. In addition to tracing migration to, within, and from Estonia, the data also allowed identifying the makers of some archaeological glass artefacts and locations and names of glassworks.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/460/index.html#references",
    "href": "submissions/460/index.html#references",
    "title": "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia",
    "section": "References",
    "text": "References\n\n\n“DataDOI.” 2024. https://datadoi.ee/.\n\n\n“Dictionary of Estonian Place Names.” 2017. https://arhiiv.eki.ee/dict/knr/.\n\n\n“DIGAR Eesti Artiklid.” 2024. https://dea.digar.ee/.\n\n\n“Luteri Koguduste Personaalraamatute Nimeregister.” 2024. https://www.ra.ee/dgs/addon/nimreg/index.php.\n\n\n“National Archives of Estonia.” 2024. https://www.ra.ee/en/national-archives/about-us/.\n\n\nPuss, Fred. 2024. “Ülevaade Personaalraamatutest Ja Projektist.” https://www.ra.ee/dgs/addon/nimreg/about.php.\n\n\nRahvusarhiiv. 2024a. “Arhiivi Infosüsteem.” https://ais.ra.ee.\n\n\n———. 2024b. “Otsi Otse Allikast.” https://rahvusarhiiv.transkribus.eu/.\n\n\nReppo, Monika. 2023a. “Dataset 1. Archaeological Glass Finds from Estonia.” https://doi.org/10.23673/re-450.\n\n\n———. 2023a. “Dataset 2. 16th–19th-Century Glassworkers in Estonia.” https://doi.org/10.23673/re-448.\n\n\n———. 2023b. “Dataset 3. 17th–20th-Century Glassworks in Estonia.” http://dx.doi.org/10.23673/re-449.\n\n\n———. 2023b. “Moving Skills, Moving Ideas – Migrant Glassworkers in 17th–19th\u0002century Estonia.” Post-Medieval Achaeology, 2023b.\n\n\n———. 2024. “Glass and Its Makers in Estonia, c. 1550–1950: An Archaeological Study.” PhD thesis, Tartu: University of Tartu.\n\n\n“Saaga.” 2024. https://www.ra.ee/dgs/explorer.php.\n\n\nTranskribus. 2024. “Unlock the Past with Transkribus.” https://www.transkribus.org/.",
    "crumbs": [
      "Abstracts",
      "20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia"
    ]
  },
  {
    "objectID": "submissions/445/index.html",
    "href": "submissions/445/index.html",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "",
    "text": "Scholars and interested laypeople who want to adequately deal with historical topics or generally extract information from differently structured historical documents need both knowledge of old scripts and methods for analysing complex layouts. Studies of written artefacts are only possible if they can be read at all – written in unfamiliar scripts such as Gothic Cursive, Humanist Minuscule or German Kurrent and sometimes with rather unconventional layouts. Until now, the relevant skills have been developed, for example, by the highly specialised field of palaeography. In the last few years, a shift in practice has taken place. With digital transcription tools based on deep learning models trained to read these old scripts and accompanying layouts on the rise, working with old documents or unusual layouts is becoming easier and quicker. However, using the corresponding software and platforms can still be intimidating. Users need to have a particular understanding of how to approach working with Automated Text Recognition (ATR) depending on their projects aims. This is why the Ad fontes platform (Ad Fontes 2018) is currently developing an e-learning module that introduces students, researchers, and other interested users (e.g. citizen scientists) to ATR, its use cases, and best practices in general and more specifically into how exactly they can use ATR for their papers and projects.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#introduction",
    "href": "submissions/445/index.html#introduction",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "",
    "text": "Scholars and interested laypeople who want to adequately deal with historical topics or generally extract information from differently structured historical documents need both knowledge of old scripts and methods for analysing complex layouts. Studies of written artefacts are only possible if they can be read at all – written in unfamiliar scripts such as Gothic Cursive, Humanist Minuscule or German Kurrent and sometimes with rather unconventional layouts. Until now, the relevant skills have been developed, for example, by the highly specialised field of palaeography. In the last few years, a shift in practice has taken place. With digital transcription tools based on deep learning models trained to read these old scripts and accompanying layouts on the rise, working with old documents or unusual layouts is becoming easier and quicker. However, using the corresponding software and platforms can still be intimidating. Users need to have a particular understanding of how to approach working with Automated Text Recognition (ATR) depending on their projects aims. This is why the Ad fontes platform (Ad Fontes 2018) is currently developing an e-learning module that introduces students, researchers, and other interested users (e.g. citizen scientists) to ATR, its use cases, and best practices in general and more specifically into how exactly they can use ATR for their papers and projects.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#teaching-atr-online-the-setting",
    "href": "submissions/445/index.html#teaching-atr-online-the-setting",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "Teaching ATR online – the setting",
    "text": "Teaching ATR online – the setting\nDigital methods allow a more comprehensive range of users to assign, analyse and interpret sources digitally. This increased availability of data (mainly in the form of digitized images) also protects the original documents. Recognising handwriting with the help of machine learning, known as ATR, has been greatly improved and is becoming increasingly important in various disciplines. Machine learning methods, especially deep learning, have been used for complex evaluation decisions for a number of years.(Muehlberger et al. 2019) For text recognition, especially for recognising handwriting, ATR can achieve far better results than conventional Optical Character Recognition (OCR). However, many non-standardised fonts and layouts will lead to high error rates in the recognition processes, which is why it is essential to clean up data manually. The reading order of individual lines or blocks of text, in particular, poses major challenges for machine transcription tools.\nEven ATR itself presents several hurdles; the existing tools are often only intuitive to use to a limited extent. Due to the error rates described above, cleaning up the automatically recognized texts by hand is essential. New users must familiarise themselves with these processes, whether this is on their own at home or in a mentored university or non-academic course. In addition, text recognition itself is only part of the learning curve: to work independently with ATR, it is also necessary to recognise when which form of text and layout recognition makes sense, where it is worth investing time to save more time later on and how to proceed with the output. For these reasons, the ongoing DIZH project PATT (Potentials of Advanced Text Technologies: Machine Learning-based Text Recognition)(UZH 2024) at the University of Zurich and the Zurich University of Applied Sciences is currently developing an open-source e-learning module teaching students, young researchers, and the interested public (in the sense of citizen science) how to use ATR.\nDeveloping an exhaustive learning module is a desideratum, as many researchers working in history or linguistics today want to work with automated text and layout recognition. This complex digital skill set involves the critical categorisation of the machine’s feedback. Although the steps involved in manuscript reading are becoming more efficient, they also require new skills: the work no longer centres on direct engagement with handwriting or print, but on the efficient and task-appropriate correction of the results of automated text and layout recognition, often bringing in the original sources later in for a combined distant and close reading.\nThe learning module will be published on Ad fontes. This e-learning tool has been helping researchers to prepare for their work in the archive for around 20 years.(Kränzle and Ritter 2004) It consists of exercises and tutorials that introduce researchers to different types of sources and techniques used to transcribe and analyse them. The platform was completely revised in autumn 2018, meets current standards, and is designed to be interactive to provide a good learning experience.(Hodel and Nadig 2019) The platform has attracted a great deal of attention, both within Switzerland and internationally. Ad fontes’ open access policy (modules are published under a Creative Commons license) ensures that the new module will not be hidden behind a paywall or disappear from the WWW after a short time. The Ad fontes e-learning project is based at the Chair of Medieval History of Prof. Dr. Simon Teuscher.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#when-and-how-to-use-atr-for-a-specific-project",
    "href": "submissions/445/index.html#when-and-how-to-use-atr-for-a-specific-project",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "When and how to use ATR for a specific project",
    "text": "When and how to use ATR for a specific project\nClear target groups and precise learning objectives have been defined for the learning module. As a first step in this direction, we involved experts in the PATT project, defining the following two target groups: First, people with no experience with ATR and the corresponding software and platforms; second, scholars who are already informed about the principles but would like to expand their knowledge systematically and targeted. Our general learning objective is to convey how ATR can be used in a time-efficient, project-specific manner without the recognition process becoming a time waster.\nThe new e-learning module will consist of three parts: (1) What and why ATR?, (2) When is ATR useful? (3) How do I use ATR precisely for my work? The first chapter leads new users into the topic of ATR, OCR and HTR (Handwritten Text Recognition), deep learning, and the most prominent platforms and software. The second chapter draws up various application scenarios a student could find themselves in by showing best practices. This chapter speaks on the research aims as well as the ‘quality’ and quantity of sources. The third chapter is based on these application scenarios and shows signposts for each of the identified applications.\nChapters 2 and 3 will contain the project’s greatest contributions: The aim is to show, from the perspective of historical scholarship, how the prerequisites of the corpora used and the project’s own objective influence how we can use ATR sensibly. From a research perspective, the usefulness of using ATR in this way can be determined primarily by whether it saves time when cataloguing and reading documents. For example, if there are many different manuscripts in a corpus, it is more or less time-efficient, depending on the number of documents, to find a suitable HTR model with a low error rate or to, alternatively, train a model independently. Correction and production of ground truths is a factor to be considered. The amount of text to be transcribed plays a decisive role at this stage: If only a few pages need to be transcribed (depending on the size of the project, “a few” could be 5 or 100), a manual transcription often makes more sense. What constitutes a “good” transcription also varies depending on the research requirements of the corpus: If only individual sections (e.g., certain persons or concepts) in a larger corpus are of interest for the research question, a transcription with a character error rate (CER) of up to 10% may be sufficient to identify those text passages (e.g., with keyword spotting or full text search). Being able to quickly filter out relevant text passages from imperfectly transcribed text volumes promises a relevant expansion of the source base that can potentially be considered, even for smaller research projects. If your own research question is interested in the entire text – a close-reading – a significantly lower CER is necessary to be able to read the text. Both, when it comes to identifying relevant text passages and when analysing smaller text parts for close reading, the reading skills of ATR users are still indispensable, as specific parts of a digitized image need to be consulted. However, also here ATR can be very useful as a first step in a larger process.\nWe use a spider diagram model to communicate the various influences on the benefits of ATR for your own project. This provides a visual representation of various factors that influence work with ATR. The four factors we identified are: (1) the heterogeneity of hands, i.e. the variety of handwriting in the texts; (2) the amount of text; (3) the method, this refers to the types of analysis distant reading and close reading on a sliding scale; (4) the research question; respectively its narrowness or breadth.\n\n\n\nOur model showing identified factors for the use of ATR in historical projects\n\n\nA high heterogeneity of handwriting could affect the accuracy of ATR as the recognition software might have difficulties to recognize the text consistently. A higher degree of heterogeneity requires a broader model based on larger amount of training data.(Hodel et al. 2021) Large amounts of text often mean that ATR needs to be able to work efficiently and scalable to process large amounts of data. For small amounts of text, the focus could be on the level of detail of the recognition or more manual correction. A close reading would require a more precise and detailed analysis of the texts, which means that the ATR models must be accurate and able to recognize fine details, while distant reading focuses more on the recognition of patterns and trends. A broad research question might require a general analysis of many texts, which means that the ATR models should be versatile and robust e.g. based on TrOCR models. A specific research question might mean the ATR must focus on specific details and accurate detections. In summary, working with ATR requires a careful balance between the quantity and type of texts, the desired accuracy and detail of the analysis, and the heterogeneity of the manuscripts to be analysed. The diagram helps to organise these factors visually and to understand their interactions.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#our-difficulties",
    "href": "submissions/445/index.html#our-difficulties",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "Our difficulties",
    "text": "Our difficulties\nWe have already recognised some difficulties for our module and would like to address them briefly: The fast-moving nature of tools and products prevents us from providing precise instructions and means that we can only provide a general introduction to the technology rather than the software(-suites) themselves. Some of this software requires licence or operates on a pay-per-use basis and is therefore not a viable option for everyone. When referring to these products, our open-source teaching module also provides free advertising for paid tools. On the other hand, free tools have disadvantages, which means they are not helpful in all cases. We, therefore, must find a good balance between these two poles. In the technical realisation of our teaching module, we are limited by the options developed during the relaunch. We, therefore, must develop our teaching module within the structures of the existing options. We would furthermore like to set up a FAQ page on ATR, but this requires us to be able to collect and identify these problems and questions systematically.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#references",
    "href": "submissions/445/index.html#references",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "References",
    "text": "References\n\n\nAd Fontes. 2018. “Ad fontes. Eine Einführung in den Umgang mit Quellen im Archiv.” 2018. https://www.adfontes.uzh.ch.\n\n\nHodel, Tobias, and Michael Nadig. 2019. “Grundlagen der Mediävistik digital vermitteln: ‚Ad fontes‘, aber wie?” Das Mittelalter 24 (1): 142–56. https://doi.org/doi:10.1515/mial-2019-0010.\n\n\nHodel, Tobias, David Schoch, Christa Schneider, and Jake Purcell. 2021. “General Models for Handwritten Text Recognition: Feasibility and State-of-the Art. German Kurrent as an Example.” Journal of Open Humanities Data, July. https://doi.org/10.5334/johd.46.\n\n\nKränzle, Andreas, and Gerold Ritter. 2004. “Ad fontes. Zu Konzept, Realisierung und Nutzung eines e-Learning-Angebots. Zürich.” Dissertation, Zürich: Universität Zürich. https://www.zora.uzh.ch/id/eprint/163194/1/20050043.pdf.\n\n\nMuehlberger, Guenter, Louise Seaward, Melissa Terras, Sofia Ares Oliveira, Vicente Bosch, Maximilian Bryan, Sebastian Colutto, et al. 2019. “Transforming Scholarship in the Archives Through Handwritten Text Recognition.” Journal of Documentation 75 (5): 954–76. https://doi.org/10.1108/JD-07-2018-0114.\n\n\nUZH. 2024. “Potentials of Advanced Text Technologies: Machine Learning-Based Text Recognition (PATT).” 2024. https://www.hist.uzh.ch/de/fachbereiche/mittelalter/lehrstuehle/teuscher/forschung/projekte/PATT.html.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/474/index.html",
    "href": "submissions/474/index.html",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "",
    "text": "We live in the information age, in the information society, respectively. Scholars might disagree on when exactly it began and what its defining characteristics are, but they all agree that the flow and power of information are of unprecedented importance in our world (Castells and Castells 2000; Cortada 2002; Beniger 1997). The advent of digital information and communication technology has accelerated the pace and increased the amount of information humanity is producing, processing, and transmitting. Thus, to navigate through our world made up of information, it has become imperative to develop a sort of “digital literacy,” commonly defined as the ability to acquire information, assess its quality, and apply it to a given problem (Lankshear and Knobel 2008; Reedy and Parker 2018; Carmi and Yates 2020). This essay argues that historians can contribute significantly to the formulation of a canon in digital literacy, because their training and epistemic traditions are based on evaluating the authenticity, credibility, perspective, and context of sources. However, this paper will emphasize that a foundational understanding of the functional principles of digital information processing and basic approaches of digital forensics must be incorporated into the historian’s toolbox. It will demonstrate that the history of computing offers a path to acquire this knowledge and to disseminate it. To conclude, the paper will point out that the technological, social, political, cultural, and economic context and embeddedness of information, its production, and circulation, are fundamental for interpretation and understanding, highlighting again the favorable position for historians to play a significant part in providing orientation and critique in the information age and contributing to general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#introduction",
    "href": "submissions/474/index.html#introduction",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "",
    "text": "We live in the information age, in the information society, respectively. Scholars might disagree on when exactly it began and what its defining characteristics are, but they all agree that the flow and power of information are of unprecedented importance in our world (Castells and Castells 2000; Cortada 2002; Beniger 1997). The advent of digital information and communication technology has accelerated the pace and increased the amount of information humanity is producing, processing, and transmitting. Thus, to navigate through our world made up of information, it has become imperative to develop a sort of “digital literacy,” commonly defined as the ability to acquire information, assess its quality, and apply it to a given problem (Lankshear and Knobel 2008; Reedy and Parker 2018; Carmi and Yates 2020). This essay argues that historians can contribute significantly to the formulation of a canon in digital literacy, because their training and epistemic traditions are based on evaluating the authenticity, credibility, perspective, and context of sources. However, this paper will emphasize that a foundational understanding of the functional principles of digital information processing and basic approaches of digital forensics must be incorporated into the historian’s toolbox. It will demonstrate that the history of computing offers a path to acquire this knowledge and to disseminate it. To conclude, the paper will point out that the technological, social, political, cultural, and economic context and embeddedness of information, its production, and circulation, are fundamental for interpretation and understanding, highlighting again the favorable position for historians to play a significant part in providing orientation and critique in the information age and contributing to general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#forensic-source-criticism",
    "href": "submissions/474/index.html#forensic-source-criticism",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "Forensic Source Criticism",
    "text": "Forensic Source Criticism\nThe need for historians and archivists to engage with the methods of computer forensics is well established among those who work with digitally born objects (Ries 2022; Duranti and Endicott-Popovsky 2010; Fickers 2020). Increasingly, digitized sources and re-born digitals are retrieved via the internet and incorporated into scholarship. Therefore, “digital basics” become indispensable for evaluating the credibility and meaning of a given source (Milligan 2019, 241). Learning to deal with digital objects and their specific qualities, elements, and characteristics is as indispensable for historians of the information age as learning the scriptures and languages of the past for historians of the pre-modern period. As Trevor Owens and Thomas Padilla stated: “In much the same way that a historian who studies eighteenth-century documents needs to learn to read various kinds of handwriting scripts to develop an ability to read and decipher those texts, historians are going to need to develop sophisticated understandings of how digital media systems functioned at particular points in time and how different kinds of users used them” (Owens and Padilla 2021, 12). Arguing for the importance of forensic methods for historical inquiry and source criticism, Thorsten Ries has stated: “If historians are to critically appraise primary sources and establish the circumstances of their creation, provenance, processing history, so as to facilitate the identification of forgeries, fakes and disinformation, it is essential to explore the forensic history of the material creation of these records” (Ries 2022, 184).\nThe most basic and essential skills of forensic source criticism for historians working on born-digital objects include overcoming “screen essentialism,” the ability to retrieve and interpret metadata, knowledge of encoding formats and their meaning, and the ability to read and understand code. Overcoming “screen essentialism” (Owens 2018, 46) means acknowledging that the interfaces through which we interact with computing devices should be understood as performances. Interfaces themselves are complex interactions between various programs, routines, hardware, and software, not only the displays, screens, input-output devices, and peripherals allowing us to control the operations of the computer. They are designed to free the user’s mind from thinking about all of these underlying functions, enabling her to focus on her specific task. The downside of this comfortable arrangement is that the user loses sight of many preconfigured decisions on how to process, render, and display data. “Screen essentialism” refers to the tendency to take “what you see for what there is.” Overcoming it means understanding that the visual impression we get on a given system is just one of many possible others. Think of resolution and colors on a basic level, think of the difference between a text-editor on the one hand and an integrated development environment on the other. Being able to distinguish between the “performative” elements of the display and the core properties of a digital object is an essential part of digital literacy. The properties characterizing a digital object are often stored in its metadata.\n“Metadata is our friend,” James Baker wrote (Baker 2019). Indeed, metadata, i.e., descriptive data about digital objects attached to them by the system that produced the object or data, can contain valuable information for answering some of the most important questions of source criticism: date of production, authorship, size, format, and some more or less useful properties. Knowing where to find metadata of files and objects and checking whether or not they are consistent with the content of the data is thus a basic and pivotal first step in evaluating born-digital sources. An example would be to check if the metadata in a text file about the time of creation and last modification is in line with what it claims to report and from what perspective. However, to substantiate such evaluations based on metadata, historians also need to know how metadata can be misleading or manipulated. The date-time-stamp attached to each file, for example, is automatically generated by the operating system. This again is dependent on the configuration of the system time and the time zone in the system’s settings and can be changed (Baker 2019).\nFurther important insights into the characteristics of any given digital object can be derived from some understanding of the principles of encoding and formats. Basically, all digital objects are comprised of two distinct bits (1 and 0), but there is an infinite number of ways to encode information based on the binary representation of signals. Text or letters, respectively, can be encoded in different ways: there is the Morse/alphabet, for example, which uses only short and long signals; there are modern and widely used encoding schemes for letters like ASCII or UTF/8, which supports also non-western characters (Pargman and Palme 2009). The same goes for numbers, which can be represented in binary, hexadecimal, or any nested encoding. Images can be represented in various ways, depending on the way the distribution of black, white, or colored pixels in a grid are encoded (Dourish 2017). To understand and critically read the encoding of any digital object is to acknowledge and scrutinize the choices of the significant properties and their representation determined by the respective encoding format.\nFinally, an integral part of digital literacy is a basic understanding of algorithms and code. Here again, the history of computing serves as an introduction and explanation at the same time. In his book “Computer Power and Human Reason” from 1976, computing pioneer Joseph Weizenbaum provides a simple explanation of the principle of a Turing machine, demonstrating that there is no functional difference between data and processing instruction, because they are equally codified in binaries and stored in the same memory. Almost in passing, he introduces the concept of giving human-readable and easy-to-memorize names for the instruction, like “STORE,” “GET,” etc., thereby conveying Assembler language to his readers (Weizenbaum 1976). With these concepts in mind, it is simple to understand that even different higher programming languages employ a similar set of basic concepts and instructions, such as functions, values, arguments, loops, conditional statements, and so on. These basics, which are available in countless introductory chapters and tutorials all over the internet, are sufficient to follow the arguments made by proponents of critical code studies on single lines of code or longer programs (Marino 2020; Krajewski 2020; Jaton and Bowker 2020; Montfort 2014).\nDigital literacy and source criticism of born-digital objects employing basic concepts of forensics, therefore, aims to understand the logical and functional location within and relations to its environment and operating system, because such objects can neither exist nor can they be understood outside of these relationships. How can these insights be made productive for source criticism, i.e., for evaluating the integrity and authenticity of a born-digital object? One such application is scrutinizing a given file’s integrity by comparing different versions of it. Most systems automatically produce backup copies of each file and also store temporary versions while the file is in use. These previous and alternative versions are often either invisible in the contents of a given directory as displayed by the common file managers of personal computers. In addition, such files are often stored within an application’s directory instead of the directory the user is working on (Kirschenbaum 2012). If a copy or a previous version of a file can be located, assumptions about its coherence and originality are possible. Even without the ability to open or read a file, comparing its size and the one of the previous version can be revealing. If a copy and the original of a given file are truly identical, it can be verified by comparing the hash-sum of both files. There are numerous open-source tools and instructions to do that available also to novices (Altheide and Carvey 2011; Hosmer and Kessler 2014).\nAnother approach to source criticism inspired by digital forensics is to read between the lines, or “between the bits,” more precisely. Thorsten Ries, for example, has demonstrated that text files can contain much more than they might reveal at first sight, i.e., overcoming the perspective of screen essentialism (Ries 2022, 176–78). In his examples, he reads the Revision Identifier for Style Definition (RSID) automatically attached to each MS Word file to make statements about a file’s creation and revision history. Similarly, Trevor Owens has demonstrated that important information about a file’s history can be retrieved simply by changing its file-type extension and thus opening it with different software (Owens 2018, 43). In Owens’ example, he opens an .mp3 music file with a simple text editor by changing the extension to .txt, which enables him literally to “read” all of the file’s metadata. Depending on the scheme employed by the file managing system, this reading “against the grain” might reveal information that is not accessible with a simple right-click. Similarly, it might be worth a try to open a file of unknown format with the vim editor for a first inspection.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#system-and-environment-contextualizing-digital-objects",
    "href": "submissions/474/index.html#system-and-environment-contextualizing-digital-objects",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "System and Environment: Contextualizing digital objects",
    "text": "System and Environment: Contextualizing digital objects\nAll elements of the expanded and updated version of source criticism outlined above point to an increased attention towards the systems and environments into which the production and processing of digital objects are embedded. On a basic level, computation always relies on specific logical, material, and technical systems and environments, i.e., the operating system, hardware, storage media, exchange formats, transmission protocols, etc. Inspired by platform studies and the “new materialism,” recent research on digital objects has emphasized the platform character of all digital media and objects and argued for their understanding as “assemblages” (Owens 2018; Zuanni 2021). This line of research emphasizes the multiple relations and dependencies of all digital objects to systems and environments. All data has to be organized according to certain file formats and standards to be transferable and processable; file formats require specific applications to be read and manipulated; applications and programs, in turn, rely on operating systems, which again are bound to specific hardware configurations and must be maintained and updated, and so on. With networked computing, web-based applications, and cloud storage, complex and nested platforms and assemblages have become the norm. Consequently, any concept of digital literacy or data literacy must incorporate critical reflection on the relations, dependencies, and determinations of systems and infrastructure (Gray, Gerlitz, and Bounegru 2018). This is in line with recent research in science and technology studies and the materialistic turn in the history of computing, which center on connectivity and reliance on large and complex infrastructure networks in their studies (Parks and Starosielski 2015; Galloway and Thacker 2014; Edwards et al. 2009). Here again, following the historical unfolding and development of these infrastructures helps to understand both their general functionality and their specifics, which are sometimes more the result of traditions and path dependencies than of technical necessity.\nIn the same way that historians trace back the provenance, perspective, and implicit presuppositions of a “classic” paper-based source, they must reflect on the system-environment of a digital object, its relations to it, its location within it, and the epistemic consequences of that positionality and relations.\nTheorizing characteristics of born-digital sources, Chiara Zuanni illustrates this positionality within technological assemblages with the example of a social media post: “Provenance might refer in the first instance to the author of a post, but it can also be traced to the data center hosting the specific content (thinking about where the content is written on a server, its forensic origin), reflecting the ways the post has traveled through the infrastructure, e.g., from a personal device to a server, and has subsequently been queried by its viewer. The agency of assemblage is therefore critical in delivering content through its material infrastructure. This agency leads to the circulation of information, a global participation in events and cultural trends, and the environmental and economic impacts of the infrastructure” (Zuanni 2021, 189).\nThis quote touches upon a different meaning of the term “environment,” referring to the ecological consequences of data processing and transmission and the necessary infrastructures. The impact and environmental costs of data transfer and routing via the internet are difficult to evaluate, but the energy needs and the direct and indirect results of environmental destruction are enormous (Pasek, Vaughan, and Starosielski 2023). Similarly, the term “system” can be employed to describe the socio-economic, political, and cultural configurations and power structures in which the production of computing software and hardware as well as data-driven knowledge are organized and enforced. While proponents of “new materialism” have convincingly demonstrated that the “cloud” is actually a very material and manifest assemblage consisting of data centers and routing and transmission infrastructures, the term “smart technologies” suggests that data processing is a predominantly automatized and de-humanized affair, omitting the work of humans at each and every level and corner of the information society: from underpaid female workers assembling processors to click-workers around the world training algorithms and normalizing data, to the often unremarked and uncredited specialists maintaining and repairing the systems we take for granted. Even Artificial Intelligence, the very symbol of working and thinking without human involvement, actually consists of a lot of human labor (Mullaney et al. 2021).\nIncluding context, systems, and environment into the analysis and reflection of computing and born-digital objects is therefore at the same time a productive research agenda for the history of computing and an approach to an updated variant of source criticism and general digital literacy. Historians, trained to contextualize and situate information provided by sources within specific historic, social, cultural, and spatial contexts, can apply their instruments of critique and evaluation easily to digital objects and additionally provide guidance to the formulation of a general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#contextualization-and-critique",
    "href": "submissions/474/index.html#contextualization-and-critique",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "Contextualization and Critique",
    "text": "Contextualization and Critique\nThis essay has so far argued that historians already have some valuable methods and approaches at their disposal to adapt their inquiries to novel, digital-born artifacts and media, and that they need to incorporate knowledge about the basic principles of computing and its history into their toolbox to be able to make sense of new media and archives. However, it is pivotal to keep in mind that providing interpretation and critique remains their core task. Decoding a digital artifact, tracing the history of its emergence, and understanding its relation to its technical environment serve but one objective: to make claims and arguments about its meaning. This is and remains a fundamentally critical approach that does not exclude a reflection on the methods themselves. Zack Lischer-Katz, for example, reminds us that digital forensics were not developed by and for historians, but serve a specific task in police investigations and the courtroom: “However, caution must be exercised when considering forensics as a guiding approach to archives. The epistemological basis of forensic science embeds particular assumptions about knowledge and particular systems of verification and evidence that are based on hierarchical relations of power, positivist constructions of knowledge, and the role of evidence […] A critical approach to the tools of digital forensics by archivists and media scholars requires thinking through how the forensic imagination may impose forms of knowing that reproduce particular power relations” (Lischer-Katz 2016, 5–6).\nAt a very basic level, historians and humanists, in general, are particularly strong exactly when their findings are more than just an addition and re-arrangement of available information. Using the distinctions between symbols and signals, Berry and colleagues have formulated an eloquent reminder of this task: “Digital humanists must address the limits of signal processing head-on, which becomes even more pressing if we also consider another question brought about by the analogy to Shannon and Weaver’s model of communication. The sender-receiver model describes the transmission of information. The charge of the digital humanities is, instead, the production of knowledge. An uncritical trust in signal processing becomes, from this perspective, quite problematic, insofar as it can confuse information for knowledge, and vice versa. […] Neither encoding or coding (textual analysis) is in fact a substitute for humanistic critique (understood in the broad sense)” (Berry et al. 2019).\nCritique is and must remain the central concern of historians. This critique must be directed to the authenticity and credibility of born-digital objects and the systems that produce them. To do so, they must learn from the tools and approaches of computer forensics. But what distinguishes historians from the forensic experts is that they don’t stop at the limits of the technical systems but extend their contextualization to the broader cultural, economic, and social structures that enable the development of specific technologies. This is why the historian’s perspective and approach are indispensable for general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#references",
    "href": "submissions/474/index.html#references",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "References",
    "text": "References\n\n\nAltheide, Cory, and Harlan A. Carvey. 2011. Digital Forensics with Open Source Tools. Burlington, MA: Syngress.\n\n\nBaker, James. 2019. “Digital Forensics in the House of Lords: Six Themes Relevant to Historians.” Software Sustainability Institute. https://www.software.ac.uk/blog/digital-forensics-house-lords-six-themes-relevant-historians-part-one.\n\n\nBeniger, James Ralph. 1997. The Control Revolution: Technological and Economic Origins of the Information Society. 5. print. Cambridge, Mass.: Harvard Univ. Press.\n\n\nBerry, David M., Beatrice M. Fazi, Ben Roberts, and Alan Webb. 2019. “No Signal Without Symbol: Decoding the Digital Humanities.” In Debates in the Digital Humanities 2019, edited by Matthew K. Gold and Lauren F. Klein, 61–74. University of Minnesota Press. https://doi.org/10.5749/j.ctvg251hk.\n\n\nCarmi, Elinor, and Simeon J. Yates. 2020. “What Do Digital Inclusion and Data Literacy Mean Today?” Internet Policy Review 9 (2). https://doi.org/10.14763/2020.2.1474.\n\n\nCastells, Manuel, and Manuel Castells. 2000. The Rise of the Network Society. 2nd ed. Information Age, v. 1. Oxford ; Malden, Mass: Blackwell Publishers.\n\n\nCortada, James W. 2002. Making the Information Society: Experience, Consequences, and Possibilities. Upper Saddle River, NJ: Prentice Hall.\n\n\nDourish, Paul. 2017. The Stuff of Bits: An Essay on the Materialities of Information. Cambridge, Massachusetts: The MIT Press.\n\n\nDuranti, Luciana, and Barbara Endicott-Popovsky. 2010. “Digital Records Forensics: A New Science and Academic Program for Forensic Readiness.” Journal of Digital Forensics, Security and Law. https://doi.org/10.15394/jdfsl.2010.1075.\n\n\nEdwards, Paul, Geoffrey Bowker, University of Pittsburgh, Steven Jackson, University of Michigan, Robin Williams, and University of Edinburgh. 2009. “Introduction: An Agenda for Infrastructure Studies.” Journal of the Association for Information Systems 10 (5): 364–74. https://doi.org/10.17705/1jais.00200.\n\n\nFickers, Andreas. 2020. “Update für die Hermeneutik. Geschichtswissenschaft auf dem Weg zur digitalen Forensik?” https://doi.org/10.14765/ZZF.DOK-1765.\n\n\nGalloway, Alexander, and Eugene Thacker. 2014. “Protokoll, Kontrolle Und Netzwerke.” In Big Data. Analysen Zum Digitalen Wnadel von Wissen, Macht Und Ökonomie, edited by Ramón Reichert, 289–311. Bielefeld: Transcript.\n\n\nGray, Jonathan, Carolin Gerlitz, and Liliana Bounegru. 2018. “Data Infrastructure Literacy.” Big Data & Society 5 (2): 1–13. https://doi.org/10.1177/2053951718786316.\n\n\nHosmer, Chet, and Gary C. Kessler. 2014. Python Forensics: A Workbench for Inventing and Sharing Digital Forensic Technology. Amsterdam Boston Paris: Elsevier Syngress.\n\n\nJaton, Florian, and Geoffrey C. Bowker. 2020. The Constitution of Algorithms: Ground-Truthing, Programming, Formulating. Inside Technology. Cambridge, Massachusetts London: The MIT Press.\n\n\nKirschenbaum, Matthew G. 2012. Mechanisms: New Media and the Forensic Imagination. 1. paperback ed. Cambridge, Mass: MIT Press.\n\n\nKrajewski, Markus. 2020. “’Branch’, ’Diff’, ’Merge’. Versionskontrolle Und Quellcodekritik.” In Duplikat, Abschrift & Kopie. Kulturtechniken Der Vervielfältigung, edited by Jörg Paulus, Andrea Hübner, and Fabian Winter, 21–40. Wien: Böhlau.\n\n\nLankshear, Colin, and Michele Knobel, eds. 2008. Digital Literacies: Concepts, Policies and Practices. New Literacies and Digital Epistemologies, vol. 30. New York: Peter Lang.\n\n\nLischer-Katz, Zack. 2016. “Studying the Materiality of Media Archives in the Age of Digitization: Forensics, Infrastructures and Ecologies.” First Monday, December. https://doi.org/10.5210/fm.v22i1.7263.\n\n\nMarino, Mark C. 2020. Critical Code Studies: Initial Methods. Software Studies. Cambridge, Massachusetts: The MIT Press.\n\n\nMilligan, Ian. 2019. History in the Age of Abundance?: How the Web Is Transforming Historical Research. Montreal: McGill-Queen’s University Press.\n\n\nMontfort, Nick. 2014. 10 PRINT CHR$(205.5+RND(1))[semi-Colon] [Colon] GOTO 10. Cambridge: The MIT Press.\n\n\nMullaney, Thomas S., Benjamin Peters, Mar Hicks, and Kavita Philip, eds. 2021. Your Computer Is on Fire. Cambridge, Massachusetts ; London England: The MIT Press.\n\n\nOwens, Trevor. 2018. The Theory and Craft of Digital Preservation. Baltimore: Johns Hopkins University Press.\n\n\nOwens, Trevor, and Thomas Padilla. 2021. “Digital Sources and Digital Archives: Historical Evidence in the Digital Age.” International Journal of Digital Humanities 1 (3): 325–41. https://doi.org/10.1007/s42803-020-00028-7.\n\n\nPargman, Daniel, and Jacob Palme. 2009. “ASCII Imperialism.” In Standards and Their Stories : How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life, edited by Susan Leigh Star and Martha Lampland, 177–299. Ithaca: Cornell University Press.\n\n\nParks, Lisa, and Nicole Starosielski, eds. 2015. Signal Traffic: Critical Studies of Media Infrastructures. The Geopolitics of Information. Urbana: University of Illinois Press.\n\n\nPasek, Anne, Hunter Vaughan, and Nicole Starosielski. 2023. “The World Wide Web of Carbon: Toward a Relational Footprinting of Information and Communications Technology’s Climate Impacts.” Big Data & Society 10 (1): 205395172311589. https://doi.org/10.1177/20539517231158994.\n\n\nReedy, Katharine, and Jo Parker, eds. 2018. Digital Literacy Unpacked: 1st ed. Facet. https://doi.org/10.29085/9781783301997.\n\n\nRies, Thorsten. 2022. “Digital History and Born-Digital Archives: The Importance of Forensic Methods.” Journal of the British Academy 10: 157–85. https://doi.org/10.5871/jba/010.157.\n\n\nWeizenbaum, Joseph. 1976. Computer Power and Human Reason: From Judgment to Calculation. San Francisco: Freeman.\n\n\nZuanni, Chiara. 2021. “Theorizing Born Digital Objects: Museums and Contemporary Materialities.” Museum and Society 19 (2): 184–98. https://doi.org/10.29311/mas.v19i2.3790.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/427/index.html",
    "href": "submissions/427/index.html",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "",
    "text": "Nowadays, software is a central component of every research project. Since the establishment of personal computers digital tools are used for a wide range of tasks, from simple text processing to machine assisted recognition in all sorts of historical documents. Research projects however, in particular those that produce digital scholarly editions, rarely rely just on existing tools, they often create new ones. Starting from the development or customization of own data formats ending with the implementation of often complex web applications for presentation, it is not uncommon for the tools developed in this context to be ‘quick hacks’ rather than well-designed software projects.1 In many cases, this is not a problem at all, because the duration of research projects in the humanities is often rather short (e.g. between three and six years). Software developed in such a short amount of time must first and foremost achieve the project’s goals, and therefore adaptation to other subjects or subsequent use is usually not intended. However, this becomes a problem if the corresponding research project is scheduled for a longer term, or if it is part of a series of projects depending on each other. In this case, quick solutions often become serious issues and are not really FAIR for either internal or external subsequent use. Not least for this reason, this phenomenon is the subject of discussion in the digital humanities community under the heading of research software engineering.2 This paper describes practical experiences from the perspective of a long-term editorial project and explores opportunities for sustainable development practices by utilizing modern methods that have long been established outside the academic world.\n\n\n\nThe Collection of Swiss Law Sources (SLS) is a 120 year old research project that publishes Swiss legal texts in German, French, Latin, Italian and Romansh from the 6th to the 18th century. The edited texts are published in a printed reference publication and in digital form.3 By the time of writing ten edition projects are currently carried out by 23 researchers in three languages throughout Switzerland: In French, volumes are to be published in the cantons of Geneva (1 vol.), Vaud (2 vols.) Neuchatel (1 vol.), Fribourg (1 vol.); in German Valais (1 vol.), Lucerne (2 vols.), Schaffhausen (2 vols.), St. Gallen (1 vol.), Graubünden (1 vol.) and in Italian Ticino (1 vol.). Further editions projects are planned or applied for, while the overall project is scheduled for another ~ 50 years. The entire technical infrastructure is provided and developed by the SLS core team, which consists of the project manager and two members of staff specializing in DH (the authors of this paper). This team is also responsible for coordinating the projects, processing the data, typesetting the printed volumes and digital publishing of the edited texts.\nIn this context, the development of new and the improvement of already existing software is not only a technical challenge, but also an organizational one. Existing applications must run continuously to provide the researchers with the tools they need for their daily work (and to grant the users of the digital edition access to all information), while new requirements must be met on an ongoing basis as each project deals with unique documents.\n\n\n\nAbout 15 years ago the Swiss Law Foundation, which stands behind the SLS, decided to retro-digitize the over hundred volumes published up to that point. Since then, the results of these initial digitization efforts have been presented in a web application which, as a ‘browsing machine’, makes the results of the many years of editing work, previously locked between two book covers, available to a broad public. This also marked the start of the project’s transition to a predominantly digital editing and working method. In these 15 years numerous (web) applications have been created: These include databases that collate information on historical entities (people, organizations, places and terms), a digital application that presents the transcriptions, now encoded in TEI-XML, in both a web and a print view and a lot of other tools used in the various tasks at hand. The ongoing nature of the project was one of the reasons why many of these applications were often ‘ad hoc solutions’ or proof of concepts that were neither designed for long-term operation nor for integration—i.e. collaboration—with other tools. As a result, a rather diverse ecosystem of different technologies has developed on the data side and on the processing and presentation side.4",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#introduction",
    "href": "submissions/427/index.html#introduction",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "",
    "text": "Nowadays, software is a central component of every research project. Since the establishment of personal computers digital tools are used for a wide range of tasks, from simple text processing to machine assisted recognition in all sorts of historical documents. Research projects however, in particular those that produce digital scholarly editions, rarely rely just on existing tools, they often create new ones. Starting from the development or customization of own data formats ending with the implementation of often complex web applications for presentation, it is not uncommon for the tools developed in this context to be ‘quick hacks’ rather than well-designed software projects.1 In many cases, this is not a problem at all, because the duration of research projects in the humanities is often rather short (e.g. between three and six years). Software developed in such a short amount of time must first and foremost achieve the project’s goals, and therefore adaptation to other subjects or subsequent use is usually not intended. However, this becomes a problem if the corresponding research project is scheduled for a longer term, or if it is part of a series of projects depending on each other. In this case, quick solutions often become serious issues and are not really FAIR for either internal or external subsequent use. Not least for this reason, this phenomenon is the subject of discussion in the digital humanities community under the heading of research software engineering.2 This paper describes practical experiences from the perspective of a long-term editorial project and explores opportunities for sustainable development practices by utilizing modern methods that have long been established outside the academic world.\n\n\n\nThe Collection of Swiss Law Sources (SLS) is a 120 year old research project that publishes Swiss legal texts in German, French, Latin, Italian and Romansh from the 6th to the 18th century. The edited texts are published in a printed reference publication and in digital form.3 By the time of writing ten edition projects are currently carried out by 23 researchers in three languages throughout Switzerland: In French, volumes are to be published in the cantons of Geneva (1 vol.), Vaud (2 vols.) Neuchatel (1 vol.), Fribourg (1 vol.); in German Valais (1 vol.), Lucerne (2 vols.), Schaffhausen (2 vols.), St. Gallen (1 vol.), Graubünden (1 vol.) and in Italian Ticino (1 vol.). Further editions projects are planned or applied for, while the overall project is scheduled for another ~ 50 years. The entire technical infrastructure is provided and developed by the SLS core team, which consists of the project manager and two members of staff specializing in DH (the authors of this paper). This team is also responsible for coordinating the projects, processing the data, typesetting the printed volumes and digital publishing of the edited texts.\nIn this context, the development of new and the improvement of already existing software is not only a technical challenge, but also an organizational one. Existing applications must run continuously to provide the researchers with the tools they need for their daily work (and to grant the users of the digital edition access to all information), while new requirements must be met on an ongoing basis as each project deals with unique documents.\n\n\n\nAbout 15 years ago the Swiss Law Foundation, which stands behind the SLS, decided to retro-digitize the over hundred volumes published up to that point. Since then, the results of these initial digitization efforts have been presented in a web application which, as a ‘browsing machine’, makes the results of the many years of editing work, previously locked between two book covers, available to a broad public. This also marked the start of the project’s transition to a predominantly digital editing and working method. In these 15 years numerous (web) applications have been created: These include databases that collate information on historical entities (people, organizations, places and terms), a digital application that presents the transcriptions, now encoded in TEI-XML, in both a web and a print view and a lot of other tools used in the various tasks at hand. The ongoing nature of the project was one of the reasons why many of these applications were often ‘ad hoc solutions’ or proof of concepts that were neither designed for long-term operation nor for integration—i.e. collaboration—with other tools. As a result, a rather diverse ecosystem of different technologies has developed on the data side and on the processing and presentation side.4",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#data-as-a-solid-ground-developing-an-xml-schema-for-a-scholarly-edition",
    "href": "submissions/427/index.html#data-as-a-solid-ground-developing-an-xml-schema-for-a-scholarly-edition",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "Data as a solid ground: developing an XML Schema for a scholarly edition",
    "text": "Data as a solid ground: developing an XML Schema for a scholarly edition\nThe foundation of a digital scholarly edition is undoubtedly the transcribed and annotated data, which usually is encoded in an XML format.5 All our newly edited texts are encoded in XML and as time permits all previously edited texts will be converted to this format. Therefore all further application layers, such as web presentation or printed output, have to be based on these XML files according to the single source principle. Over the last two decades, the guidelines of the Text Encoding Initiative (TEI)6 have established themselves as the de facto standard for this markup work. These guidelines are primarily a broad collection of suggestions rather than a clear set of rules, necessitating a precise formulation of philological concepts into a logical data model, specifically the creation of a TEI subset as an XML schema. The TEI itself offers a format called ODD (One Document Does it all) for creating an XML schema in a literate programming-fashion7, which itself is TEI-XML.8\nA schema’s main use case is validation, i.e. checking whether the XML data corresponds to certain structures and constraints. As a TEI subset it defines which components and elements provided by the TEI guidelines are used and how they are used, making it an important part of the editing concept itself. The validation against a schema ensures the consistency of the resulting data sets in an ongoing project and is necessary to continuously support and check the researchers during the transcription and annotation process. Therefore we regard an XML schema as a key software component, although the development of a schema is typically not understood as software development in the true sense of the word. This is probably one of the reasons why most of the modern engineering practices we want to demonstrate are not yet applied in this field (at least as far as we know).\n\nFour modern engineering practices and their application\nIn order to deal with a complex situation, as described above, the authors of this paper propose to make use of the following software engineering practices9:\n\nmodular software development\ntest driven development\nsemantic versioning\nsemiautomatic documentation\n\nThe development of the XML schema used in our project will be used as an example to show how these practices can be utilized for digital humanities projects. In the context of the ongoing reworking of the SLS application landscape, we developed a test based and modular workflow (see Figure 1) for the creation of a new schema, based on ODD-files as input.10\n\n\n\n\n\n\nFigure 1: Test and build pipeline of a modern schema development workflow\n\n\n\n\n\nModular software development\nIf you download a sample ODD file from the TEI homepage11 which contains all elements and components, such a file may be made up of 70000 lines of code. Our ODD file—which is just a limited subset—still contains way over 20000 lines of code. The first step to handle such a large and complex object is to split it into manageable pieces. For each TEI-element we need, we created a separate file containing the element’s specification. Common parts like attribute classes, data types or custom definitions that are used by multiple elements each went into their own files.\nA rather simple specification for the element &lt;pc/&gt; may look like this:\n&lt;elementSpec\n  xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:rng=\"http://relaxng.org/ns/structure/1.0\" ident=\"pc\" module=\"analysis\" mode=\"change\"&gt;\n  &lt;desc xml:lang=\"en\" versionDate=\"2024-04-30\"&gt;\n    Contains a punctuation mark, which is processed specially\n    considering linguistic regulations (for example, by adding a space).\n  &lt;/desc&gt;\n  &lt;classes mode=\"replace\"/&gt;\n  &lt;content&gt;\n    &lt;rng:data type=\"string\"&gt;\n      &lt;rng:param name=\"pattern\"&gt;[;:?!]&lt;/rng:param&gt;\n    &lt;/rng:data&gt;\n  &lt;/content&gt;\n  &lt;attList&gt;\n    &lt;attDef ident=\"force\" mode=\"delete\"/&gt;\n    &lt;attDef ident=\"unit\" mode=\"delete\"/&gt;\n    &lt;attDef ident=\"pre\" mode=\"delete\"/&gt;\n  &lt;/attList&gt;\n&lt;/elementSpec&gt;\nThis principle of atomicity enforces a clear structure, provides better maintainability in the future, made the files way more easy to grasp and to modify and had also the benefit of reducing redundancy, because shared parts were refactored and can be used throughout the schema while being defined in one place. The downside to this, of course, is the need to compile all those files into one ODD in a separate step. But this may be a small price to pay for the benefits.\n\n\nTest driven development (TDD)12\nThe second step was to define a set of tests for all element, attribute and datatype definitions.13 Each test set describes the expected behavior of a piece of the schema and consists of three components: a title of the test set, the markup being tested and the expected result, which can either be valid (True) or invalid (False). Each test set is executed and evaluated by a Python function which invokes an XML-Parser.\nThe following tests describe the contents and attributes of the element &lt;pc/&gt;.\n@pytest.mark.parametrize(\n    \"name, markup, result\",\n    [\n        (\n            \"valid-pc\",\n            \"&lt;pc&gt;;&lt;/pc&gt;\",\n            True,\n        ),\n        (\n            \"invalid-pc-with-wrong-char\",\n            \"&lt;pc&gt;-&lt;/pc&gt;\",\n            False,\n        ),\n        (\n            \"invalid-pc-with-attribute\",\n            \"&lt;pc unit='c'&gt;;&lt;/pc&gt;\",\n            False,\n        ),\n    ],\n)\ndef test_pc(\n    test_element_with_rng: RNG_test_function,\n    name: str,\n    markup: str,\n    result: bool,\n):\n    test_element_with_rng(\"pc\", name, markup, result, False)\nIf each specification is coupled with one or more tests, it is guaranteed that individual changes to the schema will not compromise the overall functionality and possible side-effects may be detected early on. Such test cases are abstract enough to enable representative testing of the software components to be developed, but at the same time concrete enough to make them readable for employees specializing in philology, thus they can be used as a means of communication between the digital humanist team and the philological or historical team. We can simply ask: Should this piece of XML be True or False?\n\n\nSemiautomatic documentation\nThe schema has to be documented for those who use it to encode the files as well as for those who use the files for any other purpose. We decided to generate as much of this documentation automatically as possible using markdown as a language and a site generator called MkDocs14. Our documentation website15 is constructed like this: A self written Python program reads all parts of the schema, converts them to markdown files and hands those to the MkDocs processor which returns a static HTML webpage that can easily be accessed and searched.\n\n\nSemantic versioning (SemVer and git)\nIt is obvious that each change to the schema not only affects the XML files to be validated16, but also changes the documentation. For this purpose every release of the schema is versioned with git and is reflected in a new corresponding build of the documentation site. All versions of the schema are named in accordance to the principles of semantic versioning17 so a user of any XML file that has to be validated against our schema can see which versions are available and is able to read a specific documentation for any schema version.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#a-brief-outlook",
    "href": "submissions/427/index.html#a-brief-outlook",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "A brief outlook",
    "text": "A brief outlook\nAlthough our journey of refactoring has just begun, we are already seeing the benefits of the principles we have applied. If the ground your are standing on is a solid one, you can build on it. Currently, we are working on a multilingual translation of our schema from German as the main language to English, French and Italian and hope to enrich the schema with extensive examples from actual XML files. Furthermore, we are rewriting the existing rendering-mechanisms (e.g. TEI to HTML), applying the same rules as described above. All in all, the work done and the cost we had to pay for is already paying off.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#references",
    "href": "submissions/427/index.html#references",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "References",
    "text": "References\n\n\nAlsaqqa, Samar, Samer Sawalha, and Heba Abdel-Nabi. 2020. “Agile Software Development: Methodologies and Trends.” International Journal of Interactive Mobile Technologies (iJIM) 14 (11): 246–70. https://doi.org/10.3991/ijim.v14i11.13269.\n\n\nBurghardt, Manuel, and Claudia Müller-Birn. 2019. “Software Engineering in Den Digital Humanities 2. Workshop Der Fachgruppe Informatik Und Digital Humanities (InfDH).” In 50 Jahre Gesellschaft Für Informatik - Informatik Für Gesellschaft Workshopbeiträge Der 49. Jahrestagung Der Gesellschaft Für Informatik: 23.-26.9.2019, Kassel, Deutschland, 75. Proceedings, volume 295. Bonn Gesellschaft für Informatik e.V. [2019].\n\n\nCarver, Jeffrey C., Nic Weber, Karthik Ram, Sandra Gesing, and Daniel S. Katz. 2022. “A Survey of the State of the Practice for Research Software in the United States.” PeerJ Computer Science 8 (May): e963. https://doi.org/10.7717/peerj-cs.963.\n\n\nChristie, Tom. 2024. “MkDocs. Project Documentation with Markdown.” 2024. https://www.mkdocs.org.\n\n\nHaaf, Susanne, and Christian Thomas. 2016. “Enabling the Encoding of Manuscripts Within the DTABf: Extension and Modularization of the Format.” Journal of the Text Encoding Initiative, December. https://doi.org/10.4000/jtei.1650.\n\n\nKnuth, Donald Ervin. 1992. Literate Programming. CSLI Lecture Notes, no. 27. Stanford, Calif.: Center for the Study of Language; Information.\n\n\nLaw Sources Foundation of the Swiss Lawyers Society. 2024a. “Collection of Swiss Law Sources Online. Editio.” 2024. https://editio.sls-online.ch.\n\n\n———. 2024b. “Transkriptionsrichtlinien Und Dokumentation. SSRQ Dokumentation.” 2024. https://schema.ssrq-sds-fds.ch/latest/.\n\n\nMartin, Robert C., ed. 2009. Clean Code: A Handbook of Agile Software Craftsmanship. Upper Saddle River, NJ: Prentice Hall.\n\n\nNeuber, Frederike. 2023. “Der Digitale Editionstext. Technologische Schichten, ‚Editorischer Kerntext‘ Und Datenzentrierte Rezeption.” In Der Text Und Seine (Re)produktion, edited by Niklas Fröhlich, Bastian Politycki, Dirk Schäfer, and Annkathrin Sonder, 55:69–84. Beihefte Zu Editio. Berlin/Boston.\n\n\nPolitycki, Bastian, Christian Sonder, and Pascale Sutter. 2024. “TEI-XML Schema Der Sammlung Schweizerischer Rechtsquellen.” https://doi.org/10.5281/zenodo.10625840.\n\n\nPorter, Dot. 2024. “What Is an Edition Anyway? My Keynote for the Digital Scholarly Editions as Interfaces Conference, University of Graz.” July 25, 2024. http://www.dotporterdigital.org/what-is-an-edition-anyway-my-keynote-for-the-digital-scholarly-editions-as-interfaces-conference-university-of-graz/.\n\n\nPreston-Werner, Tom. 2023. “Semantic Versioning 2.0.0.” 2023. https://semver.org.\n\n\nText Encoding Initiative. 2024a. “Guidelines. TEI: Text Encoding Initiative.” 2024. https://tei-c.org/release/doc/tei-p5-doc/en/html/index.html.\n\n\n———. 2024b. “Roma. TEI: Text Encoding Initiative.” 2024. https://roma.tei-c.org.\n\n\nZundert, Joris van, and Tara Andrews. 2018. “What Are You Trying to Say? The Interface as an Integral Element of Argument.” In Digital Scholarly Editions as Interfaces, 3–33. Norderstedt.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#footnotes",
    "href": "submissions/427/index.html#footnotes",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCarver et al. recently demonstrated this with a survey, which shows that many researchers developing software have never received training in software development, and best practices are often ignored. See Carver et al. (2022).↩︎\nSee Manuel Burghardt and Claudia Müller-Birn organised a workshop specifically on this topic at the 50th Annual Conference of the German Informatics Society, see Burghardt and Müller-Birn (2019).↩︎\nSee Law Sources Foundation of the Swiss Lawyers Society (2024a) for the web presentation.↩︎\nThe edited texts themselves are available as PDF (the retro-digitized collection), TeX and FileMaker (transition phase) and TEI-XML (current projects). These are processed by scripts and applications in the programming languages Perl, OCaml, Python, JavaScript and XQuery. Relational as well as graph-based and document-orientated databases are used to store the entity data.↩︎\nThere have been various discussion what’s the key value of a digital scholarly digition. Maybe it’s the data (see Porter (2024)) or it’s the interface (see Zundert and Andrews (2018)). In the recent time it’s becoming more and more clear, it could be both. Therefore models have been developed, which understand scholarly editions as a stack of data, the processing applied to it and the resulting presentation (see Neuber (2023), p. 71).↩︎\nFor details see Text Encoding Initiative (2024a).↩︎\nThe term literate programming usually refers to a programming paradigm introduced by Donald E. Knuth. It describes an approach where programming is done in a human readable style at first. See Knuth (1992).↩︎\nThe ODD-format is used in various contexts, e.g. the German Textarchiv (DTA) uses ODD-files as a source for their TEI-subset DTABf. See Haaf and Thomas (2016).↩︎\nThis principles have been described in various books by many authors; one of the most famous is the book Clean code by Robert C. Martin (2009).↩︎\nThe source code of this pipeline as well as the ODD sources are open sourced and can be found in the corresponding GitHub-Repo as well as on Zenodo. See Politycki, Sonder, and Sutter (2024).↩︎\nThe starting point for the creation of ODD files is usually a tool called Roma. See Text Encoding Initiative (2024b).↩︎\nThe term TDD usually refers to Kent Beck, who reintroduced this idea in the early 2000s. It describes a programming paradigm where tests are written before the actual code. See Alsaqqa, Sawalha, and Abdel-Nabi (2020), p. 255.↩︎\nThese tests would normally be set up before the concrete description in the ODD-module is created, but we started with an already existing schema and decided to add the test later on.↩︎\nSee Christie (2024).↩︎\nSee Law Sources Foundation of the Swiss Lawyers Society (2024b).↩︎\nIt may sometimes be necessary to convert them with XSLT to be valid against the newer version of the schema.↩︎\nSee Preston-Werner (2023).↩︎",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/486/index.html",
    "href": "submissions/486/index.html",
    "title": "From words to numbers. Methodological perspectives on large scale Named Entity Linking",
    "section": "",
    "text": "Named entity recognition, disambiguation, and linking are pivotal methods in Natural Language Processing (NLP) applied to historical research. These methods present unique and complex challenges in the context of historical texts (Bunout, Ehrmann, and Clavert 2023; Luthra et al. 2022; Ehrmann et al. 2023). They grapple with the complexities arising from context-dependent meanings of named entities, as well as the issues of polysemy, homonymy, and naming variations.\nHistorically, solutions ranged from basic string matching to intricate rule-based heuristics. While these methods are still widely used, they often fall short in terms of scalability, generalization, and accuracy, particularly when compared to current machine-learning techniques. Recent advances have seen a shift towards leveraging contextual embeddings to achieve groundbreaking accuracy in these tasks, as evidenced by seminal works such as Yamada et al. (2016); Ganea and Hofmann (2017); and Chen et al. (2020).\nVector embeddings are an essential tool used in NLP to represent words as numerical vectors. When applied appropriately, they can capture semantic information of words depending on the context in which they appear. For instance, in sentences such as «I opened an account at the bank» and «Beavers build dams in river banks,» the word «bank» would be embedded differently. On the other hand, the vector embeddings for «I sat down on the chair» and «I lowered myself onto the seat» would be «close» in the vector space, as they contain similar content.\nRegarding linking named entities in a text, e.g. persons, this would mean that we embed them based on the context in which they appear. If there are two viable options (such as the same first name, last name, and time period) for a match between a name and a person, but the name we are searching for appears in an article about architecture and one of the two options is an architect and one a medical doctor, we can now take into account this semantic context as an additional parameter to calculate a possible match.",
    "crumbs": [
      "Abstracts",
      "From words to numbers. Methodological perspectives on large scale Named Entity Linking"
    ]
  },
  {
    "objectID": "submissions/486/index.html#introduction",
    "href": "submissions/486/index.html#introduction",
    "title": "From words to numbers. Methodological perspectives on large scale Named Entity Linking",
    "section": "",
    "text": "Named entity recognition, disambiguation, and linking are pivotal methods in Natural Language Processing (NLP) applied to historical research. These methods present unique and complex challenges in the context of historical texts (Bunout, Ehrmann, and Clavert 2023; Luthra et al. 2022; Ehrmann et al. 2023). They grapple with the complexities arising from context-dependent meanings of named entities, as well as the issues of polysemy, homonymy, and naming variations.\nHistorically, solutions ranged from basic string matching to intricate rule-based heuristics. While these methods are still widely used, they often fall short in terms of scalability, generalization, and accuracy, particularly when compared to current machine-learning techniques. Recent advances have seen a shift towards leveraging contextual embeddings to achieve groundbreaking accuracy in these tasks, as evidenced by seminal works such as Yamada et al. (2016); Ganea and Hofmann (2017); and Chen et al. (2020).\nVector embeddings are an essential tool used in NLP to represent words as numerical vectors. When applied appropriately, they can capture semantic information of words depending on the context in which they appear. For instance, in sentences such as «I opened an account at the bank» and «Beavers build dams in river banks,» the word «bank» would be embedded differently. On the other hand, the vector embeddings for «I sat down on the chair» and «I lowered myself onto the seat» would be «close» in the vector space, as they contain similar content.\nRegarding linking named entities in a text, e.g. persons, this would mean that we embed them based on the context in which they appear. If there are two viable options (such as the same first name, last name, and time period) for a match between a name and a person, but the name we are searching for appears in an article about architecture and one of the two options is an architect and one a medical doctor, we can now take into account this semantic context as an additional parameter to calculate a possible match.",
    "crumbs": [
      "Abstracts",
      "From words to numbers. Methodological perspectives on large scale Named Entity Linking"
    ]
  },
  {
    "objectID": "submissions/486/index.html#methods",
    "href": "submissions/486/index.html#methods",
    "title": "From words to numbers. Methodological perspectives on large scale Named Entity Linking",
    "section": "Methods",
    "text": "Methods\nIn our presentation, we will show a glimpse of the current state of our ambitious project, which aims to create a robust and scalable pipeline for applying embeddings-based NEL to historical texts. In our work, we focus on three key aspects. Firstly, on embeddings-based linking and disambiguation workflow applied to a historical corpus of Swiss magazines (E-Periodica) that uses Wikipedia, Gemeinsame Normdatei (GND), and – since our primary use cases deal with historical material from Switzerland – the Historical Dictionary of Switzerland (HDS) as reference knowledge bases. This part aims to develop a performant and modular pipeline to recognize named entities in retro-digitized texts and link them to so-called authority files (Normdaten), e.g., the German Authority File (GND). With this workflow, we will help to identify historical actors in source material and contribute to the in-depth FAIRification of large datasets through persistent identifiers on the text level. Our proposed pipeline is modular with respect to the embedding model, enabling performance comparison across different embedding model choices and leaving room for future improved embedding models, which capture semantic similarities even better than current popular open-source models such as BERT.\nSecondly, we plan to use this case study to reflect upon the interpretation of metrics provided by algorithmic models and their relevance in historical research methodology. We will focus on three key areas: Contextual Sensitivity, Ambiguity Resolution, and Computational Efficiency. By focusing on these aspects, we will provide a comprehensive insight into the models’ operational capabilities, particularly in large-scale historical text analysis. Given the challenges of retro-digitized historical data (OCR quality, heterogeneous contents in large collections, etc.), it is necessary to not only select appropriate models and methods to the specific needs of such material but also to create representative ground truth data for OCR, NER, and NEL. Furthermore, scale considerations drive our case study, as some of our use cases consist of millions of pages.\nFinally, we will discuss the role of GLAM (galleries, libraries, archives, and museums) institutions as drivers of change and facilitators, especially when it comes to the use of their collections as data (Padilla et al. 2023).\n\n\n\nPipeline of end-to-end Named Entity Linking.",
    "crumbs": [
      "Abstracts",
      "From words to numbers. Methodological perspectives on large scale Named Entity Linking"
    ]
  },
  {
    "objectID": "submissions/486/index.html#conclusion",
    "href": "submissions/486/index.html#conclusion",
    "title": "From words to numbers. Methodological perspectives on large scale Named Entity Linking",
    "section": "Conclusion",
    "text": "Conclusion\nCurrent solutions for NEL need more accuracy and scalability. At the same time, such enrichment processes will become standard processes for GLAM institutions so that they can offer enriched data layers to their users as a service. This raises several challenges: The technical challenge to improve the linking workflow itself, the challenge to document the workflow in a transparent and reproducible form, and finally, the methodological challenge to negotiate and interpret the results at the intersection of GLAM institutions, data science, and historical research.",
    "crumbs": [
      "Abstracts",
      "From words to numbers. Methodological perspectives on large scale Named Entity Linking"
    ]
  },
  {
    "objectID": "submissions/486/index.html#references",
    "href": "submissions/486/index.html#references",
    "title": "From words to numbers. Methodological perspectives on large scale Named Entity Linking",
    "section": "References",
    "text": "References\n\n\nBunout, Estelle, Maud Ehrmann, and Frédéric Clavert, eds. 2023. Reflections on Tools, Methods and Epistemology. Berlin, Boston: De Gruyter Oldenbourg. https://doi.org/doi:10.1515/9783110729214.\n\n\nChen, Haotian, Andrej Zukov-Gregoric, Xi David Li, and Sahil Wadhwa. 2020. “Contextualized End-to-End Neural Entity Linking.” https://arxiv.org/abs/1911.03834.\n\n\nEhrmann, Maud, Ahmed Hamdi, Elvys Linhares Pontes, Matteo Romanello, and Antoine Doucet. 2023. “Named Entity Recognition and Classification in Historical Documents: A Survey.” ACM Comput. Surv. 56 (2). https://doi.org/10.1145/3604931.\n\n\nGanea, Octavian-Eugen, and Thomas Hofmann. 2017. “Deep Joint Entity Disambiguation with Local Neural Attention.” In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, edited by Martha Palmer, Rebecca Hwa, and Sebastian Riedel, 2619–29. Copenhagen, Denmark: Association for Computational Linguistics. https://doi.org/10.18653/v1/D17-1277.\n\n\nLuthra, Mrinalini, Konstantin Todorov, Charles Jeurgens, and Giovanni Colavizza. 2022. “Unsilencing Colonial Archives via Automated Entity Recognition.” https://arxiv.org/abs/2210.02194.\n\n\nNozza, Debora, Cezar Sas, Elisabetta Fersini, and Enza Messina. 2019. “Word Embeddings for Unsupervised Named Entity Linking.” In Knowledge Science, Engineering and Management, edited by Christos Douligeris, Dimitris Karagiannis, and Dimitris Apostolou, 115–32. Cham: Springer International Publishing.\n\n\nPadilla, Thomas, Hannah Scates Kettler, Stewart Varner, and Yasmeen Shorish. 2023. “Vancouver Statement on Collections as Data.” Zenodo. https://doi.org/10.5281/zenodo.8342171.\n\n\nVasilyev, Oleg, Alex Dauenhauer, Vedant Dharnidharka, and John Bohannon. 2022. “Named Entity Linking with Entity Representation by Multiple Embeddings.” https://arxiv.org/abs/2205.10498.\n\n\nYamada, Ikuya, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. “Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation.” In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, edited by Stefan Riezler and Yoav Goldberg, 250–59. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/K16-1025.",
    "crumbs": [
      "Abstracts",
      "From words to numbers. Methodological perspectives on large scale Named Entity Linking"
    ]
  },
  {
    "objectID": "submissions/444/index.html",
    "href": "submissions/444/index.html",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "",
    "text": "This paper discusses data and practices related to an ongoing digital humanities consortium project Constellations of Correspondence – Large and Small Networks of Epistolary Exchange in the Grand Duchy of Finland (CoCo; Research Council of Finland, 2021–2025). The project aggregates, analyses and publishes 19th-century epistolary metadata from letter collections of Finnish cultural heritage (CH) organisations on a Linked Open Data service and as a semantic web portal (the ‘CoCo Portal’), and it consists of three research teams, bringing together computational and humanities expertise. We focus exclusively on metadata considering them to be part of the cultural heritage and a fruitful starting point for research, providing access i.e. to 19th-century epistolary culture and archival biases. The project started with a webropol survey addressed to over 100 CH organisations to get an overview of the preserved 19th-century letters and the Finnish public organisations willing to share their letter metadata with us. Currently the CoCo portal includes seven CH organisations and four online letter publications with the metadata of over 997.000 letters and with 95.000 actors (senders and recipients of letters).",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/444/index.html#introduction",
    "href": "submissions/444/index.html#introduction",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "",
    "text": "This paper discusses data and practices related to an ongoing digital humanities consortium project Constellations of Correspondence – Large and Small Networks of Epistolary Exchange in the Grand Duchy of Finland (CoCo; Research Council of Finland, 2021–2025). The project aggregates, analyses and publishes 19th-century epistolary metadata from letter collections of Finnish cultural heritage (CH) organisations on a Linked Open Data service and as a semantic web portal (the ‘CoCo Portal’), and it consists of three research teams, bringing together computational and humanities expertise. We focus exclusively on metadata considering them to be part of the cultural heritage and a fruitful starting point for research, providing access i.e. to 19th-century epistolary culture and archival biases. The project started with a webropol survey addressed to over 100 CH organisations to get an overview of the preserved 19th-century letters and the Finnish public organisations willing to share their letter metadata with us. Currently the CoCo portal includes seven CH organisations and four online letter publications with the metadata of over 997.000 letters and with 95.000 actors (senders and recipients of letters).",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/444/index.html#the-data-and-the-portal",
    "href": "submissions/444/index.html#the-data-and-the-portal",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "The Data and the Portal",
    "text": "The Data and the Portal\nThe CoCo Data Model is based on international standards, such as CIDOC CRM (Doerr 2003), Dublin Core, and ICA Records in Contexts to promote interoperability with other datasets. The model supports the modelling of the relevant properties of letter metadata (Letter, Actor, Place, and Time-Span, provenance i.e. MetadataRecord and archival/collection level information) from the source datasets. To represent actors in these different source datasets, we use an adaptation of the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) proxy concept. The collected metadata are transformed into linked open data using an automated transformation pipeline (Drobac 2023), which consists of several steps. First, each received dataset is processed into an intermediate RDF format. Then the data are harmonised with the CoCo Data Model and enriched by linking the recognised actors and places to external resources, such as Wikidata, Finnish National Biographies, as well as the Finnish AcademySampo (Drobac 2023) and the BiographySampo (Tamper 2023). Finally, the transformation pipeline produces a harmonised dataset of correspondence metadata. The availability of the aggregated letter metadata in the linked open data format facilitates the use of the data for data exploration and answering humanities research questions by publishing the data in a semantic portal or by using SPARQL queries. The project’s semantic portal allows users to search, browse and analyse the letters, archives, actors, and places in the CoCo dataset. It is based on the Sampo model (Hyvönen 2022) and is implemented using the Sampo-UI programming framework (Ikkala 2022). The user interface works on a faceted search paradigm, which allows the user to search, e.g. for letters sent by a certain person, letters from a certain period or letters kept in a certain organisation. Data such as the sending places can be visualised on a map, and other visualisations include the yearly distributions of letters, top correspondents, and correspondence networks. The portal also offers some network analysis figures.",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/444/index.html#constant-datawork-in-an-interdiscplinary-team",
    "href": "submissions/444/index.html#constant-datawork-in-an-interdiscplinary-team",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "Constant Datawork in an Interdiscplinary Team",
    "text": "Constant Datawork in an Interdiscplinary Team\nAt this stage (mid 2024), the three consortium teams have been working together for three full years. Two of the teams (Aalto University and the University of Helsinki) have a computational profile and are responsible for data modelling and transformation and the development of the user interface (semantic portal), and the team based at the Finnish Literature Society maintains relations with the data providers (CH organisations), but also participates in the first stages of data processing, for example by manually harmonising finding aids in word format to prepare them for algorithmic processing, and by performing quality checks on automatically processed data. Members of all teams participate in research activities (conferences, seminars, etc.) in their fields, but we mostly publish co-authored papers, currently focusing on what we call ‘critical collection history’. The starting point for this interdisciplinary work was fortuitous, as the key members had years of experience in – to borrow a term from Jo Guldi (2023) – hybrid teamwork. Guldi prefers the word hybrid to the more common interdisciplinary because ‘it is more specific to the kind of aliveness, the ongoing exchange, that I have in mind as the basis for the genesis of a new field. … I can be interdisciplinary in solitude … But hybrid teams require ongoing support and thinking between people trained in and who identify as members of far-flung disciplines’. (Guldi 2023). Thinking together across disciplines requires a strong commitment to developing a common language and an atmosphere in which it is possible to express uncertainty and ask questions. Despite the previous experience, working with the specificities of archival metadata and transforming them into ‘big metadata’ has forced us to study and develop a new shared vocabulary that reflects the particular characteristics or ‘localities’ (Loukissas 2019) of this particular data in the context of Linked Open Data and the chosen Humanities’ questions. One of the realisations and lessons of the project is the importance of collection-level information (identifying the so-called records creators and the fonds, i.e. the archival collections). Data models developed in previous projects dealing with epistolary metadata (usually based on curated and published editions of letters) did not highlight these characteristics, and all the datasets we acquired from archives, libraries and museums conveyed this information in different ways. In order to understand and visualise this aspect of the data, we needed to combine a practical understanding of archival work, theoretical reflections from cultural heritage studies and critical archival theory, and domain expertise in cultural heritage data ontologies and models. Such hybrid thinking takes time, especially when the aim is both to create a new dataset and to understand and analyse it in order to answer historical research questions. About seven months into the project, we started to see aggregated data. It was also at this point that we realised that monthly project meetings were far from enough; we needed to develop systematic ways of looking at the data, and to do so together as often as possible. As a result, a series of events were developed to explore and discuss the data: First, data meetings, where we decided, for example, which parts of the various data collected from the CH organisations would be transformed into linked open data; second, SPARQL meetings, where we learned the query language and used it to interrogate the datasets (this proved to be a good way of detecting errors and anomalies either in the transformation process or in the original data); third, portal meetings, where we discuss the features and properties of the interface and, again, look for patterns in the data by looking at the visualisations; and fourth, the so-called ‘CoCothons’, annual two-day intensive workshops dedicated to data exploration, but with some research content as well. On a monthly basis, we spend a total of 1–2 working days together ‘at the data’. The funding mechanism – a three-part consortium project – makes it possible to work across different universities and research institutions to gain expertise that is not available in any one place. The downside, however, is that we are working in different organisations; this is not a fashionable digital humanities ‘LAB’. All three teams in the project work in the Helsinki metropolitan area, which makes face-to-face meetings possible, but even with the time and effort invested in team work and the digital tools for easy communication (Slack, Trello, etc.), we still find from time to time that the algorithmic premises used by the computational team do not stand up to historical scrutiny, or that the research assistants have misinterpreted instructions for manual data processing.",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/444/index.html#portal-test-users-and-their-feedback",
    "href": "submissions/444/index.html#portal-test-users-and-their-feedback",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "Portal Test Users and their Feedback",
    "text": "Portal Test Users and their Feedback\nWe realised early on that it was extremely important to gather user experience before actually launching the CoCo portal. So, in early February 2024, we opened the portal to a test group of 17 people for a period of 2.5 months. The group was assembled partly through an open call on social media and at some conferences, and partly by asking specific people to join. The volunteers were mostly academic humanities researchers and invited specialists from two museums with 19th-century letter collections. We provided the testers with background material on the portal and the CoCo data endpoint documentation, the organisations whose data was currently in the portal, with user instructions, and with some questions or tasks to help them get started. We asked five specific questions to which we wanted answers, but we welcomed all kinds of feedback. Building an engaged and motivated community of test users proved to be a challenge. The opening online session, where the project’s portal experts taught how to use it, received only four participants. However, they were active in asking questions and satisfied with the introduction. We also offered the possibility of a joint online closing session to discuss the experiences, but only two people registered, so we cancelled it. All this shows that once the initial excitement of joining the test group had worn off, it was difficult to reach the testers and very difficult to engage them in the test or create a group spirit. People got lost in their own research and daily work. In the end, however, after a reminder, we received feedback from eight testers, i.e. from half of the group. We are still waiting for feedback from the later test group of archivists in the Finnish Literary Society. The feedback can be divided into two groups: comments on errors in the data, mostly errors in the disambiguation of actors, and comments on the functionalities and performance of the portal. The latter was what we had hoped for. It is interesting to note that the testers, who work in CH organisations and are used to working with collection management systems and have even catalogued archival material themselves, commented more on the functionalities, while the researchers clearly focused on data errors. In our view, this shows how much hands-on experience with databases affects a humanist’s ability to study mass data material and turn into digital humanities. You have to change your mindset, as our own experience in the project has shown. We also noticed that working only with the metadata of letters was a barrier for some of the testers; in the wish list for the future development of the portal, digitised letters or their transliterations stood out. A positive result from the project’s point of view was that the testers found the portal easy to use, the user interface with its four perspectives clear and the data offered useful both for their own research and for information services in CH organisations. They were able to find unknown connections, relationships and people in the data. The metadata available seemed to respond well to their research questions and to encourage further research. The ability to do queries with places was particularly appreciated. The fact that we had to write ‘preliminary’ instructions and update the portal for the testers in advance stimulated internal discussion about the future development of the portal and how to create a smooth feedback system for data errors and functional problems. Ethical data work (Ahnert and Weingart 2020) requires us to be very open, precise and thorough in documenting what data we have received and how we have harmonised it, which organisations are included and how they are implemented in the portal. Some difficulties with functionalities, visualisations and data errors have either already been fixed or are in progress or at least under discussion for further action. The feedback has provided us with an insight into the difficulties that users, both ‘traditional’ humanists and members of the wider public, may encounter when using the portal, and some guidelines for us to help them learn to read digital. The feedback proves that we are on the right track towards our goal of creating a new research resource, a virtual archive that crosses organisational silos.",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/444/index.html#references",
    "href": "submissions/444/index.html#references",
    "title": "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata",
    "section": "References",
    "text": "References\n\n\nAhnert, Ahnert, Ruth, and Scott B. Weingart. 2020. The Network Turn: Changing Perspectives in the Humanities. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108866804.\n\n\nDoerr, Martin. 2003. “The CIDOC CRM—an Ontological Approach to Semantic Interoperability of Metadata.” AI Magazine 24(3), 75–92.\n\n\nDrobac, Enqvist, Senka. 2023. “The Laborious Cleaning: Acquiring and Transforming 19th-Century Epistolary Metadata,” 248–62. https://doi.org/10.5617/dhnbpub.10669.\n\n\nGuldi, Jo. 2023. The Dangerous Art of Text Mining. A Methodology for Digital History. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781009263016.\n\n\nHyvönen, Eero. 2022. “Digital Humanities on the Semantic Web: Sampo Model and Portal Series.” Semantic Web – Interoperability, Usability, Applicability, 14(4), 729–44. https://doi.org/10.3233/SW-223034.\n\n\nIkkala, Hyvönen, Esko. 2022. “Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces.” Semantic Web – Interoperability, Usability, Applicability, 13(1), 69–84. https://doi.org/10.3233/SW-210428.\n\n\nLoukissas, Yanni Alexander. 2019. All Data Are Local: Thinking Critically in a Data-Driven Society. Cambridge: The MIT Press. https://doi.org/10.7551/mitpress/11543.001.0001.\n\n\nTamper, Leskinen, Minna. 2023. “Analyzing Biography Collection Historiographically as Linked Data: Case National Biography of Finland.” Semantic Web – Interoperability, Usability, Applicability, 14(2), 385–419. https://doi.org/10.3233/SW-222887.",
    "crumbs": [
      "Abstracts",
      "Learning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata"
    ]
  },
  {
    "objectID": "submissions/keynote/index.html",
    "href": "submissions/keynote/index.html",
    "title": "When Literacy Goes Digital: Rethinking the Ethics and Politics of Digitisation",
    "section": "",
    "text": "Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@misc{zaagsma2024,\n  author = {Zaagsma, Gerben},\n  editor = {Baudry, Jérôme and Burkart, Lucas and Joyeux-Prunel,\n    Béatrice and Kurmann, Eliane and Mähr, Moritz and Natale, Enrico and\n    Sibille, Christiane and Twente, Moritz},\n  title = {When {Literacy} {Goes} {Digital:} {Rethinking} the {Ethics}\n    and {Politics} of {Digitisation}},\n  date = {2024-05-10},\n  url = {https://digihistch24.github.io/book-of-abstracts/submissions/keynote/},\n  langid = {en},\n  abstract = {In recent years, the critical turn in digital humanities\n    has sparked numerous discussions about digital literacy in the\n    discipline of history. While critical work has focused on data,\n    tools, and the skills that historians need in the current digital\n    age, questions remain about the broader contours of digital literacy\n    and the multiple meanings that could be attributed to it. Amidst the\n    shift to a culture of digital abundance and a research environment\n    that privileges what is available online, digitisation has brought\n    old questions about heritage, power, and the production and\n    construction of historical knowledge to the fore. This calls for an\n    approach that expands our current methodological purview to include\n    broader epistemological and normative considerations. To this end,\n    my talk will foreground the ethics and politics of digitisation as\n    an essential component of digital historical literacy. I propose to\n    do so in three intertwined steps. First comes historical context.\n    Just as digital history urgently needs historicising, so too does\n    digital literacy, not only as a product of precursors such as\n    information and media literacy, but also in relation to notions of\n    literacy and its ethical dimensions more generally. Second, thinking\n    through digital literacy inevitably implies reckoning with the\n    global dimensions of cultural heritage digitisation and its effects\n    on historical knowledge production beyond the oft-posited Global\n    North/South binary. Third, to exercise digital literacy is to\n    acknowledge how ethics and politics suffuse digital epistemologies\n    that fundamentally reframe historical research practices.\n    Ultimately, I argue that integrating these considerations in our\n    discussions of digital literacy is crucial for a discipline still\n    grappling to come to terms with the digital age.}\n}\nFor attribution, please cite this work as:\nZaagsma, Gerben. 2024. “When Literacy Goes Digital: Rethinking the\nEthics and Politics of Digitisation.” Edited by Jérôme Baudry,\nLucas Burkart, Béatrice Joyeux-Prunel, Eliane Kurmann, Moritz Mähr,\nEnrico Natale, Christiane Sibille, and Moritz Twente. Digital\nHistory Switzerland 2024: Book of Abstracts. https://digihistch24.github.io/book-of-abstracts/submissions/keynote/.",
    "crumbs": [
      "Abstracts",
      "When Literacy Goes Digital: Rethinking the Ethics and Politics of Digitisation"
    ]
  },
  {
    "objectID": "submissions/465/index.html",
    "href": "submissions/465/index.html",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "",
    "text": "Over the last few years, Machine Learning applications became more and more popular in the humanities and social sciences in general, and therefore also in history. Handwritten Text Recognition (HTR) and various tasks of Natural Language Processing (NLP) are now commonly employed in a plethora of research projects of various sizes. Even for PhD projects it is now feasible to research large corpora like serial legal source, which would not be possible entirely by hand. This acceleration of research processes implies fundamental changes to how we think about sources, data, research and workflows.\nIn history, Machine Learning systems are typically used to speed up the production of research data. As the output of these applications is never entirely accurate or correct, this raises the question how historians can use machine generated data together with manually created data without propagating errors and uncertainties to downstream tasks and investigations.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#introduction",
    "href": "submissions/465/index.html#introduction",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "",
    "text": "Over the last few years, Machine Learning applications became more and more popular in the humanities and social sciences in general, and therefore also in history. Handwritten Text Recognition (HTR) and various tasks of Natural Language Processing (NLP) are now commonly employed in a plethora of research projects of various sizes. Even for PhD projects it is now feasible to research large corpora like serial legal source, which would not be possible entirely by hand. This acceleration of research processes implies fundamental changes to how we think about sources, data, research and workflows.\nIn history, Machine Learning systems are typically used to speed up the production of research data. As the output of these applications is never entirely accurate or correct, this raises the question how historians can use machine generated data together with manually created data without propagating errors and uncertainties to downstream tasks and investigations.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#facticity",
    "href": "submissions/465/index.html#facticity",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Facticity",
    "text": "Facticity\nThe question of the combined usability of machine-generated and manually generated data is also a question of the reliability or facticity of data. Data generated by humans are not necessarily complete and correct either, as they are a product of human perception. For example, creating transcriptions depends on the respective transcription guidelines and individual text understanding, which can lead to errors. However, we consider transcriptions by experts as correct and use them for historical research. This issue is even more evident in the field of editions. Even very old editions with methodological challenges are valued for their core content. Errors may exist, but they are largely accepted due to the expertise of the editors, treating the output as authorised. This pragmatic approach enables efficient historical research. Historians trust their ability to detect and correct errors during research.\nFrancesco Beretta represents data, information, and knowledge as a pyramid: data form the base, historical information (created from data through conceptual models and critical methods) forms the middle, and historical knowledge (produced from historical information through theories, statistical models and heuristics) forms the top (Beretta 2023, fig. 3). Interestingly, however, he makes an important distinction regarding digital data: “Digital data does not belong to the epistemic layer of data, but to the layer of information, of which they are the information technical carrier” (Translation: DW. Original Text: “[L]les données numériques n’appartiennent pas à la strate épistémique des données, mais bien à celle de l’information dont elles constituent le support informatique.” Beretta 2023, 18)\nAndreas Fickers adds that digitization transforms the nature of sources, affecting the concept of the original (Fickers 2020, 162). Sources are preprocessed using HTR/OCR and various NLP strategies. The resulting digital data are already processed historical information. This shift from analog to digital means that what we extract from sources is not just given but constructed (Beretta 2023, 26). Analog historical research, which relies on handwritten archival documents, also depends on transcriptions or editions to conduct research pragmatically; and here, too, data becomes information. The main difference is that with the generation of digital data, the (often linear) structure of sources is typically dissolved in favour of a highly fragmented and hyperconnected structure (For hyperconnectivity see Fickers 2022, 51–54; For the underlying concept of hypertextual systems see Landow 2006, 53–58; for a a more extensive discussion of digital representations of fragmented texts see Weber 2021). This is partly due to the way sources are processed into historical information using digital tools and methods, but it is inherently connected with issues of storing, retrieving, and presenting digital data – in a very technical sense.\nThe concept of factoids introduced by Michele Pasin and John Bradley, is central to this argument. They define factoids as pieces of information about one or more persons in a primary source. Those factoids are then represented in a semantic network of subject-predicate-object triples (Pasin and Bradley 2015, 89–90). This involves extracting statements from their original context, placing them in a new context, and outsourcing verification to later steps. Therefore, factoids can be contradictory. Francesco Beretta applies this idea to historical science, viewing the aggregation of factoids as a process aiming for the best possible approximation of facticity (Beretta 2023, 20). The challenge is to verify machine output sufficiently for historical research and to assess the usefulness of the factoid concept. Evaluating machine learning models and their outputs is crucial for this.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#qualifying-error-rates",
    "href": "submissions/465/index.html#qualifying-error-rates",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Qualifying Error Rates",
    "text": "Qualifying Error Rates\nEvaluating the output of a machine learning system is not trivial. Models can be evaluated using various calculated scores, which is done continuously during the training process. However, these performance metrics are statistical measures that generally refer to the model and are based on a set of test data. Even the probabilities output by machine learning systems when applied to new data are purely computational figures, only partially suitable for quality assurance. This verification is further complicated by the potentially vast scale of the output. Therefore, historical science must find a pragmatic way to translate statistical evaluation metrics into qualitative statements and identify systematic sources of error.\nIn automatic handwriting recognition, models are typically evaluated using character error rate (CER). These metrics only tell us the percentage of characters or words incorrectly recognised compared to a ground truth. They do not reveal the distribution of these errors, which is important when comparing automatic and manual transcriptions. For detailed HTR model evaluation, CERberus is being developed (Haverals 2023). This tool compares ground truth with HTR output from the same source. Instead of calculating just the character error rate, it breaks down the differences further. Errors are categorised into missing, excess, and incorrectly recognised characters. Additionally, a separate CER is calculated for all characters and Unicode blocks in the text, aggregated into confusion statistics that identify the most frequently confused characters. Confusion plots are generated to show the most common errors for each character. These metrics do not pinpoint specific errors but provide a more precise analysis of the model’s behaviour. CERberus cannot evaluate entirely new HTR output without comparison text but is a valuable tool for Digital History, revealing which character forms are often confused and guiding model improvement or post-processing strategies.\nIn other machine learning applications, such as named entity recognition (NER), different metrics are important, requiring detailed error source analysis. Evaluating NER is more complex than HTR because it involves categorizing longer text sections based on context. Precision (how many recognised positives are true positives) and recall (how many actual positives are recognised) are combined into the F1-score to indicate model performance. Fu et al. proposed evaluating NER with a set of eight annotation attributes influencing model performance. These attributes are divided into local properties (entity length, sentence length, unknown word density, entity density) and aggregated attributes (annotation consistency and frequency at the token and entity levels) (Fu, Liu, and Neubig 2020, 3). Buckets of source points where a model performs particularly well or poorly are created and separately evaluated (Fu, Liu, and Neubig 2020, 1). This analysis identifies conditions affecting model performance, guiding further training steps and dataset expansion.\nThe qualitative error analysis presented here does not solve the question of authorizing machine learning output for historical research. Instead, it provides tools to assess models more precisely and analyse training and test datasets. Such investigations extend the crucial source criticism in historical science to digital datasets and the algorithms and models involved in their creation. This requires historians to expand their traditional methods to include new, less familiar areas.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#three-strategic-directions",
    "href": "submissions/465/index.html#three-strategic-directions",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Three Strategic Directions",
    "text": "Three Strategic Directions\nIn the following last part of this article, the previously raised questions and problem areas will be consolidated, from which three strategic directions for digital history will be derived. These will be suggestions for how the theory, methodology, and practice of Digital History could evolve to address and mitigate the identified problem areas. The three perspectives should not be viewed in isolation or as mutually exclusive. Instead, they are interdependent and should work together to meet the additional challenges.\n\nDirection 1: Formulating Clear Needs\nWhen data is collected or processed into information in the historical research process a certain pragmatism is involved. Ideally, such a project would fully and consistently transcribe the entire collection with the same thoroughness, but in practice, a compromise is often found between completeness, correctness, and pragmatism. Often, for one’s own research purposes, it is sufficient to transcribe a source only to the extent that its meaning can be understood. This compromise has not fully transitioned into Digital History. Even if a good CER is achieved, there is pressure to justify how these potential errors are managed in the subsequent research process. This skepticism is not fundamentally bad, and the epistemological consequences of erroneous machine learning output are worthy of discussion. Nonetheless, the resulting text is usually quite readable and usable.\nThus, I argue that digital history must more clearly define and communicate its needs. However, it must be remembered that Digital History also faces broader demands. Especially in machine learning-supported research, the demand for data interoperability is rightly emphasised. Incomplete or erroneous datasets are, of course, less reusable by other research projects.\n\n\nDirection 2: Creating Transparency\nThe second direction for digital history is to move towards greater transparency. The issue of reusability and interoperability of datasets from the first strategic direction can be at least partially mitigated by transparency.\nAs Hodel et al. convincingly argued, it is extremely sensible and desirable for projects using HTR to publish their training data. This allows for gradual development towards models that can generalise as broadly as possible (Hodel et al. 2021, 7–8). If a CERberus error analysis is conducted for HTR that goes beyond the mere CER, it makes sense to publish this alongside the data and the model. With this information, it is easier to assess whether it might be worthwhile to include this dataset in one’s own training material. Similarly, when NER models are published, an extended evaluation according to Fu et al. helps to better assess the performance of a model for one’s own dataset.\nPasin and Bradley, in their prosopographic graph database, indicate the provenance of each data point and who captured it (Pasin and Bradley 2015, 91–92). This principle could also be interesting for Digital History, by indicating in the metadata whether published research data was generated manually or by a machine, ideally with information about the model used and the annotating person for manually generated data. Models provide a confidence estimate with their prediction, indicating how likely the prediction is correct. The most probable prediction would be treated as the first factoid. The second or even third most probable prediction from the systems cloud provide additional factoids that can be incorporate into the source representation. These additional pieces of information can support the further research process by allowing inconsistencies and errors to be better assessed and balanced.\n\n\nDirection 3: Data Criticism and Data Hermeneutics\nThe shift to digital history requires an evaluation and adjustment of our hermeneutic methods. This ongoing discourse is not new, and Torsten Hiltmann has identified three broad directions: first, the debate about extending source criticism to data, algorithms, and interfaces; second, the call for computer-assisted methods to support text understanding; and third, the theorization of data hermeneutics, or the “understanding of and with data” (Hiltmann 2024, 208).\nEven though these discourse strands cannot be sharply separated, the focus here is primarily on data criticism and hermeneutics. The former can fundamentally orient itself towards classical source criticism. Since digital data is not given but constructed, it is crucial to discuss by whom, for what purpose, and how data was generated. This is no easy task, especially when datasets are poorly documented. Therefore, the call for data and model criticism is closely linked to the plea for more transparency in data and model publication.\nIn the move towards data hermeneutics, a thorough rethinking of the factoid principle can be fruitful. If, as suggested above, the second or even third most likely predictions of a model are included as factoids in the publication of research data, this opens up additional perspectives on the sources underlying the data. From these new standpoints, the data – and thus the sources – can be analyzed and understood more thoroughly. Additionally, this allows for a more informed critique of the data, and extensive transparency also mitigates the “black box” problem of interpretation described by Silke Schwandt (Schwandt 2022). If we more precisely describe and reflect on how we generate digital data from sources as historians, we will find that our methods are algorithmic (Schwandt 2022, 81–82). This insight can also support the understanding of how machine learning applications work. Data hermeneutics thus requires both a critical reflection of our methods and a more transparent approach to data and metadata.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#references",
    "href": "submissions/465/index.html#references",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "References",
    "text": "References\n\n\nBeretta, Francesco. 2023. “Données ouvertes liées et recherche historique : un changement de paradigme.” Humanités numériques, no. 7 (July). https://doi.org/10.4000/revuehn.3349.\n\n\nFickers, Andreas. 2020. “Update für die Hermeneutik. Geschichtswissenschaft auf dem Weg zur digitalen Forensik?” Zeithistorische Forschungen 1: 157–68. https://doi.org/10.14765/ZZF.DOK-1765.\n\n\n———. 2022. “What the D does to history: Das digitale Zeitalter als neues historisches Zeitregime?” In Digital History: Konzepte, Methoden und Kritiken Digitaler Geschichtswissenschaft, edited by Karoline Dominika Döring, Stefan Haas, Mareike König, and Jörg Wettlaufer, 45–64. De Gruyter Oldenbourg. https://doi.org/10.1515/9783110757101-003.\n\n\nFu, Jinlan, Pengfei Liu, and Graham Neubig. 2020. “Interpretable Multi-dataset Evaluation for Named Entity Recognition.” arXiv. https://doi.org/10.48550/arXiv.2011.06854.\n\n\nHaverals, Wouter. 2023. “CERberus: Guardian Against Character Errors.” https://github.com/WHaverals/CERberus.\n\n\nHiltmann, Torsten. 2024. “Hermeneutik in Zeiten der KI: Large Language Models als hermeneutische Instrumente in den Geschichtswissenschaften.” In KI:Text, 201–32. De Gruyter. https://doi.org/10.1515/9783111351490-014.\n\n\nHodel, Tobias, David Schoch, Christa Schneider, and Jake Purcell. 2021. “General Models for Handwritten Text Recognition: Feasibility and State-of-the Art. German Kurrent as an Example.” Journal of Open Humanities Data 7. https://doi.org/10.5334/johd.46.\n\n\nLandow, George P. 2006. Hypertext 3.0: Critical Theory and New Media in an Era of Globalization. 3. Auflage. Baltimore.\n\n\nPasin, Michele, and John Bradley. 2015. “Factoid-Based Prosopography and Computer Ontologies: Towards an Integrated Approach.” Digital Scholarship in the Humanities 30 (1): 86–97. https://doi.org/10.1093/llc/fqt037.\n\n\nSchwandt, Silke. 2022. “Opening the Black Box of Interpretation: Digital History Practices as Models of Knowledge.” History and Theory 61 (4): 77–85. https://doi.org/10.1111/hith.12281.\n\n\nWeber, Dominic. 2021. “Klassifizieren – Verknüpfen – Abbilden. Herausforderungen Der Digitalen Repräsentation Hypertextueller Systeme Am Beispiel Des Klingentaler Jahrzeitenbuchs H.” Master’s thesis, Basel: University of Basel. https://github.com/DominicWeber/jahrzeitenbuch-h.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/454/index.html",
    "href": "submissions/454/index.html",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "",
    "text": "Cadastral plans are a granular, relatively homogeneous, and abundant source for studying land ownership and spatial relationships in 19th century Western Europe (Clergeot 2007; Lenardo et al. 2021; Lelo 2020). Moreover, from the 18th century, cadastres are generally geometrical, and therefore well aligned with the paradigms of modern geodata. Due to the substantial number of entries and the quantitative nature of these historical sources, digital methods leveraging geographic information systems (GIS) seem most appropriate to approach these archival corpora. In addition, the Napoleonic period resulted in a remarkable harmonization of cadastral ordinances, survey methods, and conventions of representation throughout Europe, which persisted until the middle of the 19th century (Hennet 1811; Soulier and Berdez 1827; Petitpierre 2023). This representational coherence makes cadastral plans an interesting target for automated recognition (Follin, Simonetto, and Chalais 2021; Göderle et al. 2023; Petitpierre, Rappo, and Lenardo 2023; Ares Oliveira et al. 2019). This research explores how 19th-century cadastres can be massively read and vectorised by machines, and how the resulting geodata can be used to study the dynamics of persistence in the parcel fabric and land ownership.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#introduction",
    "href": "submissions/454/index.html#introduction",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "",
    "text": "Cadastral plans are a granular, relatively homogeneous, and abundant source for studying land ownership and spatial relationships in 19th century Western Europe (Clergeot 2007; Lenardo et al. 2021; Lelo 2020). Moreover, from the 18th century, cadastres are generally geometrical, and therefore well aligned with the paradigms of modern geodata. Due to the substantial number of entries and the quantitative nature of these historical sources, digital methods leveraging geographic information systems (GIS) seem most appropriate to approach these archival corpora. In addition, the Napoleonic period resulted in a remarkable harmonization of cadastral ordinances, survey methods, and conventions of representation throughout Europe, which persisted until the middle of the 19th century (Hennet 1811; Soulier and Berdez 1827; Petitpierre 2023). This representational coherence makes cadastral plans an interesting target for automated recognition (Follin, Simonetto, and Chalais 2021; Göderle et al. 2023; Petitpierre, Rappo, and Lenardo 2023; Ares Oliveira et al. 2019). This research explores how 19th-century cadastres can be massively read and vectorised by machines, and how the resulting geodata can be used to study the dynamics of persistence in the parcel fabric and land ownership.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#data-collection-approach",
    "href": "submissions/454/index.html#data-collection-approach",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "Data collection approach",
    "text": "Data collection approach\nOur data collection approach is based on supervised semantic segmentation. Sematic segmentation is a pixel classification method, where each pixel in the image, i.e. the digitised cadastral plats, is attributed a single semantic class (see Figure 1). In the present case, we follow the ontology proposed by (Petitpierre and Guhennec 2023) and annotate the images with four land use classes (built, non-built, water, and road network), plus the contours, which allow us to delineate the geometries (Chen et al. 2021). In total, 78 cadastral plats were manually annotated (42 for Lausanne, 10 for Neuchatel, 8 for Geneva, and 18 from various communes in the states of the First French Empire). Thirty percent of the dataset was preserved for validation (2/3), and testing (1/3). A Mask2Former (Cheng et al. 2022; OpenMMLab 2022) neural model was trained on the remaining labels, using 768x768 image crops, until convergence.\nThe trained segmentation model is used to automate the recognition of the remaining cadastral plats for three cadastral series of Lausanne (Melotte and Perey 1722; Berney 1831; Deluz 1886), which total 570 plats. In a first step, the sheets were georeferenced and separated from the legend. Second, the semantic segmentation was applied directly on the georeferenced images. Third, the resulting predictions masks were manually reviewed, and the most salient inconsistencies were corrected. Finally, the raster masks were vectorized, as described in (Vaienti et al. 2023), resulting in 69,083 extracted geometries. The data collection workload is estimated to 15 workdays for the specific annotation of 42 plats, 5 workdays for the initial correction of the predictions, and 2 workdays (excluding research and development) for the automatic segmentation and vectorisation. By contrast, the estimated workload for manually vectorising the whole corpus would be over 260 days.\n\n\n\n\n\n\nFigure 1: Illustration of the raster/pixel annotation of the 1886 cadastre of Lausanne by Louis Deluz, using five semantic classes (red: built, dark grey: non-built, light grey: road network, violet: water, white: contours, black: background).",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#persistence",
    "href": "submissions/454/index.html#persistence",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "Persistence",
    "text": "Persistence\nWe adopt two complementary approaches for studying the structure of land ownership. The first is based on the measure of persistence in the parcel fabric. The second, which will be presented later, focuses on the social structure of land ownership, through the analysis of owners. Regarding persistence, two processes are considered: the division of a plot into two or more plots, or the fusion –i.e. the grouping– of two or more plots together. We call continuity the state in which parcel fabric is unchanged. Finally, discontinuity denotes a situation in which the parcel fabric has been otherwise altered by rezoning.\nFusion and division are measured separately. The detection of both processes is based on the spatial matching of the parcels in two distinct historical layers. Only child parcels whose overlap with the parent is at least 50% are considered for division, and vice versa for fusion. For fusion, the measure of shape similarity is based on the computation of the absolute difference of turning functions (Arkin et al. 1991) between the child parcels and the parent one, and vice versa. This method, which intuitively focuses on a shape’s salient angles, excels at comparing shapes which exhibit inequal level of detail or resolution, which is precisely the case for the extracted geometries, originating from diverse series1. This process allows us to establish maps of land persistence and continuity (Figure 2, Figure 4, and Figure 6), whose results will be presented and discussed in a chronological order. The relative code is published along with this article.\n\n\n\n\n\n\nFigure 2: Dynamics of land plot persistence in Lausanne between 1722 and 1831. The red areas underwent fusion processes, whereas division dynamics are observed in blue areas. Magenta indicates continuity.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Expansion of the city outside its historical enclosure between 1722 (top) and 1831 (bottom).\n\n\n\nWhile the changes observed in the first time stratum (1722-1831, Figure 2) might appear somehow disordered at first sight, it is in fact due to the complexity of the dynamics observed. At the beginning of the 18th century, the development of the city was mainly contained within its enclosure, to the exception of the three faubourgs: Halle, Martheray and Chêne2. The demand for space was stimulating the densification of buildings within the enclosure (Rickli 1978). The Rôtillon neighbourhood is a good example of this phenomenon. The vegetable gardens found there, whose yield was probably not very profitable since they were in the shadow of the hill of Bourg, were replaced by small buildings.\nFacing continued demand for space, the city’s medieval fortifications were progressively dismantled. The new buildings and the reorganization of land parcels at that time (Figure 2) met four distinct demands. The first two impacted development in the immediate vicinity of the city (Figure 3). First, infrastructures, such as the chapel and the charity school of Valentin, the casino to the south, and the “maison de force” (prison) to the east. Second, the expansion of productive areas, concentrating mainly along the Flon, downstream (sawmills, mills) and upstream (mills, tanneries) of the town. The two remaining demands impacted not only the close suburbs, but the whole commune. The first was the multiplication of dispersed rural housing, with the construction of several small agricultural estates scattered across the territory. We can also observe the emergence of periurban land estates, such as the most spectacular mansion in Mon Repos, owned by Alexandre Perdonnet (Figure 3), but also the new estate of Bellerive (Figure 2), reconstructed around 1787 by the Francillon family, for example (Grandjean 1982). Farms and estates had contrasting effects on the redrawing of parcels, fueling both fusions and divisions. It is noteworthy, however, that their impact is above all localized and dispersed; hence the strong alternation between both dynamics observed in Figure 2. Finally, let us also highlight that many plots to the south-east and south-west of the town show remarkable persistence. These are mainly vineyards, which stability also suggests continuity in the economic and cultural value of these lands.\n\n\n\n\n\n\nFigure 4: Dynamics of land plot persistence in Lausanne between 1831 and 1883.\n\n\n\n\n\n\n\n\n\nFigure 5: Morcellation of the land in Prélaz (west), as depicted in the 1883 cadastre.\n\n\n\nThis stability contrasts with the dynamics observed in the 19th and 20th centuries (Figure 4). Between 1831 and 1883, the prevailing trend was to extend the city westwards (to Prélaz, Figure 5) and southwards (to Georgette), in the direction of the new railway station. This expansion is taking place precisely at the expense of the vineyards. It is characterized by the division of agricultural plots into smaller subdivisions on which mainly apartment buildings are built. Near Ponthaise3, we note a marked fusion trend. This is due to the purchase of fields by the State of Vaud and the Commune for the construction of new training grounds and barracks, to replace the place d’armes in Montbenon, reallocated for the new Federal Court of Justice. It is noteworthy that the plots of land located in the historic city center remain relatively stable, despite the first major works including the construction of the Grand Pont and the vaulting of the Flon and Louve rivers.\n\n\n\n\n\n\nFigure 6: Dynamics of land plot persistence in Lausanne between 1883 and 2023.\n\n\n\n\n\n\n\n\n\n\nFigure 7: Dynamics of parcel fusions in the city center between 1883 (top) and 2023 (bottom).\n\n\n\nFrom 1883 onwards, on the contrary, the layout of the downtown area changed significantly (Figure 6, Figure 7). Following the 1896 publication of the Schnetzler report on health conditions, most of the buildings, deemed dilapidated, and particularly the entire Rôtillon district, were demolished to be replaced by modern housing. The scale of this work can be seen in the impressive dynamics of plot mergers, through which old buildings were being replaced by larger constructions. Urban sprawl, on the other hand, is characterized by a dynamic of morcellation, which no longer impacts only the former vineyards, but in fact all agricultural land, which was sold in allotments to meet the high demand for city. The main exceptions are large infrastructures such as the Milan, Mon Repos, Valency and Bourget public parks, the Montoie cemetery, the Sébeillon train marshalling yard, the Blécherette airport, the CHUV hospital and the Rovéréaz agricultural estate. However, despite the time that separates both cadastres, several plots of land remain, such as the urban houses in Maupas.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#social-structure-of-land-ownership",
    "href": "submissions/454/index.html#social-structure-of-land-ownership",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "Social structure of land ownership",
    "text": "Social structure of land ownership\nTo investigate the social structure of land ownership, we adopt a distinct approach. More specifically, we rank the owners in the 1831 cadastre of Lausanne by the total area they own. From this, we randomly pick a representative sample of 60 owners. We cross-reference these owners by looking for a corresponding entry in the 1832 and 1835 population censuses of Lausanne Petitpierre, Kramer, and Rappo (2023). We gather two kinds of information from that source: occupation, and number of servants. This makes it possible to relate the ownership strata back with the social status of individuals.\nThe aggregated results of that step are described in Figure 8. The first thing that can be noted is the great disparity in the repartition of land ownership. The upper decile owned several hectares of land, which almost always included a landed estate and its dependencies, a vineyard, farmlands, and forests. Most of them simply define their occupation as “owners” (propriétaires); other occupations in the sample include “banker” and “marshal”. Just after come also an “annuitant” (rentier) and a “district judge”. This social stratum also typically employs one, two, or up to eight household servants. Later in the sample, we also encounter farmers, winegrowers, and forester. These typically own farmlands, along with a farm, usually comprising a stable, barn and hayloft. Finally, a third category of owners is composed of craftsmen and merchants. They usually own a house inside the city, sometimes accompanied by a courtyard, a garden, barn or other annexes. They sometimes employ one or two servants. Finally, the last category, by far the largest, is composed of non-owners. They were 86.4% of the adult population at the time. A large part of them were tenants, but also servants or boarders, who did not event live with a nuclear family of their own. In comparison, in 2022, the non-owners represent 61.3% of the households. Even if we exclude the fact that, in 1831, married women were generally denied shared ownership, we still reach an estimate of 72% of men who were non-owners. The Gini index of land ownership in Lausanne in 1831 would equal to 0.9, which indicates a particularly unequal distribution4.\n\n\n\n\n\n\nFigure 8: Representative samples of owners (n=60) from the 1831 cadastre, ranked by total area owned (blue, in hectares), along with the number of servants employed in their household (red). The occupation of each owner, extracted from the 1832 or 1835 population census, is indicated after their names, in the labels of the horizontal axis.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#general-discussion-and-conclusion",
    "href": "submissions/454/index.html#general-discussion-and-conclusion",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "General discussion and Conclusion",
    "text": "General discussion and Conclusion\nSpatial matching and the measure of morphological proximity is key to harnessing the wealth of geodata and operationalizing the analysis of plot dynamics. Spatial analysis of the territory reveals complex, protean trends that would be difficult to uncover without a spatial, quantitative and comparative approach based on digitized sources. Although certain manual steps, such as georeferencing, are still required, segmentation and vectorization are instrumental to processing such large cartographic corpora.\nDigitization also makes it possible to create links between sources, such as cadastres and censuses, by making their content searchable. Disambiguating individuals remains partly qualitative, as it is often necessary to reason on a cluster of clues. This is due in particular to a scant pool of names, resulting in many homonyms: for instance, there were eight household heads named “Jean Louis Blanc” in Lausanne in 1835. Digitisation therefore also highlights this type of contradiction, which might otherwise go unnoticed when reading the sources in progressive order.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#references",
    "href": "submissions/454/index.html#references",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "References",
    "text": "References\n\n\nAres Oliveira, Sofia, Isabella di Lenardo, Bastien Tourenc, and Frederic Kaplan. 2019. “A Deep Learning Approach to Cadastral Computing.” In DH2019. Utrecht, Netherlands. https://dev.clariah.nl/files/dh2019/boa/0691.html.\n\n\nArkin, E. M., L. P. Chew, D. P. Huttenlocher, K. Kedem, and J. S. B. Mitchell. 1991. “An Efficiently Computable Metric for Comparing Polygonal Shapes.” IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (3): 209–16. https://doi.org/10.1109/34.75509.\n\n\nBerney, Abraham. 1831. “Plan Cadastral Par l’arpenteur Abraham Berney.”\n\n\nChen, Yizi, Edwin Carlinet, Joseph Chazalon, Clément Mallet, Bertrand Duménieu, and Julien Perret. 2021. “Vectorization of Historical Maps Using Deep Edge Filtering and Closed Shape Extraction.” In Document Analysis and Recognition – ICDAR 2021, edited by Josep Lladós, Daniel Lopresti, and Seiichi Uchida, 510–25. Cham: Springer International Publishing.\n\n\nCheng, Bowen, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. “Masked-Attention Mask Transformer for Universal Image Segmentation.” arXiv. https://doi.org/10.48550/arXiv.2112.01527.\n\n\nClergeot, Pierre. 2007. Cent Millions de Parcelles En France: 1807, Un Cadastre Pour l’Empire. Paris: Editions Publi-Topex.\n\n\nDeluz, Louis. 1886. “Plans de Lausanne.”\n\n\nFollin, Jean-Michel, Élisabeth Simonetto, and Anthony Chalais. 2021. “Détection Automatique Des Parcelles Sur Les Plans Napoléoniens : Comparaison de Deux Méthodes.” Humanités Numériques, no. 3 (May). https://doi.org/10.4000/revuehn.1779.\n\n\nGöderle, Wolfgang, Christian Macher, Katrin Mauthner, Oliver Pimas, and Fabian Rampetsreiter. 2023. “AI-Driven Structure Detection and Information Extraction from Historical Cadastral Maps (Early 19th Century Franciscean Cadastre in the Province of Styria) and Current High-Resolution Satellite and Aerial Imagery for Remote Sensing.” arXiv. https://doi.org/10.48550/arXiv.2312.07560.\n\n\nGrandjean, Marcel. 1982. Lausanne: Villages, Hameaux Et Maisons de l’ancienne Campagne Lausannoise. Birkhäuser. Vol. 4. Monuments d’art Et d’histoire Du Canton de Vaud. Basel.\n\n\nHennet, Albert-Joseph-Ulpien. 1811. Recueil Méthodique Des Lois, Décrets, Règlemens, Instructions Et Décisions Sur Le Cadastre de La France. Paris: Imprimerie impériale.\n\n\nLelo, Keti. 2020. “Analysing Spatial Relationships Through the Urban Cadastre of Nineteenth-Century Rome.” Urban History 47 (3): 467–87. https://doi.org/10.1017/S0963926820000188.\n\n\nLenardo, Isabella di, Raphaël Barman, Federica Pardini, and Frédéric Kaplan. 2021. “Une Approche Computationnelle Du Cadastre Napoléonien de Venise.” Humanités Numériques 3 (May). https://doi.org/0.4000/revuehn.1786.\n\n\nMelotte, Sebastian, and Claude Perey. 1722. “Plans Du Territoire de Lausanne.”\n\n\nOpenMMLab. 2022. “MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark.” OpenMMLab. https://github.com/open-mmlab/mmsegmentation.\n\n\nPetitpierre, Remi. 2023. “Mapping Memes in the Napoleonic Cadastre: Expanding Frontiers in Memetics.” In Digital Humanities 2023: Book of Abstracts, 3. Graz, Austria: Zenodo. https://doi.org/10.5281/zenodo.8107916.\n\n\nPetitpierre, Remi, and Paul Guhennec. 2023. “Effective Annotation for the Automatic Vectorization of Cadastral Maps.” Digital Scholarship in the Humanities, March. https://doi.org/10.1093/llc/fqad006.\n\n\nPetitpierre, Remi, Marion Kramer, and Lucas Rappo. 2023. “An End-to-End Pipeline for Historical Censuses Processing.” International Journal on Document Analysis and Recognition (IJDAR), March. https://doi.org/10.1007/s10032-023-00428-9.\n\n\nPetitpierre, Remi, Marion Kramer, Lucas Rappo, and Isabella di Lenardo. 2023. “1805-1898 Census Records of Lausanne : A Long Digital Dataset for Demographic History.” https://doi.org/10.5281/zenodo.7711640.\n\n\nPetitpierre, Remi, Lucas Rappo, and Isabella di Lenardo. 2023. “Recartographier l’espace Napoléonien.” In. https://hal.science/hal-04109214.\n\n\nRickli, Jean Daniel. 1978. “Lausanne: Deux Siècles de Devenir Urbain.” Habitation 51 (12).\n\n\nSoulier, and Berdez. 1827. Instructions Données Par Le Département Des Finances Pour La Levée Des Plans Et l’établissement Du Cadastre, Ensuite de La Décision Du Conseil d’Etat Du 6 Décembre 1826. Frères Blanchard. Lausanne.\n\n\nVaienti, Beatrice, Rémi Petitpierre, Isabella di Lenardo, and Frédéric Kaplan. 2023. “Machine-Learning-Enhanced Procedural Modeling for 4D Historical Cities Reconstruction.” Remote Sensing 15 (13): 3352. https://doi.org/10.3390/rs15133352.",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/454/index.html#footnotes",
    "href": "submissions/454/index.html#footnotes",
    "title": "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that here we use the absolute difference L1 instead of the squared difference L2, which is favored by Arkin. This is intentional, as we believe that shape dissimilarities should be equally weighted in the present use case.↩︎\nHere, we use the historical spelling of place names, as given in the sources.↩︎\nidem.↩︎\nIn comparison, we can easily estimate the distribution of fortune in Switzerland in 2022 by interpolating the aggregated statistics of the Federal Statistical Office using cubic spline, which yields a value of 0.82, still corresponding to a less unequal distribution.↩︎",
    "crumbs": [
      "Abstracts",
      "Revealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans"
    ]
  },
  {
    "objectID": "submissions/431/index.html",
    "href": "submissions/431/index.html",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "",
    "text": "The core data of RAG is based on the university registers. The registers usually contain the names and places of origin of the students as well as the date of enrolment. This data is enriched in the research database with biographical data on subjects studied, professional activities and written works. Since 2020, the RAG has been a sub-project of the umbrella project Repertorium Academicum (REPAC), which is being carried out at the Historical Institute of the University of Bern. See on the project and its developments: (Gubler, Hesse, and Schwinges 2022). Data skills in RAG can be divided into data collection, data entry and data analysis. Different data skills are required in the three areas, which have of course also changed over time as a result of digitalisation. While compiling and analysing data has been simplified by computer-aided processes, the precise recording of data in the database still requires in-depth historical knowledge and human intelligence.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#introduction",
    "href": "submissions/431/index.html#introduction",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "",
    "text": "The core data of RAG is based on the university registers. The registers usually contain the names and places of origin of the students as well as the date of enrolment. This data is enriched in the research database with biographical data on subjects studied, professional activities and written works. Since 2020, the RAG has been a sub-project of the umbrella project Repertorium Academicum (REPAC), which is being carried out at the Historical Institute of the University of Bern. See on the project and its developments: (Gubler, Hesse, and Schwinges 2022). Data skills in RAG can be divided into data collection, data entry and data analysis. Different data skills are required in the three areas, which have of course also changed over time as a result of digitalisation. While compiling and analysing data has been simplified by computer-aided processes, the precise recording of data in the database still requires in-depth historical knowledge and human intelligence.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#project-history",
    "href": "submissions/431/index.html#project-history",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Project history",
    "text": "Project history\nThe RAG started with a Microsoft Access database as a multi-user installation. In 2007, the switch was made to a client-server architecture, with MS Access continuing to serve as the front end and a Microsoft SQL server being added as the back end. This configuration had to be replaced in 2017 as regular software updates for the client and server had been neglected. As a result, it was no longer possible to update the MS Access client to the new architecture in good time and the server, which was running on the outdated MS SQL Server 2005 operating system, increasingly posed a security risk. In addition, publishing the data on the internet was only possible to a limited extent, as a fragmented export from the MS SQL server to a MySQL database with a PHP front end was required. In 2017, it was therefore decided to switch to a new system (Gubler 2020).\n\n\n\nFig. 1: Former frontend of the RAG project for data collection in MS Access 2003.\n\n\nOver one million data records on people, events, observations, locations, institutions, sources and literature were to be integrated in a database migration - a project that had previously been considered for years without success. After a evaluation of possible research environments, nodegoat was chosen (Bree and Kessels 2013). Nodegoat was a tip from a colleague who had attended a nodegoat workshop (Gubler 2021b). With nodegoat, the RAG was able to implement the desired functions immediately:\n\nLocation-independent data collection thanks to a web-based front end.\nData visualisations (maps, networks, time series) are integrated directly into nodegoat, which means that exporting to other software is not necessary, but possible.\nResearch data can be published directly from nodegoat without the need to export it to other software.\n\nFrom then on, the RAG research team worked with nodegoat in a live environment in which the data collected can be made available on the Internet immediately after a brief review. This facilitated the exchange with the research community and the interested public and significantly increased the visibility of the research project. The database migration to nodegoat meant that the biographical details of around 10,000 people could be published for the first time, which had previously not been possible due to difficulties in exporting data from the MS SQL server. On 1 January 2018, the research teams at the universities in Bern and Giessen then began collecting data in nodegoat, starting with extensive standardisation of the data. Thanks to a multi-change function in nodegoat, these standardisations could now be carried out efficiently by all users. Institutions where biographical events took place (e.g. universities, schools, cities, courts, churches, monasteries, courts) were newly introduced.\n\n\n\nFig. 2: Frontend of the RAG project for data collection in nodegoat.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#methodology",
    "href": "submissions/431/index.html#methodology",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Methodology",
    "text": "Methodology\nThese institutions were assigned to the events accordingly, which forms the basis for the project’s method of analysis: analysing the data according to the criteria ‘incoming’ and ‘outgoing’ (Gubler 2022). The key questions here are: Which people, ideas or knowledge entered an institution or space?\n\n\n\nFig. 3: Incoming: Places of origin of students at the University of Basel 1460-1550 with the large dot in the centre as the city of Basel., data: repac.ch, 07/2024.\n\n\nHow was this knowledge shared and passed on there? Spaces are considered both as geographical locations and as knowledge spaces within networks of scholars. In addition, the written works of scholars are taken into account in order to document their knowledge. The people themselves are seen as knowledge carriers who acquire knowledge and pass it on. Consequently, the people are linked to their knowledge in the database using approaches from the history of knowledge (Steckel 2015). The methodology described can therefore not only be used to research the circulation of knowledge between individuals and institutions, but also to digitally reconstruct spheres of influence and knowledge, for example by discipline: Spaces that were shaped by jurists, Physicians or theologians. The map shows places or regions where a particularly large number of Basel jurists were active. The second graphic shows the network of the same group with famous Bonifacius Amerbach as a strong link in the centre. The network is formed based on a Force-directed graph.\n\n\n\nFig. 4: Outgoing: Spheres of activity of jurists with a doctorate from the University of Basel 1460-1550., data: repac.ch, 07/2024.\n\n\n\n\n\nFig. 5: Network: Jurists with a doctorate from the University of Basel 1460-1550., data: repac.ch, 07/2024.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#data-literacy",
    "href": "submissions/431/index.html#data-literacy",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Data literacy",
    "text": "Data literacy\nStudents and researchers working on the RAG project can acquire important data skills. We can make a distinction, as said, between the skills required to collect, enter and analyse the biografical data. Key learning content related to the data entering process for students working in the RAG project are:\n\nBasics of data modelling\n\nBasic knowledge of the use of digital research tools and platforms. Students learn how to design and adapt data structures in order to systematically enter, manage and analyse historical information. They understand how to define entities (such as people, places, events) and their relationships.\n\nBasics of data collection\n\nThe collection of data in a historical project involves several steps and methods to ensure data consistency. In the project, students learn how to search and evaluate sources based on research questions and extract the relevant information. Both quantitative and qualitative approaches are considered in the methods of data collection. An SNSF Spark project provides an example of a quantitative approach on dynamic data ingestion of linked open data in one nodegoat environment (Gubler 2021a)\n\nData entry and management\n\nStudents acquire practical experience in entering and maintaining data within a digital research environment. Additionally, they learn to document workflows and data sources to ensure transparency and traceability. For effective data entry, both students and researchers must develop essential skills related to the extraction and evaluation of historical information.\n\nSource criticism and information extraction\n\nThe project’s most challenging task is extracting relevant biographical information from sources and literature and systematically recording and documenting it in the database according to project-specific guidelines. The goal is to achieve the highest possible standardization to ensure data quality and consistency. Specifically, students must select life events from approximately 900 biographical categories to accurately record an event. These categories are divided into three major blocks: 1) personal data (birth, death, social and geographical origin, etc.), 2) academic data (specializations, degrees), and 3) professional activities. These encompass all potential fields of activity in both ecclesiastical and secular administration in the late Middle Ages. Collecting data and accurately evaluating information from sources and research literature is a demanding task that requires a solid knowledge of history and Latin.\nKey learning content related to data analysis is:\n\nLearning how to query a database. The use of filters and search functions for targeted data analysis requires a solid understanding of the data model, the data collection methodology, and the available content. For an initial overview of the data and, if necessary, for in-depth analysis, AI tools for data analysis will also be used in the project in the future. Such tools can help with data retrieval, as the data can be queried using natural language prompts.\nGeographical and temporal visualisations\n\nUse of GIS functionalities to create and analyse geographical maps. Visualisation of historical data on time axes to show chronological processes and changes.\n\nNetwork analysis\n\nKnowing and applying methods for linking different data sets and for analysing networks and interactions between historical actors such as people, institutions, objects and others. The data can also be exported from nodegoat in order to evaluate it with other visualisation software, for example such as Gephi for network analyses. The graphic shows the general settings in nodegoat for network analyses.\n\n\n\nFig. 6: General settings for network analyses in nodegoat.\n\n\n\nInterpretation of the digital findings (patterns, developments)\n\nThe most important skill in the entire research process is, of course, the ability to interpret the results. The data is always interpreted against the background of the historical context. Without well-founded historical expertise, however, the data cannot provide in-depth insights for historical research, but at best enable superficial observations. It follows that when working with research data, a double source criticism must always take place: when obtaining the information from the sources (data collection) and when analysing the digital results obtained from the information (data interpretation).",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#digitisation",
    "href": "submissions/431/index.html#digitisation",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Digitisation",
    "text": "Digitisation\nHow have the described data competences changed since the start of the project in 2001? This question is linked to changes in the research infrastructure, the availability of digitised material (sources and literature) and with the question of how computer-aided automation, in particular, artificial intelligence have influenced and will influence the practices of data collection, entry and analysis in the project, expanding the epistemological framework? The most important factors in connection with digitalisation in general are:\n\nResources: The increasing availability of digitized texts, particularly through Google Books, has significantly transformed prosopographical and biographical research. Not only is a wealth of information more accessible today, but it can also be entered into databases more efficiently. Consequently, skills for digital research and information processing have had to be continuously adapted throughout the course of the project.\nTools: Since the start of the project, new software tools have significantly transformed the processes of collecting, extracting, entering, and analyzing information. The most substantial development has been in data analysis, which, thanks to advanced tools and user-friendly graphical interfaces, has become accessible to a wide range of researchers, no longer being limited to computer scientists. AI tools for data analysis also open up huge potential for data analysis. Large amounts of data can be analyzed in a short time using simple query languages. However, when using AI, the results must be examined even more critically than with conventional data analysis.\nData analysis: The visualization of research data in historical studies has seen significant advancements. For instance, data can now be displayed on historical maps, within networks, or in time series, and dynamically over time using a time slider in a research environment like nodegoat. This has accelerated data analysis: tasks like creating a map, which took weeks in the early years of the RAG project, now take only a few minutes.\nInterpretation of the data: The core method of historical scholarship, source criticism, has also evolved significantly. While it traditionally involved evaluating information from sources and literature, today it also requires the ability to analyze data visualizations and network representations derived from these sources. To adequately assess these digital findings, a thorough understanding of the data model, data context, and historical background is essential. Consequently, data analysis presents new challenges for historical research, necessitating advanced data competencies at multiple levels.\nCollaboration: Web-based research environments have made collaboration much easier and more transparent. Teams are now able to follow each other’s progress in real time, making the location of the work less important and communication smoother.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#human-and-artificial-intelligence",
    "href": "submissions/431/index.html#human-and-artificial-intelligence",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Human and artificial intelligence",
    "text": "Human and artificial intelligence\nRegarding data collection, entry, and analysis, artificial intelligence significantly impacts several, though not all, tasks within the RAG project.\n\nData collection: AI supports the rapid processing and pre-sorting of digital information used for data collection. For example, Transkribus is utilized to create OCR texts, which are then directly imported into nodegoat and matched with specific vocabularies using algorithms (Gubler 2023). This technology aids the RAG project by efficiently detecting references to students and scholars within large text corpora, significantly speeding up the identification and extraction process.\n\n\n\n\nFig. 7: Example settings for the algorithm for reconciling textual data in nodegoat.\n\n\n\nData entry: In this area, human intelligence remains crucial. In-depth specialist knowledge of the historical field under investigation is essential, particularly concerning the history of universities and knowledge in the European Middle Ages and the Renaissance. Due to the heterogeneous and often fragmented nature of the sources, AI cannot yet replicate this expertise. The nuanced understanding required to interpret historical events and their semantic levels still necessitates human insight.\nData analysis: While AI support for data entry is still limited, it is much greater for data analysis. The epistemological framework has expanded considerably not only in digital prosopography and digital biographical research, but in history in general. Exploratory data analysis in particular will become a key methodology in history through the application of AI.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#conclusion",
    "href": "submissions/431/index.html#conclusion",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Conclusion",
    "text": "Conclusion\nSince the 1990s, digital resources and tools have become increasingly prevalent in historical research. However, skills related to handling data remain underdeveloped in this field. This gap is not due to a lack of interest from students, but rather stems from a chronic lack of available training opportunities. This situation has gradually improved in recent years, with a growing number of courses and significant initiatives promoting digital history. Nevertheless, the responsibility now lies with academic chairs to take a more proactive role in integrating a sustainable range of digital courses into the general history curriculum. It is crucial that data literacy becomes a fundamental component of the training for history students, particularly considering their future career prospects and the increasingly complex task of evaluating information, including the critical use of artificial intelligence methods, tools and results. Especially with regard to the methodology of source criticism, which is now more important than ever in the evaluation of AI-generated results. In addition to formal teaching, more project-based learning should be offered to support students in acquiring digital skills.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#references",
    "href": "submissions/431/index.html#references",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "References",
    "text": "References\n\n\nBree, Pim van, and Geert Kessels. 2013. Nodegoat: A Web-Based Data Management, Network Analysis & Visualisation Environment, Http://Nodegoat.net from LAB1100, Http://Lab1100.com. https://nodegoat.net.\n\n\nGubler, Kaspar. 2020. Database Migration Case Study: Repertorium Academicum Germanicum (RAG). histdata.hypotheses.org. https://doi.org/10.58079/pldk.\n\n\n———. 2021a. Data Ingestion Episode III – May the Linked Open Data Be with You. histdata.hypotheses.org. https://doi.org/10.58079/pldv.\n\n\n———. 2021b. The Coffee Break as a Driver of Science: Nodegoat @ Uni Bern (2017-2021). histdata.hypotheses.org. https://doi.org/10.58079/ple4.\n\n\n———. 2022. Von Daten zu Informationen und Wissen. Zum Stand der Datenbank des Repertorium Academicum Germanicum, in: Kaspar Gubler, Christian Hesse, Rainer C. Schwinges (Hrsg.): Person und Wissen. Bilanz und Perspektiven (RAG Forschungen 4). vdf,Zürich. https://boris.unibe.ch/174773/2/Gubler__Von_Daten_zu_Informationen_und_Wissen.pdf.\n\n\n———. 2023. Transkribus kombiniert mit nodegoat: Ein vielseitiges Werkzeug für Datenanalysen. histdata.hypotheses.org. https://doi.org/10.58079/plex.\n\n\nGubler, Kaspar, Christian Hesse, and Rainer Christoph Schwinges. 2022. Person und Wissen. Bilanz und Perspektiven (RAG Forschungen 4). vdf,Zürich. https://doi.org/10.3218/4114-9.\n\n\nSteckel, Sita. 2015. Wissensgeschichten. Zugänge, Probleme und Potentiale in der Erforschung mittelalterlicher Wissenskulturen, in: Akademische Wissenskulturen. Praktiken des Lehrens und Forschens vom Mittelalter bis zur Moderne, hg. v. Martin Kintzinger / Sita Steckel. Bern. https://repositorium.uni-muenster.de/document/miami/6532a89c-da39-4d14-9a28-f550471da4e7/steckel_2015_wissensgeschichte.pdf.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/452/index.html",
    "href": "submissions/452/index.html",
    "title": "Belpop, a history-computer project to study the population of a town during early industrialization",
    "section": "",
    "text": "The Belpop project aims to reconstruct the demographic behavior of the population of a mushrooming working-class town during industrialization: Belfort. Belfort is a hapax in the French urban landscape of the 19th century, as the demographic growth of its main working- class district far outstripped that of the most dynamic Parisian suburbs. The underlying hypothesis is that the massive Alsatian migration following the 1870-71 conflict, along with concomitant industrialization and militarization of the city, profoundly altered the demographic behavior of the people of Belfort.\nThis makes Belfort an ideal place to study the sexualization of social relations in 19th-century Europe. These relationships will first be understood through the study of out-of-wedlock births, in their socio-cultural and bio-demographic dimensions. In line with our initial hypothesis, the random sampling of 1010 birth certificates to build a manual transcription database shows that the number of births outside marriage peaks in the century in the decade following the annexation of Alsace-Moselle to Germany and then remains at a high level. In fact, after 1870, Alsatians arrived in the city in droves, first to escape annexation by Germany, then to follow Alsatian employers who were relocating their mechanical and textile industries to Belfort: the documents needed for marriage were now found beyond the new border, while the presence of a large military population (the department had by far the highest number of single men at the beginning of the 20th century) fueled legal and illegal prostitution and, in part, the number of births out of wedlock. In the long term, this project will also enable us to answer many other questions related to event history analysis, a method that is currently undergoing major development, thanks to artificial intelligence (AI), and which is profoundly modifying the questions raised by historical demography and social history.\nThe contributions of deep learning make it possible to plan a complete analysis of Belfort’s birth (ECN) and death (ECD) civil registers (1807-1919), thanks to HTR methods applied to these sources (two interdisciplinary computer science-history theses in progress). This project is part of the SOSI CNRS ObHisPop (Observatoire de l’Histoire de la Population française: grandes bases de données et IA), which federates seven laboratories and aims to share the advances of interdisciplinary research in terms of automating the constitution of databases in historical demography. Challenges also include linking (matching individual data) the ECN and ECD databases, and eventually the DMC database (DMC is the city’s main employer of women).\nThe Belfort Civil Registers of Birth comprise 39,627 birth declarations inscribed in French in a hybrid format (printed and handwritten text). The declarations consist of four components (declaration number, declaration name, primary paragraph, marginal annotations) and provide information such as the child’s name, parent’s name, date of birth, and other details. The pages of the registers have been scanned at a resolution of 300 dpi each.\n\n\n\nA sample image from the ECN dataset with each four components\n\n\nStudying these invaluable resources is crucial for understanding the expansion of civilizations within Belfort. This necessitates establishing a knowledge database comprising all the information offered by these registers utilizing Artificial intelligence techniques, such as printed and handwritten text recognition models. The development of these models requires a training dataset to address the challenges imposed by these historical documents, such as text style variation, skewness, and overlapping words and text lines. Two stages have been carried on to construct the training dataset. First, manual transcription of 1,010 declarations and 984 marginal annotations with a total of 21,939 text lines, 189,976 words, and 1,177,354 characters. This stage involves employing structure tags like XML tags to identify the characteristics of the declarations. Second, an automatic text line detection method is utilized to extract the text lines within the primary paragraphs and the marginal annotation images in a polygon boundary to preserve the handwritten text. The method is developed based on analyzing the gaps between two consecutive text lines within the images. The detection process initially identifies the core of the text lines regardless of text skewness. Moreover, the process identifies the gaps based on the identified cores. The number of gaps between each two lines is determined based on a predefined parameter value. Each gap is analyzed by examining the density of black pixels within the central third of the gap region. If the black pixel density is low, a segment point is placed at the center. Otherwise, a histogram analysis is performed to identify minimum valleys, which are then used as potential segment points. Finally, all the localized segment points are connected to form the boundary of the text in a polygon shape. The Intersection over Union (IoU), Detection Rate (DR), Recognition Accuracy (RA), and F-Measure (FM) metrics have been employed to provide a comprehensive evaluation of different performance aspects of the method, achieving accuracies of 97.5% IoU, 99% DA, 98% (RA), and 98.50% (FM) for the detection of the text lines within the primary paragraphs. Moreover, the marginal annotations exhibit accuracies of 93.1%, 96%, 94%, and 94.79% across the same metrics, respectively. A structured data tool has been developed for correlating the extracted text line images with their corresponding transcriptions at both the paragraph and text line levels by generating .xml files. These files structure the information within the registers based on the reading order of the components within the document and assign a unique index number for each. Additionally, several essential properties are incorporated within each component block, including the component name, the coordinates within the image, and the corresponding transcribed text. The .xml file generation processes are ongoing to expand the structured declarations to enrich the dataset essential for training artificial intelligence models.\nBelfort Civil Registers of Death (ECD) are composed of 39,238 death declarations with 18,381 fully handwritten certificates and 20,857 hybrid certificates. This corpus spans from 1807 to 1919. ECDs have the same resolution (300 dpi) and the same structure as the Civil Registers of Birth (ECN). The information given by each declaration is somewhat different: the name, the age, the profession of the deceased, the place of death, and even the profession of the witness, can be found. Concerning ECDs, a different strategy was chosen for the text segmentation and the data extraction: the Document Attention Network (DAN). This network recently published is used to get rid of the pre-segmentation step which is highly beneficial for the heterogeneity of our dataset. It was developed for the recognition of handwritten dataset such as READ 2016 and RIMES 2009. Moreover, this architecture can focus on relevant parts of the document, improving the precision and identifying and extracting specific segments of interests. The choice was also made because this network is very efficient in handling large volumes of data while maintaining data integrity. The DAN architecture is made of a Fully Convolutional Network (FCN) encoder to extract feature maps of the input image. This type of network is the most popular approach for pixel-pixel document layout analysis because it maintains spatial hierarchies. Then, a transformer is used as a decoder to predict sequences of variable length. Indeed, the output of this network is a sequence of tokens describing characters of the French language or layout (beginning of paragraph or end of page for instance). These layout tokens or tags were made to structure the layout of a register double page and to unify the ECD and ECN datasets. The ECD training dataset was built by picking around four certificates each year of the full dataset. For the handwritten records (1807-1885) the first two declarations of the double page were annotated and the first four for the hybrid records (1886-1919). This led to annotating 460 declarations for the first period and 558 declarations for the second one to give a total of 1118 annotated death certificates. We are currently verifying these annotations to start the pre-training phase of the DAN in the coming months.",
    "crumbs": [
      "Abstracts",
      "Belpop, a history-computer project to study the population of a town during early industrialization"
    ]
  },
  {
    "objectID": "submissions/452/index.html#extended-abstract",
    "href": "submissions/452/index.html#extended-abstract",
    "title": "Belpop, a history-computer project to study the population of a town during early industrialization",
    "section": "",
    "text": "The Belpop project aims to reconstruct the demographic behavior of the population of a mushrooming working-class town during industrialization: Belfort. Belfort is a hapax in the French urban landscape of the 19th century, as the demographic growth of its main working- class district far outstripped that of the most dynamic Parisian suburbs. The underlying hypothesis is that the massive Alsatian migration following the 1870-71 conflict, along with concomitant industrialization and militarization of the city, profoundly altered the demographic behavior of the people of Belfort.\nThis makes Belfort an ideal place to study the sexualization of social relations in 19th-century Europe. These relationships will first be understood through the study of out-of-wedlock births, in their socio-cultural and bio-demographic dimensions. In line with our initial hypothesis, the random sampling of 1010 birth certificates to build a manual transcription database shows that the number of births outside marriage peaks in the century in the decade following the annexation of Alsace-Moselle to Germany and then remains at a high level. In fact, after 1870, Alsatians arrived in the city in droves, first to escape annexation by Germany, then to follow Alsatian employers who were relocating their mechanical and textile industries to Belfort: the documents needed for marriage were now found beyond the new border, while the presence of a large military population (the department had by far the highest number of single men at the beginning of the 20th century) fueled legal and illegal prostitution and, in part, the number of births out of wedlock. In the long term, this project will also enable us to answer many other questions related to event history analysis, a method that is currently undergoing major development, thanks to artificial intelligence (AI), and which is profoundly modifying the questions raised by historical demography and social history.\nThe contributions of deep learning make it possible to plan a complete analysis of Belfort’s birth (ECN) and death (ECD) civil registers (1807-1919), thanks to HTR methods applied to these sources (two interdisciplinary computer science-history theses in progress). This project is part of the SOSI CNRS ObHisPop (Observatoire de l’Histoire de la Population française: grandes bases de données et IA), which federates seven laboratories and aims to share the advances of interdisciplinary research in terms of automating the constitution of databases in historical demography. Challenges also include linking (matching individual data) the ECN and ECD databases, and eventually the DMC database (DMC is the city’s main employer of women).\nThe Belfort Civil Registers of Birth comprise 39,627 birth declarations inscribed in French in a hybrid format (printed and handwritten text). The declarations consist of four components (declaration number, declaration name, primary paragraph, marginal annotations) and provide information such as the child’s name, parent’s name, date of birth, and other details. The pages of the registers have been scanned at a resolution of 300 dpi each.\n\n\n\nA sample image from the ECN dataset with each four components\n\n\nStudying these invaluable resources is crucial for understanding the expansion of civilizations within Belfort. This necessitates establishing a knowledge database comprising all the information offered by these registers utilizing Artificial intelligence techniques, such as printed and handwritten text recognition models. The development of these models requires a training dataset to address the challenges imposed by these historical documents, such as text style variation, skewness, and overlapping words and text lines. Two stages have been carried on to construct the training dataset. First, manual transcription of 1,010 declarations and 984 marginal annotations with a total of 21,939 text lines, 189,976 words, and 1,177,354 characters. This stage involves employing structure tags like XML tags to identify the characteristics of the declarations. Second, an automatic text line detection method is utilized to extract the text lines within the primary paragraphs and the marginal annotation images in a polygon boundary to preserve the handwritten text. The method is developed based on analyzing the gaps between two consecutive text lines within the images. The detection process initially identifies the core of the text lines regardless of text skewness. Moreover, the process identifies the gaps based on the identified cores. The number of gaps between each two lines is determined based on a predefined parameter value. Each gap is analyzed by examining the density of black pixels within the central third of the gap region. If the black pixel density is low, a segment point is placed at the center. Otherwise, a histogram analysis is performed to identify minimum valleys, which are then used as potential segment points. Finally, all the localized segment points are connected to form the boundary of the text in a polygon shape. The Intersection over Union (IoU), Detection Rate (DR), Recognition Accuracy (RA), and F-Measure (FM) metrics have been employed to provide a comprehensive evaluation of different performance aspects of the method, achieving accuracies of 97.5% IoU, 99% DA, 98% (RA), and 98.50% (FM) for the detection of the text lines within the primary paragraphs. Moreover, the marginal annotations exhibit accuracies of 93.1%, 96%, 94%, and 94.79% across the same metrics, respectively. A structured data tool has been developed for correlating the extracted text line images with their corresponding transcriptions at both the paragraph and text line levels by generating .xml files. These files structure the information within the registers based on the reading order of the components within the document and assign a unique index number for each. Additionally, several essential properties are incorporated within each component block, including the component name, the coordinates within the image, and the corresponding transcribed text. The .xml file generation processes are ongoing to expand the structured declarations to enrich the dataset essential for training artificial intelligence models.\nBelfort Civil Registers of Death (ECD) are composed of 39,238 death declarations with 18,381 fully handwritten certificates and 20,857 hybrid certificates. This corpus spans from 1807 to 1919. ECDs have the same resolution (300 dpi) and the same structure as the Civil Registers of Birth (ECN). The information given by each declaration is somewhat different: the name, the age, the profession of the deceased, the place of death, and even the profession of the witness, can be found. Concerning ECDs, a different strategy was chosen for the text segmentation and the data extraction: the Document Attention Network (DAN). This network recently published is used to get rid of the pre-segmentation step which is highly beneficial for the heterogeneity of our dataset. It was developed for the recognition of handwritten dataset such as READ 2016 and RIMES 2009. Moreover, this architecture can focus on relevant parts of the document, improving the precision and identifying and extracting specific segments of interests. The choice was also made because this network is very efficient in handling large volumes of data while maintaining data integrity. The DAN architecture is made of a Fully Convolutional Network (FCN) encoder to extract feature maps of the input image. This type of network is the most popular approach for pixel-pixel document layout analysis because it maintains spatial hierarchies. Then, a transformer is used as a decoder to predict sequences of variable length. Indeed, the output of this network is a sequence of tokens describing characters of the French language or layout (beginning of paragraph or end of page for instance). These layout tokens or tags were made to structure the layout of a register double page and to unify the ECD and ECN datasets. The ECD training dataset was built by picking around four certificates each year of the full dataset. For the handwritten records (1807-1885) the first two declarations of the double page were annotated and the first four for the hybrid records (1886-1919). This led to annotating 460 declarations for the first period and 558 declarations for the second one to give a total of 1118 annotated death certificates. We are currently verifying these annotations to start the pre-training phase of the DAN in the coming months.",
    "crumbs": [
      "Abstracts",
      "Belpop, a history-computer project to study the population of a town during early industrialization"
    ]
  },
  {
    "objectID": "submissions/687/index.html",
    "href": "submissions/687/index.html",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "",
    "text": "As historians today, we profit from an unmatched availability of historical sources online, with most of the information contained in these sources digitally accessible. This greatly facilitates the use of computer-assisted methods to support or augment historical analyses. How and when to use which methods in a research endeavor are questions that cannot easily be answered, as the application of appropriate techniques more often than not is something to be clarified or revised during a project. Therefore, we need to find a way to not only teach computer-assisted methods to history students, but also how to enable them to conceptualize a historical research project and how to solve technical problems along the way, empowering them to develop and apply different methods in a practical and inspiring way. In the following, I will discuss an approach that proposes designing semester-long courses with a thematic focus, where students progressively learn how to use computational tools through continuous engagement with a historical source.",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/687/index.html#introduction",
    "href": "submissions/687/index.html#introduction",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "",
    "text": "As historians today, we profit from an unmatched availability of historical sources online, with most of the information contained in these sources digitally accessible. This greatly facilitates the use of computer-assisted methods to support or augment historical analyses. How and when to use which methods in a research endeavor are questions that cannot easily be answered, as the application of appropriate techniques more often than not is something to be clarified or revised during a project. Therefore, we need to find a way to not only teach computer-assisted methods to history students, but also how to enable them to conceptualize a historical research project and how to solve technical problems along the way, empowering them to develop and apply different methods in a practical and inspiring way. In the following, I will discuss an approach that proposes designing semester-long courses with a thematic focus, where students progressively learn how to use computational tools through continuous engagement with a historical source.",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/687/index.html#motivation-and-course-design",
    "href": "submissions/687/index.html#motivation-and-course-design",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "Motivation and Course Design",
    "text": "Motivation and Course Design\nIn a text-based field like history, techniques such as text recognition, text/data mining or natural language processing are very valuable for historical analyses (see for example Jockers and Underwood 2016). However, university courses for history students should go beyond merely teaching a specific technique. They should also equip digital novices with the skills to navigate the digital realm, whether that involves (basic) computer skills, effective collaboration on projects or questions related to data management (for two recent handbooks on how to teach digital history see Battershill and Ross 2022; Guiliano 2022).\nOver the years, I have experimented with various course designs, with introductions to specific software as well as to programming languages. Approaching the topic from the perspective of a course based on programming in order to analyse historical sources, however, has consistently produced the best results in both project outcomes and course evaluations.1 Now, with the rise of large language models, it has been argued that AI can easily generate any script, prompting some to question the necessity of teaching programming. For effective use of this technology, though, learning basic programming skills is essential. Relying on AI generated output without understanding its mechanics will result in mistakes, unnoticed misinterpretations, and, eventually, useless research. By learning the basics of scripting, students not only acquire the ability to perform their own analyses on a data set, but also learn how to use generative AI productively, enabling them to critically assess, correct and refine the output.\nThe current curriculum at the Department of History at the University of Basel does not include foundational, semester-long courses that cover digital literacy or computational skills on a broad basis for all students. Since 2022, however, a self-paced introductory course to digital history has become a mandatory part of the first semester (Serif 2022). This course provides an initial overview of digital methods and their use for historical research, along with a practical component where students learn to apply different methods to a corpus. They are gently introduced to the command line,2 learning about APIs, regular expressions, string extraction, automation and other relevant techniques, as well as ways to visualize first results. By encountering a computer-based approach to historical sources early in their studies, students become more aware of the subject when planning their courses for the following semesters.\nIn the absence of a comprehensive introductory course, the digital history courses I offer still begin at a basic level. By showing students the command line as a way to use the computer, I aim to dispel any unfounded fears and encourage a different way of thinking: A task that initially seems overwhelming can be broken down into several small steps, leading to its completion. I let students work on a small multi-step task that increases their motivation and demonstrates the potential relevance of these methods for historical research. The courses are student-project based, and while we also discuss some examples of digital history projects and reflect on the methods used (reading assignments include Romein et al. 2020; Graham et al. 2022; and Lemercier and Zalc 2019), the focus lies on learning by doing, this is by working with their own material, towards the completion of their project.\nWhen developing such a course, I take inspiration from the problems I face. In ongoing research, for example, I am examining book advertisements placed in an early modern newspaper.3 One part of this project requires a matching of the advertised titles with an existing database of printed books4 in order to enrich the dataset with additional information such as format, number of pages, edition, or genre. To achieve this, one has to overcome a series of obstacles, such as extracting book titles from an advertisement, creating database queries, transforming the received format, handling missing or incorrect metadata, and adding the information to the original dataset. Neither a simple list of commands nor an out-of-the-box solution exists to solve all these problems, encountered while working on the source, at once. However, when tackled individually, each step becomes more understandable and manageable for a programming novice. Using this as a classroom example, programming becomes directly linked to a specific historical source to answer very concrete questions, for example as simple as determining the number of book titles advertised in the newspaper during a particular year.\nThrough small programming exercises, students learn principles of automation, standardization, scalability, etc., while also understanding the importance of metadata and data formats. Examples are drawn from the same historical corpora that will be used in their projects later in the semester, allowing the students to become familiar with both the methods and the sources. This approach helps students gradually understand the potential of computer-assisted analysis and how to apply it to historical research. After being equipped with the technical basics, students form small groups to develop research questions that they would like to explore using digital methods. At the end of the semester, they present their findings, including any challenges faced, discuss how their analyses addressed their initial questions, and reflect on further analyses that could be performed and new questions that arise in light of their results. This structure ensures that students remain connected to the historical material, learn programming not merely for its own sake and identify usefulness and understand limitations of computer-assisted methods in answering historical research questions.",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/687/index.html#conclusion-and-outlook",
    "href": "submissions/687/index.html#conclusion-and-outlook",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "Conclusion and Outlook",
    "text": "Conclusion and Outlook\nThe described courses provide an opportunity for every student to learn how to use computer-assisted methods for historical research. From the course evaluations we know that the courses have been largely appreciated, and that there is a strong demand for more classes of this kind. Furthermore, a significant portion of the participants come from other humanities disciplines, as their own curricula lack equivalent courses. Admittedly, the learning curve is quite steep, and the pace at the beginning is fast. In the current setting, this is unavoidable, but those who persevere often enjoy experimenting with their new skills and achieve unexpected results.\nSo far, only few students choose to focus on computational analyses for their bachelor’s or master’s thesis,5 mostly because they do not feel fully confident with their new skill set (and also because potential supervisors often lack sufficient expertise to support them). Consequently, changes in the humanities curriculum seem necessary if we aim to educate more students in digital methods for historical research. With the increasing prominence of large language models, it seems all the more crucial to ensure that future historians can produce verifiable and reproducible results, leveraging computer-assisted methods both effectively and meaningfully.",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/687/index.html#references",
    "href": "submissions/687/index.html#references",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "References",
    "text": "References\n\n\nBattershill, Claire, and Shawna Ross. 2022. Using Digital Humanities in the Classroom: A Practical Introduction for Teachers, Lecturers, and Students. Second edition. London New York Oxford New Delhi Sydney: Bloomsbury Academic.\n\n\nBlaney, Jonathan, Jane Winters, Sarah Milligan, and Martin Steer. 2021. Doing Digital History: A Beginner’s Guide to Working with Text as Data. IHR Research Guides. Manchester: Manchester University Press.\n\n\nDickmann, Lars. 2022. “Topographien Des Verlorenen. Zur Praxis Des Verlierens Und Findens Im Basler ‘Avisblatt,’ 1729-1844.” Thesis, Universität Basel. https://edoc.unibas.ch/91836/.\n\n\nGraham, Shawn, Ian Milligan, Scott B Weingart, and Kim Martin. 2022. Exploring Big Historical Data: The Historian’s Macroscope. 2nd ed. WORLD SCIENTIFIC. https://doi.org/10.1142/12435.\n\n\nGuiliano, Jennifer. 2022. A Primer for Teaching Digital History: Ten Design Principles. Design Principles for Teaching History. Durham: Duke University Press.\n\n\nJockers, Matthew L., and Ted Underwood. 2016. “Text-Mining the Humanities.” In A New Companion to the Digital Humanities, edited by Susan Schreibman, Ray Siemens, and John Unsworth, 291–306.\n\n\nLemercier, Claire, and Claire Zalc. 2019. Quantitative Methods in the Humanities. An Introduction. Charlottesville: University of Virginia Press.\n\n\nRomein, C. Annemieke, Max Kemman, Julie M. Birkholz, James Baker, Michel De Gruijter, Albert Meroño‐Peñuela, Thorsten Ries, Ruben Ros, and Stefania Scagliola. 2020. “State of the Field: Digital History.” History 105 (365): 291–312. https://doi.org/10.1111/1468-229X.12969.\n\n\nSerif, Ina. 2022. “Introduction to Digital History.” https://wissen-ist-acht.github.io/digitalhistory.intro/.",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/687/index.html#footnotes",
    "href": "submissions/687/index.html#footnotes",
    "title": "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEither R or Python is taught, as both offer a wide range of packages and libraries for humanities data as well as abundant tutorials for different methods.↩︎\nProbably most discussed if necessary or not – I found confirmation for introducing the command line among others in (Blaney et al. 2021).↩︎\nThe subset of book ads had been created in the context of a SNF project, see https://avisblatt.philhist.unibas.ch/.↩︎\nVerzeichnis Deutscher Drucke des 18. Jahrhunderts VD18, https://vd18.k10plus.de.↩︎\nSome of the underlying ideas for the analytic part in the master thesis of Dickmann (2022) was developed by him in a course of mine in fall 2020, see https://github.com/LarsDIK/avis-analysis.↩︎",
    "crumbs": [
      "Abstracts",
      "Go Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research"
    ]
  },
  {
    "objectID": "submissions/447/index.html",
    "href": "submissions/447/index.html",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "",
    "text": "The movement of Open Science has grown in importance in the Humanities, advocating for better accessibility of scientific research, especially in the form of the publication of research data (UNESCO 2023). This has led funding agencies like SNSF, ANR, and Horizon Europe to ask research projects to publish their research data and metadata along the FAIR principles in public repositories (see for instance (ANR 2023; EC 2023; SNSF 2024). Such requirements are putting pressure on researchers, who need to learn and understand the principles and standards of FAIR data and its impact on research data, but also require them to acquire new methodologies and know-how, such as in data management and data science.\nAt the same time, this accessibility of an increasing volume of interoperable quality data and the new semantic methodologies might bring a change of paradigm in the Humanities by the way knowledge is produced (Beretta 2023; Feugère 2015). The utilization of Linked Open Data (LOD) grants scholars access to large volumes of interoperable and high-quality datasets, at a scale analogue methods cannot reach, fundamentally altering their approach to information. This enables scholars to pose novel research questions, marking a departure from traditional modes of inquiry and facilitating a broader range of analytical perspectives within academic discourse. Moreover, drawing upon semantic methodologies rooted in ontology engineering, scholars can effectively document the intricate complexities inherent of social and historical phenomena, enabling a nuanced representation essential to the Social Sciences and Humanities domains within their databases. This meticulous documentation not only reflects a sophisticated understanding of multifaceted realities but also empowers researchers to deepen the digital analysis of rich corpora.\nThe transition from analogical to digital research methodologies does not come without challenges for researchers, thus necessitating the development of new tools and research infrastructures to support them in this evolution. The demand arises for user-friendly tools that abstract the technical complexity, as well as project accompaniment organisations that can provide support in digital methodologies and strategies to help scholars to better manage their data for computational analysis and information sharing.\nThis is the goal of Geovistory. It is conceived as a virtual research and data publication environment designed to strengthen Open Research Data practices. Geovistory is developed for research projects in the Humanities and Social Sciences, whether in history, geography, literature or other related fields, according to the participatory method of “user experience design”. It supports researchers with simple and easy-to-use interfaces and allows them to make their research accessible in an attractive way to people interested in history.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#introduction",
    "href": "submissions/447/index.html#introduction",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "",
    "text": "The movement of Open Science has grown in importance in the Humanities, advocating for better accessibility of scientific research, especially in the form of the publication of research data (UNESCO 2023). This has led funding agencies like SNSF, ANR, and Horizon Europe to ask research projects to publish their research data and metadata along the FAIR principles in public repositories (see for instance (ANR 2023; EC 2023; SNSF 2024). Such requirements are putting pressure on researchers, who need to learn and understand the principles and standards of FAIR data and its impact on research data, but also require them to acquire new methodologies and know-how, such as in data management and data science.\nAt the same time, this accessibility of an increasing volume of interoperable quality data and the new semantic methodologies might bring a change of paradigm in the Humanities by the way knowledge is produced (Beretta 2023; Feugère 2015). The utilization of Linked Open Data (LOD) grants scholars access to large volumes of interoperable and high-quality datasets, at a scale analogue methods cannot reach, fundamentally altering their approach to information. This enables scholars to pose novel research questions, marking a departure from traditional modes of inquiry and facilitating a broader range of analytical perspectives within academic discourse. Moreover, drawing upon semantic methodologies rooted in ontology engineering, scholars can effectively document the intricate complexities inherent of social and historical phenomena, enabling a nuanced representation essential to the Social Sciences and Humanities domains within their databases. This meticulous documentation not only reflects a sophisticated understanding of multifaceted realities but also empowers researchers to deepen the digital analysis of rich corpora.\nThe transition from analogical to digital research methodologies does not come without challenges for researchers, thus necessitating the development of new tools and research infrastructures to support them in this evolution. The demand arises for user-friendly tools that abstract the technical complexity, as well as project accompaniment organisations that can provide support in digital methodologies and strategies to help scholars to better manage their data for computational analysis and information sharing.\nThis is the goal of Geovistory. It is conceived as a virtual research and data publication environment designed to strengthen Open Research Data practices. Geovistory is developed for research projects in the Humanities and Social Sciences, whether in history, geography, literature or other related fields, according to the participatory method of “user experience design”. It supports researchers with simple and easy-to-use interfaces and allows them to make their research accessible in an attractive way to people interested in history.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#geovistory-as-a-research-environment",
    "href": "submissions/447/index.html#geovistory-as-a-research-environment",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Geovistory as a Research Environment",
    "text": "Geovistory as a Research Environment\nGeovistory aims to be a comprehensive research environment that accompanies scholars throughout the whole research cycle. Geovistory includes:\n\nThe Geovistory Toolbox, which allows to manage and curate projects’ research data. The Toolbox is freely accessible for all individual projects. Each research project works on its own data perspective but at the same time directly contributes to a joint knowledge graph.\nA joint Data repository that allows to connect and link the different research projects under a unique and modular ontology, thus creating a large Knowledge Graph.\nThe Geovistory Publication platform (http://geovistory.org), where data is published using the RDF framework and can be accessed via the community page or project-specific webpages and its graphical search tools or a SPARQL-endpoint.\nAn active Community to foster information and know-how exchange among the researchers, users and technological experts.\n\n\n\n\n\n\n\nFigure 1\n\n\n\nAs per current terms of service, all data produced in the information layer of Geovistory are licensed under creative commons BY-SA 4.0. Initiated by KleioLab GmbH, the different infrastructure components are currently being developed jointly by LARHRA and the University of Bern, while other actors are welcome to join the Geovistory vision.. All the web components and the publication platform have been made available as open source, as well as the toolbox. The LOD4HSS project (https://www.geovistory.org/lod4hss), co-funded by swissuniversities, structures these efforts and aims at creating a larger community of users and supporters of this vision.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#the-aim-of-breaking-information-silos",
    "href": "submissions/447/index.html#the-aim-of-breaking-information-silos",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "The aim of breaking information silos",
    "text": "The aim of breaking information silos\nThe goal of producing and publishing FAIR research data is to break the information silos that hinder the sharing and reusing of scientific data. However, achieving interoperability hinges on two critical components (Beretta 2024a):\n\nFirstly, the unambiguous identification of real-world entities (e.g., persons, places, concepts) with unique identifiers (e.g., URIs in Linked Open Data) and the establishment of links between identical entities across different projects (e.g., ensuring that the entity “Paris” is identified by the same URI in all projects);\nSecondly, the utilization of explicit ontologies that can be aligned across projects. Nevertheless, mapping between ontologies may prove challenging, or even unfeasible, particularly when divergent structural frameworks are employed (e.g., an event-centric ontology may have limited compatibility with an object-centric one).\n\nIn Geovistory, those challenges are addressed by producing a unique Knowledge Graph that integrates the various projects. This necessitates from each project the adherence to the Semantic Data for History and Social Sciences (SDHSS) ontology ecosystem. It includes a methodology of ontological foundational analysis, based on the principles of OntoClean, from the domain of semantic engineering (Guarino and Welty 2004), and the high-level conceptual categories of the DOLCE ontology (Borgo et al. 2022). This has been applied to the CIDOC CRM ontology, the ICOM standard for the Heritage domain, while extending it to include the social and mental realities crucial for documenting essential aspects of human history, like ownership, membership, collective beliefs, etc. (Beretta 2024b). On this basis, a standardised semantic methodology for the development of domain-oriented ontologies in different fields of the Humanities, such as archaeology, prosopography, and geography has been created.. The SDHSS ontology ecosystem provides adaptability to the specificities of the various research projects while ensuring full interoperability among them. It is collaboratively managed in the ontome.net (http://ontome.net) application, so that scholars and domain experts can participate in its development if interested.\nThis shared Knowledge Graph streamlines the entity creation process by enabling users to navigate the graph, identify existing objects, and reuse them in their project using the same URIs for entity identification. By leveraging a common ontology ecosystem, users can not only easily identify and reuse information pertaining to specific entities but also ensure seamless integration and interoperability across projects within the Geovistory platform.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#a-modular-system-for-managing-complex-hss-information",
    "href": "submissions/447/index.html#a-modular-system-for-managing-complex-hss-information",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "A modular system for managing complex HSS information",
    "text": "A modular system for managing complex HSS information\nScholars within the Humanities domain grapple with intricate information, significantly more complex when compared to other scientific disciplines. Historical sources, whether textual, oral, visual, or material, provide fragmented and biased glimpses into the past, necessitating contextualization and interpretation. Consequently, this dynamic can engender a considerable degree of information uncertainty and discordance that need to be meticulously documented. Any digital infrastructure or model employed must adeptly navigate this multifaceted information landscape and accommodate its inherent complexity.\nAn inherent strength of Geovistory lies in its handling of the challenges associated with scientific information in the Humanities and Social Sciences domain. Noteworthy among these challenges is the nuanced, context-sensitive nature of information and its relation with different research agendas, as well as the wide variations in meaning for the same terms and vocabulary complexities, competing views or gaps and fragmentation of available information. These complexities are deftly managed through the application of the SDHSS methodology, which tends to limit the number of classes and properties in the ontology ecosystem, while inviting projects to develop and share rich collections of controlled vocabularies of concepts that enrich the data model according to the different research agendas and perspectives.\nMoreover, the project-partition of the Knowledge Graph within Geovistory enables users to repurpose existing information while also accommodating contradictory data, particularly when discrepancies are identified by researchers. Each project graph is stored within a designated dataset, maintaining its individual identity within the overarching Knowledge Graph. This approach allows for the coexistence and contextualization of disparate interpretations of facts, enhancing the platform’s flexibility and adaptability to varying scholarly perspectives. It is the unique amalgamation of the Geovistory graph data model and its robust semantic enrichment capabilities that render it particularly compelling for research within the Humanities and Social Sciences.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#integrating-the-dh-ecosystem",
    "href": "submissions/447/index.html#integrating-the-dh-ecosystem",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Integrating the DH ecosystem",
    "text": "Integrating the DH ecosystem\nOperating within the framework of Linked Open Data principles entails establishing connections with disparate datasets housed in various open and online repositories or Knowledge Graphs, culminating in the creation of an inclusive and interconnected Web of Data—an accomplishment characterized as the fifth star of Tim Berners Lee’s Open Data (https://5stardata.info/en/). As datasets interlink, they collectively form the Linked Open Data Cloud (https://lod-cloud.net/), wherein predominant repositories such as Wikidata or DBpedia, alongside authority files such as VIAF or GND, assume pivotal roles as data hubs, enhancing the discoverability, contextualization, and citability of information.\nThe Geovistory ecosystem applies those principles, actively engaging with the Digital Humanities landscape. It is connected dynamically to the information systems of producers of authority records (such as IdRef, GND) and data repositories (such as Wikidata) in view of interconnecting bibliographic information systems and scale up to a large Knowledge Graph. Collaborative efforts include the establishment of a data exchange pipeline with the French Agence Bibliographique de l’Enseignement Supérieur (ABES), with ongoing initiatives to forge additional partnerships.\nMoreover, ensuring long-term preservation of research data remains imperative, with initiatives to archive completed projects in the Zenodo repository and explore potential collaborations with entities like DaSCH, OLOS, and Huma-Num for dynamic updates and data management, with preliminary engagements initiated with DaSCH.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#conclusions-and-future-perspectives",
    "href": "submissions/447/index.html#conclusions-and-future-perspectives",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Conclusions and future perspectives",
    "text": "Conclusions and future perspectives\nGeovistory has been designed as a comprehensive research environment tailored by and for historians and humanists to address their needs in generating and utilizing FAIR data, thereby streamlining the research digitization process. As the utilization of Geovistory proliferates across more projects, the Knowledge Graph grows with increasingly enriched information, rendering the overall environment more advantageous for scholars either by providing reusable datasets or by enriching imported data. In this regard, Geovistory can be compared as a Wikidata dedicated to research endeavors, with the difference that projects retain full control over their data without a loss of semantic coherence throughout the graph.\nThe forthcoming years mark a critical juncture for Geovistory, as the tools and infrastructures of the environment recently transitioned into the public domain. This needed change will ease collaboration with future public institutions within Europe, but a greater part of public fundings will be needed to ensure the sustainability of the ecosystem.\nNonetheless, the Digital Humanities ecosystem remains unstable, attributed to the lack of sustained funding for infrastructural initiatives by national funding agencies and the absence of cohesive coordination among institutions. To ameliorate this landscape, prioritizing the establishment of robust collaborations and partnerships among diverse tools and infrastructures in Switzerland and Europe is imperative. Leveraging the specialized expertise of each institution holds the promise of engendering a harmonized and synergistic, distributed environment conducive to scholarly pursuits.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#references",
    "href": "submissions/447/index.html#references",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "References",
    "text": "References\n\n\nANR. 2023. “La Science Ouverte.” https://anr.fr/fr/lanr/engagements/la-science-ouverte/.\n\n\nBeretta, Francesco. 2023. “Données Ouvertes Liées Et Recherche Historique : Un Changement de Paradigme.” Humanités Numériques 7. https://doi.org/10.4000/revuehn.3349.\n\n\n———. 2024a. “Données Liées Ouvertes Et Référentiels Publics : Un Changement de Paradigme Pour La Recherche En Sciences Humaines Et Sociales.” Arabesques 112: 26–27.\n\n\n———. 2024b. “Semantic Data for Humanities and Social Sciences (SDHSS): An Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse.” In Professorale Karrieremuster. Entwicklung Einer Wissenschaftlichen Methode Zur Forschung Auf Online Verfügbaren Und Verteilten Forschungsdatenbanken Der Universitätsgeschichte, edited by Thomas Riechert, Hartmurt Beyer, Jennifer Blanke, and Edgard Marx, 73–101. Leipzig: International Handbooks on Information Systems.\n\n\nBorgo, Stefano, Roberta Ferrario, Aldo Gangemi, Nicola Guarino, Claudio Masolo, Daniele Porello, Emilio M. Sanfilippo, and Laure Vieu. 2022. “DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering.” Applied Ontology 17: 45–69.\n\n\nEC. 2023. “Open Data, Software and Code Guidelines.” https://open-research-europe.ec.europa.eu/for-authors/data-guidelines#standardsandfair.\n\n\nFeugère, Michel. 2015. “Les Bases de Données En Archéologie. De La Révolution Informatique Au Changement de Paradigme.” Cahiers Philosophiques 141: 139–47.\n\n\nGuarino, Nicola, and Chistopher A. Welty. 2004. “An Overview of OntoClean.” In Handbook on Ontologies, edited by Steffen Staab and Rudi Studer, 151–71. Berlin AND Heidelberg: International Handbooks on Information Systems.\n\n\nSNSF. 2024. “Open Research Data.” https://www.snf.ch/en/dMILj9t4LNk8NwyR/topic/open-research-data.\n\n\nUNESCO. 2023. “UNESCO Recommendation on Open Science.” https://www.unesco.org/en/open-science/about?hub=686.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#repository-structure",
    "href": "about.html#repository-structure",
    "title": "",
    "section": "Repository Structure",
    "text": "Repository Structure\nThe structure of this repository follows the Advanced Structure for Data Analysis of The Turing Way and is organized as follows:\n\nsubmissions/ Contains the submissions for the conference.\nbook-of-abstracts.md The introduction to the book of abstracts.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#use",
    "href": "about.html#use",
    "title": "",
    "section": "Use",
    "text": "Use\nThis data is openly available to everyone and can be used for any research or educational purpose. If you use this data in your research, please cite as specified in CITATION.cff. The following citation formats are also available through Zenodo:\n\nBibTeX\nCSL\nDataCite\nDublin Core\nDCAT\nJSON\nJSON-LD\nGeoJSON\nMARCXML\n\nZenodo provides an API (REST & OAI-PMH) to access the data. For example, the following command will return the metadata for the most recent version of the data\ncurl -i https://zenodo.org/api/records/ZENODO_RECORD",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#support",
    "href": "about.html#support",
    "title": "",
    "section": "Support",
    "text": "Support\nThis project is maintained by @digihistch24. Please understand that we can’t provide individual support via email. We also believe that help is much more valuable when it’s shared publicly, so more people can benefit from it.\n\n\n\nType\nPlatforms\n\n\n\n\n🚨 Bug Reports\nGitHub Issue Tracker\n\n\n📊 Report bad data\nGitHub Issue Tracker\n\n\n📚 Docs Issue\nGitHub Issue Tracker\n\n\n🎁 Feature Requests\nGitHub Issue Tracker\n\n\n🛡 Report a security vulnerability\nSee SECURITY.md\n\n\n💬 General Questions\nGitHub Discussions",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#roadmap",
    "href": "about.html#roadmap",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\n\nAdd all submissions to the Book of Abstracts\nAdd a Table of Contents to the Book of Abstracts\nAdd an introduction to the Book of Abstracts\nAdd styling to the Book of Abstracts",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "",
    "section": "Contributing",
    "text": "Contributing\nAll contributions to this repository are welcome! If you find errors or problems with the data, or if you want to add new data or features, please open an issue or pull request. Please read CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#versioning",
    "href": "about.html#versioning",
    "title": "",
    "section": "Versioning",
    "text": "Versioning\nWe use SemVer for versioning. The available versions are listed in the tags on this repository.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#authors-and-acknowledgment",
    "href": "about.html#authors-and-acknowledgment",
    "title": "",
    "section": "Authors and acknowledgment",
    "text": "Authors and acknowledgment\n\nMoritz Mähr - Initial work - maehr\nMoritz Twente - Submissions - mtwente\nKapitolina Kostina - Submissions - consincopa\n\nSee also the list of contributors who contributed to this project.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "",
    "section": "License",
    "text": "License\nThe data in this repository is released under the Creative Commons Attribution 4.0 International (CC BY 4.0) License - see the LICENSE-CCBYSA file for details. By using this data, you agree to give appropriate credit to the original author(s) and to indicate if any modifications have been made.\nThe code in this repository is released under the GNU Affero General Public License v3.0 - see the LICENSE-AGPL file for details. By using this code, you agree to make any modifications available under the same license.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "LICENSE-CCBYSA.html",
    "href": "LICENSE-CCBYSA.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nAttribution-ShareAlike 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nAdditional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0",
    "crumbs": [
      "Abstracts",
      "About",
      "License (Data)"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "",
    "section": "Our Pledge",
    "text": "Our Pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "",
    "section": "Our Standards",
    "text": "Our Standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "",
    "section": "Enforcement Responsibilities",
    "text": "Enforcement Responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at digital-history-2024@unibas.ch. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "",
    "section": "Enforcement Guidelines",
    "text": "Enforcement Guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "",
    "section": "Unreleased",
    "text": "Unreleased\n\nFeatures\n\nInitial version",
    "crumbs": [
      "Abstracts",
      "About",
      "Changelog"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Book of Abstracts",
    "section": "",
    "text": "The DigiHistCH24 conference on “Historical Research, Digital Literacy, and Algorithmic Criticism” brings together scholars and professionals to address the evolving role of digital technologies in historical research. Hosted by the University of Basel on September 12–13, 2024, the conference will focus on the integration of digital tools, the importance of digital literacy, and the critical examination of algorithms within the discipline.\nThis book of abstracts contains all the papers and posters presented at the conference, providing a comprehensive overview of current research at the intersection of history and digital technology. These cover a range of topics, from innovative methodologies and software applications to the challenges of digital data management and algorithmic analysis in historical research.\nWe are pleased to present this collection, which reflects the state of the art in digital history, and anticipate that the discussions it stimulates will make a significant contribution to the field.",
    "crumbs": [
      "Abstracts"
    ]
  },
  {
    "objectID": "index.html#paper",
    "href": "index.html#paper",
    "title": "Book of Abstracts",
    "section": "Paper",
    "text": "Paper\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Session\n        \n         \n          Title\n        \n         \n          Author(s)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nSession\n\n\nTitle\n\n\nAuthor(s)\n\n\n\n\n\n\nKeynote\n\n\nWhen Literacy Goes Digital: Rethinking the Ethics and Politics of Digitisation\n\n\nGerben Zaagsma\n\n\n\n\nSession 1A\n\n\nUsing GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)\n\n\nEliane Schmid\n\n\n\n\nSession 1A\n\n\nRevealing the Structure of Land Ownership through the Automatic Vectorisation of Swiss Cadastral Plans\n\n\nRémi Petitpierre, Isabella di Lenardo, Lucas Rappo\n\n\n\n\nSession 1B\n\n\nTables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.\n\n\nGabi Wuethrich\n\n\n\n\nSession 1B\n\n\nTeaching the use of Automated Text Recognition online. Ad fontes goes ATR\n\n\nLaura Bitterli, Lorenz Dändliker\n\n\n\n\nSession 2A\n\n\nFrom manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)\n\n\nKaspar Gubler\n\n\n\n\nSession 2A\n\n\nLearning to Read Digital? Constellations of Correspondence Project and Humanist Perspectives on the Aggregated 19th-century Finnish Letter Metadata\n\n\nHanna-Leena Paloposki, Ilona Pikkanen\n\n\n\n\nSession 2A\n\n\nData Literacy and the Role of Libraries\n\n\nCatrina Langenegger, Johanna Schüpbach\n\n\n\n\nSession 2B\n\n\nA Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents\n\n\nJérôme Baudry, Nicolas Chachereau\n\n\n\n\nSession 3A\n\n\nData-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries\n\n\nNadezhda Povroznik\n\n\n\n\nSession 3A\n\n\nImpresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.\n\n\nMaud Ehrmann, Raphaëlle Ruppen Coutaz, Simon Clematide, Marten Düring\n\n\n\n\nSession 3A\n\n\nGeovistory, a LOD Research Infrastructure for Historical Sciences\n\n\nStephen Hart, Vincent Alamercery, Francesco Beretta, Djamel Ferhod, Sebastian Flick, Tobias Hodel, David Knecht, Gaétan Muck, Alexandre Perraud, Morgane Pica, Pierre Vernus\n\n\n\n\nSession 3B\n\n\n20 godparents and 3 wives – studying migrant glassworkers in post-medieval Estonia\n\n\nMonika Reppo\n\n\n\n\nSession 3B\n\n\nConnecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity\n\n\nChristian Forney, Martin Stuber\n\n\n\n\nSession 4A\n\n\nA handful of pixels of blood\n\n\nAdrian Demleitner\n\n\n\n\nSession 4A\n\n\nFilms as sources and as means of communication for knowledge gained from historical research\n\n\nPeter Moser, Andreas Wigger\n\n\n\n\nSession 4A\n\n\nDigital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”\n\n\nMoritz Greiner-Petter, Sarine Waltenspül\n\n\n\n\nSession 4B\n\n\nTowards Computational Historiographical Modeling\n\n\nMichael Piotrowski\n\n\n\n\nSession 4B\n\n\nOn the Historiographic Authority of Machine Learning Systems\n\n\nDominic Weber\n\n\n\n\nSession 5A\n\n\nTraining engineering students through a digital humanities project: Techn’hom Time Machine\n\n\nCyril Lacheze, Marina Gasnier\n\n\n\n\nSession 5A\n\n\nContributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students\n\n\nFrancesco Beretta\n\n\n\n\nSession 5A\n\n\nGo Digital, They Said. It Will Be Fun, They Said. Teaching DH Methods for Historical Research\n\n\nIna Serif\n\n\n\n\nSession 6A\n\n\nOn a solid ground. Building software for a 120-year-old research project applying modern engineering practices\n\n\nChristian Sonder, Bastian Politycki\n\n\n\n\nSession 6A\n\n\nTheory and Practice of Historical Data Versioning\n\n\nIsabella di Lenardo, Rémi Petitpierre, Lucas Rappo, Paul Guhennec, Carlo Musso, Nicolas Mermoud-Ghraichy\n\n\n\n\nSession 6A\n\n\nWhen the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections\n\n\nVictoria Gioia Désirée Landau\n\n\n\n\nSession 6B\n\n\nBelpop, a history-computer project to study the population of a town during early industrialization\n\n\nLaurent Heyberger, Gabriel Frossard, Wissam Al-Kendi\n\n\n\n\nSession 6B\n\n\nFrom Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All\n\n\nMoritz Feichtinger\n\n\n\n\nSession 7A\n\n\nEfficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History\n\n\nAnne S. Chao, Yi Zhong, Qiwei Li, Zhandong Liu\n\n\n\n\nSession 7A\n\n\nFrom record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700\n\n\nBenjamin Hitz, Ismail Prada Ziegler, Aline Vonwiller\n\n\n\n\nSession 7A\n\n\nFrom words to numbers. Methodological perspectives on large scale Named Entity Linking\n\n\nTarun Chadha, Gentiana Rashiti, Christiane Sibille, Agnieszka Ilnicka\n\n\n\n\nSession 7B\n\n\nRockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience\n\n\nLudovic Tournès, Yi-Tang Lin\n\n\n\n\nSession 7B\n\n\nDevelop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)\n\n\nIván Lorenci de Francisco\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Abstracts"
    ]
  },
  {
    "objectID": "index.html#poster",
    "href": "index.html#poster",
    "title": "Book of Abstracts",
    "section": "Poster",
    "text": "Poster\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author(s)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nAuthor(s)\n\n\n\n\n\n\nDiscuss Data – an Open Repository for Research and Data Communities\n\n\nTorsten Kahlert, Daniel Kurzawe\n\n\n\n\nEconomies of Space: Opening up Historical Finding Aids\n\n\nLucas Burkart, Tobias Hodel, Benjamin Hitz, Aline Vonwiller, Ismail Prada Ziegler, Jonas Aeby, Katrin Fuchs\n\n\n\n\nModeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities\n\n\nAxel Matthey\n\n\n\n\nMultimodal UI for Video Retrieval at the Swiss Federal Archives\n\n\nAudray Sauvage, Julien A. Raemy\n\n\n\n\nSwiss Google Books for Research\n\n\nMartin Reisacher, Eric Dubey\n\n\n\n\ntranscriptiones – Create, Share and Access Transcriptions of Historical Manuscripts\n\n\nYvonne Fuchs, Dominic Weber\n\n\n\n\n\nNo matching items\n\n\n\n Digital History Switzerland 2024 is sponsored by: \n\n Digital History Switzerland 2024 is hosted and organized by:",
    "crumbs": [
      "Abstracts"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "",
    "section": "Pull Request Process",
    "text": "Pull Request Process\n\nEnsure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Abstracts",
      "About",
      "Contributing"
    ]
  },
  {
    "objectID": "submissions/482/index.html",
    "href": "submissions/482/index.html",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "",
    "text": "Our presentation reflects on the experience gained in the ongoing SNSF-funded research project The Internationalization of Patent Systems: From Patent Cultures to Global Intellectual Property. As recent debates on price of, and access to, patented COVID-19 vaccines have recalled, intellectual property rights are of great importance on a global scale. Our research investigates how patents have become, albeit incompletely, such globally relevant rights. While this internationalization is often seen as the consequence of agreements between macro-actors such as states, this project argues that this internationalization stems equally, if not more, from the networks of actors, economic strategies, texts and images involved in patent practices. To explore these, our project relies on the digital analysis of a large corpus of digitized patent documents, using text mining and computer vision techniques.",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#introduction",
    "href": "submissions/482/index.html#introduction",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "",
    "text": "Our presentation reflects on the experience gained in the ongoing SNSF-funded research project The Internationalization of Patent Systems: From Patent Cultures to Global Intellectual Property. As recent debates on price of, and access to, patented COVID-19 vaccines have recalled, intellectual property rights are of great importance on a global scale. Our research investigates how patents have become, albeit incompletely, such globally relevant rights. While this internationalization is often seen as the consequence of agreements between macro-actors such as states, this project argues that this internationalization stems equally, if not more, from the networks of actors, economic strategies, texts and images involved in patent practices. To explore these, our project relies on the digital analysis of a large corpus of digitized patent documents, using text mining and computer vision techniques.",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#opportunities-of-digital-analysis-for-a-history-of-patent-internationalization",
    "href": "submissions/482/index.html#opportunities-of-digital-analysis-for-a-history-of-patent-internationalization",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "Opportunities of Digital Analysis for a History of Patent Internationalization",
    "text": "Opportunities of Digital Analysis for a History of Patent Internationalization\nOur research project combines traditional historical methods with digital analysis. These various methods are more or less relevant depending on how the “internationalization” of patents is understood. The adoption of a legal agreement (the Paris Convention of 1883) by various states made patents internationally more relevant: because of the emergence of diplomatic discussions in matters of intellectual property, and because it made it easier for corporations to obtain rights for the same technology in different countries simultaneously. Similarly and relatedly, from the late 19th century on, patent specialists gathered in international private networks and advocacy groups. Both of these aspects can be studied through close-reading of archival materials and printed sources.\nDigital methods enable us to go further and examine a third meaning or aspect of internationalization: border-crossing patent practice and related business strategies. Indeed, patenting abroad was not exclusively the activity of multinational companies, nor of their founders and chief engineers, but also of craftsmen and low-level employees. Unlike, say, Thomas Edison or Alexander Graham Bell (see e.g. Beauchamp 2015), these patentees have left few traces outside of the patent record and even fewer, if any, that could shed light on their motivations and strategies. The large-scale digitization of patent documents thus creates the opportunity to study the activity of a wider variety of patentees.\nFurthermore, digital methods allow us to quantitatively study broad changes in patenting practices. For instance, over the period under consideration, a growing proportion of patents were granted or assigned to corporations, rather than individual inventors (Lamoreaux, Sokoloff, and Sutthiphisal 2009; Veyrassat 2001; Fisk 2009, chap. 6). How did this reflect on international patenting? More generally, what proportion of patents was granted, in different countries and at different times, to (foreign) companies and individuals holding similar patents in other countries? How did the situation vary from industry to industry?",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#available-digitized-sources",
    "href": "submissions/482/index.html#available-digitized-sources",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "Available Digitized Sources",
    "text": "Available Digitized Sources\nBecause patents constitute a legal claim of exclusivity, a detailed description of the invention is required by the law, inter alia to allow courts to assess whether competing technical devices or processes constitute illegal imitations. From the second half of the nineteenth century onward, most countries systematically published these descriptions, so-called patent specifications. We rely on a large corpus of these documents that have been digitized by patent offices for their own current activity (especially for assessing the novelty of patent applications).\nOur dataset currently includes around 4 million patent specifications from four large industrial countries: France, Germany, the United Kingdom and the United States. These countries represent major players in the discussions around the Paris Convention. Their residents also account for a large proportion of patents taken abroad. Furthermore, it has been proposed that they constitute distinct “patent cultures” that have constituted models for other countries (Gooday and Wilf 2020). This corpus however reproduces the historiographical tendency to neglect smaller and less-studied countries. To account for this, we plan to include patent specifications from additional states at a later point.\nThe main source of our data is the European Patent Office (EPO). Digitized specifications were downloaded by using Open Patent Services, an API offered by the EPO. We have also downloaded metadata through the same channel, including information such as a title for the patent; the date the patent was applied for; the date it was published; the name and country of the inventor and/or applicant; so-called “family data”, linking this specification to other ones. However, coverage is very uneven. For instance, we have no metadata for most German specifications before 1914.",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#operationalizing-and-exploring-internationalization",
    "href": "submissions/482/index.html#operationalizing-and-exploring-internationalization",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "Operationalizing and Exploring Internationalization",
    "text": "Operationalizing and Exploring Internationalization\nTo quantitatively trace the internationalization of patent practice, we must define this concept more precisely, or rather stipulate how we are going to measure it: we must “operationalize” the concept, i.e. turn it into “a series of operations”, “building a bridge from concepts to measurement, and then to the world” (Moretti 2013, 104). Deciding on a series of operations happens in interaction with a process of close examination and exploration of the dataset.\nSpeaking of a network of text and images implies that we can link patents from one country to those granted in another. The available “family” metadata already does so, but using it as an entry point for exploration shows that we would severely underestimate the extent of international patenting by relying only on that indication. However, this exploration reveals how very similar patents can be, confirming our expectations. We can match specifications by comparing patentee names and titles. Because available metadata is incomplete, these elements also need to be extracted from the patent document itself through text mining.\nExploration leads to a further possible operation: matching the drawings printed in these documents. Unlike the text, which changes between countries because it is translated and sometimes adapted to local patent practices and regulations, the drawings have often been reused (almost) identically from one country to another (Figure 1).\n\n\n\n\n\n\nFigure 1: Example of two pages from different patents featuring the same drawing (left: French patent 325,985; right: German patent 142,688).",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#implementing-image-matching",
    "href": "submissions/482/index.html#implementing-image-matching",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "Implementing Image Matching",
    "text": "Implementing Image Matching\nIn recent years, computational exploration and analysis of images have attracted a growing interest in digital history (Arnold and Tilton 2019, 2023; Wevers and Smits 2020). Among other approaches, historians have used convolutional neural networks (CNNs) to generate numerical representations of images, so-called image embeddings, and then find similar pictures. Pre-trained CNNs have been shown to be useful on historical documents even though they were built for another purpose, typically classifying colored photographs in categories such as iPod and hair_spray, as in the ImageNet training data.\nHowever, in itself, this method proved insufficient for our needs. First, the available images are digitizations of whole pages in the specifications. These pages might not be found to be similar, because of changes in the arrangement of the drawings on the page or because of differences in overall layout (see again Figure 1). To address this first limitation, we segment the pages by identifying regions of contiguous black pixels. A second limitation of using image embeddings for our purpose is that we are not looking for general similarity, e.g. in the overall shape of the figures, but for (near) identity. This distinction could not be made based on similarity measures given by the embeddings, possibly because of how the models were trained.\nAnother family of computer vision algorithms, feature detection and matching, is more appropriate for our goal. Predating the breakthrough of CNNs by a decade, Scale-Invariant Feature Transform (SIFT) remains one of the best available methods (Lowe 2004). SIFT can for instance find a photographed object in another photograph, even if it is scaled or rotated. However, SIFT is computationally intensive, which presents us with a challenge because of the amount of data we process. While faster algorithms exist, they have so far given inferior results on our data, leading especially to many false positives.\nBest results were obtained by combining a CNN and SIFT. Image embeddings and an efficient indexing algorithm (“Hnswlib - Fast Approximate Nearest Neighbor Search” 2024; Malkov and Yashunin 2018) allow us to reduce the search space: instead of using SIFT to compare a segment from a French patent to all the segments of British, German and US patents, we only compare it to the segments with the most similar embeddings. Preliminary results of using this method on French and German patents issued around 1902 indicate a very high precision, with very few false positives (recall still needs to be evaluated).",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#back-to-exploration-and-future-work",
    "href": "submissions/482/index.html#back-to-exploration-and-future-work",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "Back to Exploration and Future Work",
    "text": "Back to Exploration and Future Work\nOur early results prompt further exploration, leading to new insights. For instance, some of the matches point to metadata errors, e.g. wrong country indications in French patents. It also leads us to question some of our assumptions. Assuming that a patent in one country would have one corresponding patent in another, we used embeddings to get, for each segment in country A, the two segments most similar to it in country B. However, in our early results, one French patents matched six different German patents. This suggests that we might need to apply SIFT to compare each segment A to a greater number of similar B segments. Further future work includes combining matching the drawings and matching other data points. All in all, our use of computer vision methods, while not yet robust enough to answer our research questions, yields promising results demonstrating that the concept of internationalization can be operationalized in this way.",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/482/index.html#references",
    "href": "submissions/482/index.html#references",
    "title": "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents",
    "section": "References",
    "text": "References\n\n\nArnold, Taylor, and Lauren Tilton. 2019. “Distant Viewing: Analyzing Large Visual Corpora.” Digital Scholarship in the Humanities 34 (Supplement_1): i3–16. https://doi.org/10.1093/llc/fqz013.\n\n\n———. 2023. Distant Viewing: Computational Exploration of Digital Images. Cambridge (Massachusetts): The MIT Press. https://doi.org/10.7551/mitpress/14046.001.0001.\n\n\nBeauchamp, Christopher. 2015. Invented by Law: Alexander Graham Bell and the Patent That Changed America. Cambridge (Massachusetts): Harvard University Press.\n\n\nFisk, Catherine L. 2009. Working Knowledge. Employee Innovation and the Rise of Corporate Intellectual Property, 1800-1930. Studies in Legal History. Chapel Hill: University of North Carolina Press.\n\n\nGooday, Graeme, and Steven Wilf, eds. 2020. Patent Cultures: Diversity and Harmonization in Historical Perspective. Cambridge Intellectual Property and Information Law. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108654333.\n\n\n“Hnswlib - Fast Approximate Nearest Neighbor Search.” 2024. https://github.com/nmslib/hnswlib.\n\n\nLamoreaux, Naomi R., Kenneth L. Sokoloff, and Dhanoos Sutthiphisal. 2009. “The Reorganization of Inventive Activity in the United States During the Early Twentieth Century.” Working Paper 15440. National Bureau of Economic Research. https://doi.org/10.3386/w15440.\n\n\nLowe, David G. 2004. “Distinctive Image Features from Scale-Invariant Keypoints.” International Journal of Computer Vision 60 (2): 91–110. https://doi.org/10.1023/B:VISI.0000029664.99615.94.\n\n\nMalkov, Yu A, and Dmitry A Yashunin. 2018. “Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.” IEEE Transactions on Pattern Analysis and Machine Intelligence 42 (4): 824–36.\n\n\nMoretti, Franco. 2013. “‘Operationalizing’.” New Left Review, no. 84: 103–19. https://newleftreview.org/issues/ii84/articles/franco-moretti-operationalizing.\n\n\nVeyrassat, Béatrice. 2001. “De la protection de l’inventeur à l’industrialisation de l’invention.” In Innovations : incitations et résistances : des sources de l’innovation à ses effets, edited by Hans-Jörg Gilomen, Rudolf Jaun, Margrit Müller, and Béatrice Veyrassat, 367–83. Zürich: Chronos. https://doi.org/10.5169/seals-16825.\n\n\nWevers, Melvin, and Thomas Smits. 2020. “The Visual Digital Turn: Using Neural Networks to Study Historical Images.” Digital Scholarship in the Humanities 35 (1): 194–207. https://doi.org/10.1093/llc/fqy085.",
    "crumbs": [
      "Abstracts",
      "A Digital History of Internationalization. Operationalizing Concepts and Exploring Millions of Patent Documents"
    ]
  },
  {
    "objectID": "submissions/464/index.html",
    "href": "submissions/464/index.html",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "",
    "text": "Historical objects from ancient civilizations have made their way from their point of origin to institutions worldwide, the majority over the past two centuries, some long before. Today, they are often housed in knowledge, art and cultural institutions – short: GLAMs (galleries, libraries, archives, museums). When an institution makes its holdings available online, it attracts a varied audience with different interests and backgrounds when approaching objects. Among them are researchers investigating items for their archeological and (art) historical properties, or for the textual documents contained on their surface.\nOne such object type are documents on papyrus, predominantly from modern-day Egypt, but also discovered at sites in countries such as Afghanistan, Jordan, Greece and Italy (famously the Pompeii and Herculaneum papyri). While papyrus collections can display common characteristics and patterns of acquisition (e.g., targeted excavation or purchase in the late 19th and early 20th century, private donations and bequests to Classics departments and national museums in the century since), they can differ vastly in their treatment and prioritization by their holding institutions. From fully interoperable, comprehensive metadata generation and object digitization endeavors to mere mentions of the existence of papyri (edited and unedited) at a given institution in print publications, the prerequisites for accessing and interacting with a set of papyri largely depend on institutional investment in their collection.\nSo even before texts deciphered from ancient heritage objects like papyri can or should be used for innovative computational methods – such as machine learning (Sommerschield et al. 2023), digital paleography and character recognition (Marthot-Santaniello 2021), as well as fragment reassembly and reconstruction (Pirrone, Beurton-Aimar, and Journet 2021) –, we stand to benefit from enhancing existing metadata, and taking a look at the institutions and platforms that provide us with the information needed to engage with historical source material digitally.",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#introduction",
    "href": "submissions/464/index.html#introduction",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "",
    "text": "Historical objects from ancient civilizations have made their way from their point of origin to institutions worldwide, the majority over the past two centuries, some long before. Today, they are often housed in knowledge, art and cultural institutions – short: GLAMs (galleries, libraries, archives, museums). When an institution makes its holdings available online, it attracts a varied audience with different interests and backgrounds when approaching objects. Among them are researchers investigating items for their archeological and (art) historical properties, or for the textual documents contained on their surface.\nOne such object type are documents on papyrus, predominantly from modern-day Egypt, but also discovered at sites in countries such as Afghanistan, Jordan, Greece and Italy (famously the Pompeii and Herculaneum papyri). While papyrus collections can display common characteristics and patterns of acquisition (e.g., targeted excavation or purchase in the late 19th and early 20th century, private donations and bequests to Classics departments and national museums in the century since), they can differ vastly in their treatment and prioritization by their holding institutions. From fully interoperable, comprehensive metadata generation and object digitization endeavors to mere mentions of the existence of papyri (edited and unedited) at a given institution in print publications, the prerequisites for accessing and interacting with a set of papyri largely depend on institutional investment in their collection.\nSo even before texts deciphered from ancient heritage objects like papyri can or should be used for innovative computational methods – such as machine learning (Sommerschield et al. 2023), digital paleography and character recognition (Marthot-Santaniello 2021), as well as fragment reassembly and reconstruction (Pirrone, Beurton-Aimar, and Journet 2021) –, we stand to benefit from enhancing existing metadata, and taking a look at the institutions and platforms that provide us with the information needed to engage with historical source material digitally.",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#state-of-collection-metadata",
    "href": "submissions/464/index.html#state-of-collection-metadata",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "State of Collection Metadata",
    "text": "State of Collection Metadata\nMost publications in papyrology up to the 21st century, and especially editio princeps of texts on papyrus, have been print editions, making it the community’s work and responsibility to keep track of digitized papyrological editions and the texts contained in them. This is where digital tools and resources have been developed early on, from creating born-digital editions to digitizing collection holdings (Ast 2022). A starting point for an overview of these collection locations is a dataset expanded from Trismegistos’ TM Collections database, identifying around 800 papyrus-holding institutions in close to 40 countries, offering a sense of the scale of distribution of this ancient heritage object type.\nFor collections that have been digitized, there are questions and problem areas each institution must address throughout the digitization process and after, many of which can start in the analog and are transferred to the digital. Often, these are challenges connected to historical designations, institutional decision-making and accessibility of the ancient heritage objects.\nIn terms of historical designations, this can include terminology (description, typology and classification of texts, e.g., “private letter”, “amulet”, “writing exercise”) and periodization, both for objects dated to specific years (365 BCE) and to broader timespans (4th c. BCE); even assigning objects to categories like “Ptolemaic” and “Late Roman”, or “Greek world” (a cultural or geographic descriptor) is connected to both terminology and periodization. Institutional decision-making over many years further decides the storage of an object; how it is housed and inventoried is a categorization in itself, and designates which specialist or curator is in charge (classifying an object among, e.g., manuscripts, or in the Antiquities, Classics or Archeology category/department). The papyrus-holding institution (as well as the respective cataloger, curator, data manager) also sets a focus for its collections, anticipating or encouraging audience types, selecting presentation forms and grouping items. Lastly, accessibility encompasses many aspects: how findable an object is (its retrievability and being uniquely identifiable), whether understanding can be generated by proper contextualization being in place (object provenance, acquisition), and if connections and links within and beyond the collection are being offered, such as to aggregators, projects or related institutions (seeing as objects are often fragmented, the pieces having been distributed across collections decades ago). It also includes the information being provided, such as how current metadata is (date last edited), who generated it (metadata editor), and whether it can be displayed in more than one language (through tailor-made translations, or the availability of an automatic translation of the webpage as a whole). Here, with multilingualism, one can again run into the terminology problem, and creating equivalence between languages.\nWith all these elements, uniformity can be difficult to implement, particularly if oftentimes there are analog predecessors (e.g., card catalogs) being transposed to digital metadata when a collection is digitized. Standards are not often agreed upon and correctly implemented, and even it they were, accessibility pertains not only to the objects themselves and whether they can be viewed, but also to the historical – even specifically papyrological – education necessary to engage with them. When terms are not even agreed upon by scholars engaging with them every day, how can a wider audience be expected to understand and interact with either the objects or the scholarship derived from them?",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#approaches-to-onlinedigital-collections",
    "href": "submissions/464/index.html#approaches-to-onlinedigital-collections",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "Approaches to Online/Digital Collections",
    "text": "Approaches to Online/Digital Collections\nWhen the digitization of collections is analyzed, it is usually done by approaching a single institution, by conducting a national survey (of e.g., an institution type, such as all museums, or all libraries), or by using a similar institution- or collection-focused scope; thus, there are not many studies of this kind considering a specific object type across institutions worldwide. However, since this is how researchers tend to approach collections (gathering source material on particular topics, geographic regions or time periods by papyri, ostraca, inscriptions, and other object types), and since with papyrology there is a sub-discipline across several fields (Egyptology, Ancient History, Greek Philology, Latin Philology, Coptology, Arabic Studies, Iranian Studies, and more) devoted to their study, using papyri as a case study object for such research is promising.\nPapyri have enjoyed increased accessibility in the past years, thanks in great part to the dedication and work of individuals generating connections to aggregators, such as Trismegistos (Depauw 2018) and Papyri.info (Berkes 2018). Even Europeana, with its aim of being a digital ecosystem for the European cultural heritage sector, has a growing “Papyrus” category (incl. e.g., Rijksmuseum van Oudheden in Leiden), wherein institutions have made their collections available as entries on Europeana, linking back to the museum’s online catalog via an object permalink. Europeana also allows users to customize the website language to one of 24 options. This is rarely the case for institutional websites: the Musées Royaux d’Art et d’Histoire in Brussels have an entirely trilingual web presence (English, Dutch and French), while there have also been interesting compromises, such as in the case of the Medelhavsmuseet in Stockholm, where metadata categories are partially available in both English and Swedish.\nThe digitization of papyri has not always been linked to available digital metadata, meaning photographs and pertinent, connected information were not uploaded at the same time, or made available in one place. This is unfortunate, since having only the image or only the metadata at hand is rarely enough – one often helps correct the other (in the case of, e.g., inventory number mix-ups) and both offer contextualization to one another. While this has not always been the case, nowadays it is usually done this way: today, the British Library and other larger UK institutions will combine individual digitization requests – made by researchers or projects – for items not yet available on their online catalog with a metadata upload, using the requesting party’s expertise on the item. Technical solutions providing images and metadata together, like making a collection accessible online using IIIF (International Image Interoperability Framework), are gaining traction, typically implemented by the holding institution, since an image server is required to store the digitized assets. Biblioteca Universitaria di Bologna is among the institutions that have recently migrated their digital collections system to one that supports IIIF.\nWhen single institutions have not been willing or able to provide collections with a digital presence, there have been successful attempts by collections at pooling their resources: initiatives like the Papyrus Portal as a platform for primarily German papyrus-holding collections, or collaborations like UC Berkeley’s “Regional Partners” (Badè Museum of Biblical Archaeology, California State University, Stanford University, and Washington State University) for uploading to APIS (Advanced Papyrological Information System), now part of Papyri.info.\nWhile DOIs or similar persistent identifiers for single object entries are rare on institutional websites (e.g., Library of Trinity College Dublin), Trismegistos provides persistent identifiers for a number of categories related to its work, such as TM Texts IDs, TM Archive IDs and even TM Collections IDs. Currently, not all institutions with digitized papyri in their own online collections catalog refer to the connected TM identifier(s), much less provide a hyperlink to the related TM Texts entry. This would have to be remedied with institution-specific outreach initiatives. When it comes to the aforementioned fragmentation of once-whole papyri, identified matches should also be pointed out in online catalogs. This is as of yet still a service rendered by Trismegistos Texts, which must also make decisions regarding how to handle linking or merging related, physically separated texts in their IDs.\nMetadata standards from the field of cultural heritage offer themselves for implementation by GLAMs. There are institutional online collection catalogs utilizing Dublin Core (e.g., Biblioteca Universitaria di Bologna) and permitting DCMI metadata downloads (e.g., Library of Trinity College Dublin), but as with any standard, while some objects are equipped with expressive metadata classes, some rely on the «description» class for full text instead of encoding more specific metadata. This is usually a sign that a papyrus has not yet been optimized for online view for any number of reasons, or that papyri represent only a small part of the institution’s overall collection. ICOM’s CIDOC CRM is also becoming increasingly relevant (Liu, Hindmarch, and Hess 2023), including the extension CRMtex intended for cultural heritage objects with textual content, such as ancient documents, akin to CRMsci and CRMarchaeo for scientific and archeological data respectively (Felicetti and Murano 2017).\nPapyrology has benefitted from many of the thought processes and implementations originally intended for its sister-field of epigraphy. Standards have been developed specifically for ancient texts when it comes to transcription (according to the Leiden system) and encoding, namely EpiDoc for the digital edition of ancient texts in TEI XML (Bodard 2010). The EpiDoc Guidelines also offer recommendations for metadata fields aimed at the description of text-bearing objects, which apart from the description of physical characteristics includes provenance information. It also allows for linking to external controlled vocabularies.\nExisting and developing controlled vocabularies and gazetteers for locations (e.g., Getty, Pleiades, Elliott 2021), chronology (e.g., PeriodO, Golden and Shaw 2016) and terminology (e.g., EAGLE, continued by a Working Group at Epigraphy.info, Mannocci et al. 2014; Liuzzo 2018; and FAIR Epigraphy, Heřmánková, Horster, and Prag 2022) have been massive accomplishments, continuously maintained and ready to be utilized. In the direction of accessibility, multilingualism and openness of collections to a wider audience, inspiration could also be drawn from less complex schemata, but highly informative resources, such as the recent IFLA «Open Access Vocabulary», composed in English, with a translation of the terminology into Spanish, Chinese and Arabic (Bradley and Reilly 2024).",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#conclusion",
    "href": "submissions/464/index.html#conclusion",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper is a work-in-progress of a section of an ongoing dissertation project, presenting and discussing preliminary findings and continued research angles. There are further considerations to be made regarding the funding, available infrastructure and size of institutions, the priority-setting of collections, and institutional guidelines that determine if, why and how a collection is digitized. Marked changes can already be seen in the approach of institutions towards their online collections, in the direction of the tenets of the FAIR principles, making their objects more findable (DOIs, consistently maintained platforms), accessible (digitization, machine-readable metadata), interoperable (standards, nomenclature, vocabularies, IIIF) and even reusable (open licensing of images and metadata). This is also seen in how institutions are becoming increasingly sensitized to a broader audience engaging with their collections, with explicit use of the FAIR, and in some cases the CARE principles, in their online collection presentation and the steps taken to get there (Carroll et al. 2021).",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#references",
    "href": "submissions/464/index.html#references",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "References",
    "text": "References\n\n\nAst, Rodney. 2022. “Can the Digital Humanities Make Us Better Humanists? A Case Study in Papyrology.” In Digital Text Analysis of Greek and Latin Sources. Methods, Tools, Perspectives: Special Issue, Classics@ 21, edited by Stelios Chronopoulos, Felix K. Maier, and Anna Novokhatko. https://classics-at.chs.harvard.edu/volume/classics20-digital-text-analysis-of-greek-and-latin-sources/.\n\n\nBerkes, Lajos. 2018. “Perspectives and Challenges in Editing Documentary Papyri Online: A Report on Born-Digital Editions Through Papyri.info.” In Digital Papyrology II. Case Studies on the Digital Edition of Ancient Greek Papyri, edited by Nicola Reggiani, 75–85. Berlin/Boston: De Gruyter. https://doi.org/10.1515/9783110547450-004.\n\n\nBodard, Gabriel. 2010. “EpiDoc: Epigraphic Documents in XML for Publication and Interchange.” In Latin on Stone. Epigraphic Research and Electronic Archives, edited by Francisca Feraudi-Gruénais, 101–18. Lanham, MD: Lexington Books.\n\n\nBradley, Fiona, and Susan Reilly. 2024. “IFLA Open Access Vocabulary.” International Federation of Library Associations and Institutions. https://repository.ifla.org/handle/123456789/3272.\n\n\nCarroll, Stephanie Russo, Edit Herczog, Maui Hudson, Keith Russell, and Shelley Stall. 2021. “Operationalizing the CARE and FAIR Principles for Indigenous Data Futures.” Scientific Data 8 (108): 1–6. https://doi.org/10.1038/s41597-021-00892-0.\n\n\nDepauw, Mark. 2018. “Trismegistos: Optimizing Interoperability for Texts from the Ancient World.” In Crossing Experiences in Digital Epigraphy. From Practice to Discipline, edited by Annamaria De Santis and Irene Rossi, 193–201. Warsaw/Berlin: De Gruyter. https://doi.org/10.1515/9783110607208-016.\n\n\nElliott, Tom. 2021. “The Pleiadic Gaze: Looking at Archaeology from the Perspective of a Digital Gazetteer.” In Classical Archaeology in the Digital Age – the AIAC Presidential Panel: Proceedings of the 19th International Congress of Classical Archaeology. Cologne/Bonn, 22–26 May 2018, edited by Kristian Göransson, 43–51. Archaeology and Economy in the Ancient World. Heidelberg: Propylaeum. https://doi.org/10.11588/PROPYLAEUM.708.\n\n\nFelicetti, Achille, and Francesca Murano. 2017. “Scripta Manent: A CIDOC CRM Semiotic Reading of Ancient Texts.” International Journal on Digital Libraries 18 (4): 263–70. https://doi.org/10.1007/s00799-016-0189-z.\n\n\nGolden, Patrick, and Ryan Shaw. 2016. “Nanopublication Beyond the Sciences: The PeriodO Period Gazetteer.” PeerJ Computer Science 2: 1–18. https://doi.org/10.7717/peerj-cs.44.\n\n\nHeřmánková, Petra, Marietta Horster, and Jonathan Prag. 2022. “Digital Epigraphy in 2022: A Report from the Scoping Survey of the FAIR Epigraphy Project.” Zenodo. https://doi.org/10.5281/ZENODO.6610695.\n\n\nLiu, Fangchao, John Hindmarch, and Mona Hess. 2023. “A Review of the Cultural Heritage Linked Open Data Ontologies and Models.” The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLVIII-M-2-2023: 943–50. https://doi.org/10.5194/isprs-archives-XLVIII-M-2-2023-943-2023.\n\n\nLiuzzo, Pietro M. 2018. “EAGLE Continued: IDEA. The International Digital Epigraphy Association.” In Crossing Experiences in Digital Epigraphy. From Practice to Discipline, edited by Annamaria De Santis and Irene Rossi, 216–30. Warsaw/Berlin: De Gruyter. https://doi.org/10.1515/9783110607208-018.\n\n\nMannocci, Andrea, Vittore Casarosa, Paolo Manghi, and Franco Zoppi. 2014. “The Europeana Network of Ancient Greek and Latin Epigraphy Data Infrastructure.” In Metadata and Semantics Research: 8th Research Conference, MTSR 2014, Karlsruhe, Germany, November 27-29, 2014, Proceedings, edited by Sissi Closs, Rudi Studer, Emmanouel Garoufallou, and Miguel-Angel Sicilia, 286–300. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-13674-5.\n\n\nMarthot-Santaniello, Isabelle. 2021. “D-Scribes Project and Beyond: Building a Virtual Research Environment for the Digital Palaeography of Ancient Greek and Coptic Papyri.” In Ancient Manuscripts and Virtual Research Environments: Special Issue, Classics@ 18, edited by Claire Clivaz and Garrick V. Allen. https://classics-at.chs.harvard.edu/volume/classics18-ancient-manuscripts-and-virtual-research-environments/.\n\n\nPirrone, Antoine, Marie Beurton-Aimar, and Nicholas Journet. 2021. “Self-Supervised Deep Metric Learning for Ancient Papyrus Fragments Retrieval.” International Journal on Document Analysis and Recognition (IJDAR) 24 (3): 219–34. https://doi.org/10.1007/s10032-021-00369-1.\n\n\nSommerschield, Thea, Yannis Assael, John Pavlopoulos, Vanessa Stefanak, Andrew Senior, Chris Dyer, John Bodel, Jonathan Prag, Ion Androutsopoulos, and Nando de Freitas. 2023. “Machine Learning for Ancient Languages: A Survey.” Computational Linguistics 49 (3): 703–47. https://doi.org/10.1162/coli_a_00481.",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/455/index.html",
    "href": "submissions/455/index.html",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "",
    "text": "This paper presents the worldwide database of Rockefeller Foundation fellows produced as part of an SNSF project co-directed by Thomas David (UNIL), Yi-Tang Lin (UZH), Davide Rodogno (IHEID), Pierre-Yves Saunier (CNRS) and Ludovic Tournès (UNIGE) (2018-2023), with a group of PhD students (Ahmad Fahoum, Mathilde Sigalas, Hannah Tyler) and a computer scientist (Steven Piguet).\nIt is composed of four parts: first, we outline the objective of the project; second, we describe the material used to build our database of Rockefeller fellows; third, we describe the process of consolidation and standardization of data; and fourth, we provide some examples of the use of this database in teaching digital history at the university level.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#introduction",
    "href": "submissions/455/index.html#introduction",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "",
    "text": "This paper presents the worldwide database of Rockefeller Foundation fellows produced as part of an SNSF project co-directed by Thomas David (UNIL), Yi-Tang Lin (UZH), Davide Rodogno (IHEID), Pierre-Yves Saunier (CNRS) and Ludovic Tournès (UNIGE) (2018-2023), with a group of PhD students (Ahmad Fahoum, Mathilde Sigalas, Hannah Tyler) and a computer scientist (Steven Piguet).\nIt is composed of four parts: first, we outline the objective of the project; second, we describe the material used to build our database of Rockefeller fellows; third, we describe the process of consolidation and standardization of data; and fourth, we provide some examples of the use of this database in teaching digital history at the university level.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#objective-of-the-project",
    "href": "submissions/455/index.html#objective-of-the-project",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "Objective of the Project",
    "text": "Objective of the Project\nThe aim of the project was to analyze the worldwide scientific politics of Rockefeller philanthropy through the prism of individual fellowship programs. The historiography of American philanthropy is very extensive, but has most often focused on the substantial funding granted by foundations to institutions and research programs, leaving aside funding to individuals, which was far less spectacular. Yet support for individuals is at the heart of philanthropic philosophy, and in particular that of the Rockefeller Foundation, which consists in selecting specific people at specific times, in specific sectors of activity and in specific countries to achieve specific goals. Ultimately, the Rockefeller Foundation’s aim with these individual fellowship programs was to contribute to the construction of a global elite of experts and researchers involved in modernizing the world along American patterns, in line with the messianic project developed by American elites from the end of the 19th century onwards. Foundations have been among the main driving forces behind this project, and funding science one of the major means used to achieve it. This is the background to the Rockefeller Foundation’s fellowship policy developed from the outset. As early as 1913, the Foundation awarded individual fellowships in all its fields of activity (medicine, public health, natural sciences, social sciences, nursing and agriculture). Between 1914 and 1968, 14,650 individual grants were awarded to 13,633 individuals (some of whom received several grants).",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#material-used",
    "href": "submissions/455/index.html#material-used",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "Material Used",
    "text": "Material Used\nThe Foundation has continued to award individual scholarships to the present day. However, we have limited ourselves to the period 1914-1968. The two basic materials used to build this base are: the Directory of fellowships and scholarships published by the Rockefeller Foundation in 1972; and above all, the “fellowship cards”, i.e. cardboard sheets containing extensive and relatively standardized information on fellows. This standardization is uneven and required a number of adjustments, but it enabled us to have a coherent corpus from the outset. Subsequently, the information gathered from the fellowship cards was cross-referenced, clarified or supplemented with the help of numerous documents from the Rockefeller archives.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#building-a-worldwide-database-from-national-material-ludovic-tournès",
    "href": "submissions/455/index.html#building-a-worldwide-database-from-national-material-ludovic-tournès",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "Building a worldwide database from national material (Ludovic Tournès)",
    "text": "Building a worldwide database from national material (Ludovic Tournès)\nOur database is a prosopographical database like many others. Its main originality is that it is transnational, since it contains fellows from or going to 134 different countries or territories. The construction of this transnational database from exclusively American archival material necessitated the resolution of a number of problems, which we would like to outline in this presentation, based on the data standardization and consolidation work we have carried out. The major challenge in building the database was to correct or break the American-centric bias of the Rockefeller archives without altering the nature of the information. We will discuss this process with three examples: consolidation of the names of countries and territories; consolidation of the names of the institutions from/to which fellows departed or arrived; and consolidation of the disciplines and disciplinary fields in which fellows practiced.\nConsolidating the names of countries and territories (134 in all) encountered several difficulties. The first was the historical evolution of the names of territories, whether colonized territories that became independent, or newly created or disappeared territories, or whose names change over time. The second difficulty arose from mistakes in the fellowship cards, which had to be corrected without altering the sources. We standardized the names of countries and locations (cities, regions, counties, etc.) by cross-referencing our data with those of reference databases such as Geonames, Wikidata and the Correlates of War Project.\nConsolidating the names of institutions (13,000 names collected in the fellowship cards) involved a complex process of cross-referencing between the fellowship cards and other sources, such as the institutions’ own websites, Wikipedia pages, articles from scientific journals published at the time of the fellowships, and the Virtual International Authority File (VIAF), which harmonized the names while avoiding overwriting the linguistic diversity of the institutions, since in most cases it enables us to find their names in the original language. Finally, the institutions were geocoded.\nThe consolidation of disciplines and fields of study was carried out in two stages. Firstly, we standardized the 8,000 different study programs mentioned in the cards, which were reduced to 2,395 “study program standardized” taking into account disciplinary titles and their evolution in the American intellectual field (since fellowship cards were filled in by Rockefeller Foundation officers). We then grouped these “standardized study programs” into 24 major themes, using UNESCO’s Nomenclature for Fields and Science technology.\nThe work carried out makes this database a tool for writing a history of scientific disciplines that goes beyond the national framework in which it has most often been written so far. One of our future objectives is to accentuate this transnational dimension by connecting our database with other prosopographical databases.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#teaching-a-database-yi-tang-lin",
    "href": "submissions/455/index.html#teaching-a-database-yi-tang-lin",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "Teaching a database (Yi-Tang Lin)",
    "text": "Teaching a database (Yi-Tang Lin)\nI used our database as teaching material for a Master’s seminar entitled “Knowledge Flows in Twentieth-Century East Asia,” where I introduced students to existing scholarly discussions on knowledge, science, technology, and techniques, and how these knowledge and materials traveled across East Asia, a region marked by constant political changes in the 20th century. The seminar format included reading historiographies on knowledge and information transfers, followed by three source workshops where students familiarized themselves, step by step, with the source materials of Rockefeller fellows and the creation of our database.\nSpecifically, after a week of introduction to the historiography of the Rockefeller Foundation’s scientific endeavors, the first workshop involved close reading of the source materials—the Rockefeller Foundation fellowship index cards. Students in groups were asked to make online annotations to answer the following questions: What do the sources tell you, and what kinds of research questions have emerged from your reading of the fellowship cards?\nThe fellowship index cards provided a unique opportunity for students with no previous training in quantitative history analysis or knowledge of East Asian regions. The first week of the workshop showed that the Rockefeller fellowship cards were material that students could easily relate to. The cards were essentially records of training and education, with some amazing details of the fellows’ struggles, mostly financial or linguistic, in a foreign environment. Groups of students spent much time discussing how difficult it could have been for these Rockefeller fellows, coming from East Asia, to adapt to U.S. universities or administrative organizations for their training.\nThe second week of the workshop prepared students to act as database designers. I asked them to use our online database to find the fellows they had read about in their index cards and reflect on the information captured by the database, what was categorized into different values, and whether they agreed with the designers’ decisions. Many students pointed out that the designers did not capture anything about the family situation, which was omnipresent in the record cards as fellows would receive extra funding or have their families transported to the U.S. to stay with them. Students also advised that, as database designers, we could have been more explicit about our decisions regarding variable design. Through discussions with me, one of the designers, students learned about database designers’ reflections and research questions behind a seemingly straightforward dataset with clean-cut information. They also understood the reasons behind why we were more detailed in some information and less so, or even ignored some, based on resources and manpower.\nFinally, the third session was for students to play with the database by presenting a visualization of their selection in front of the class. I asked students to be creative and present what they had found by playing with the data. Despite many interesting and innovative analytical frameworks based on how the students selected a sub-population (female fellows, Japanese fellows during WWII), this session was probably the least successful, as students tended to present many figures, resulting in limited analysis.\nTo briefly conclude, my design of the workshops progressed from close reading of individual source materials to using the database, which is essentially a collective biography of fellows. Instead of training students in technical analysis skills, I emphasized the importance of close and “far” readings of Rockefeller fellows. This seminar also aimed to show them how to read and analyze a database, considering it as not neutral but driven by designers’ research questions and available resources.\nStudents reacted well to the first two sessions. However, as we delved into the “far reading” aspect of the database, students were less capable of making meaningful arguments and discussions. One of the reasons is probably that our captured fields were very much institutional within the Rockefeller galaxy, which students were not completely familiar with.\nAt the end of this class, students did find inspiration from these source materials for their seminar research papers. Not with the individual fellows, but the contexts evidenced in the record cards. For example, as they read about a Japanese fellow’s journey in the USA during the conflict between the U.S. and Japan, some students became interested in researching Japanese science during and after WWII and its relations with the U.S., or Taiwanese agronomists and agricultural policies in Taiwan.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/455/index.html#conclusion-moving-forward",
    "href": "submissions/455/index.html#conclusion-moving-forward",
    "title": "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience",
    "section": "Conclusion: Moving Forward",
    "text": "Conclusion: Moving Forward\nToday, we presented a dataset of transnational nature. Our team has rediscovered and standardized the transnational elements within the Rockefeller Foundation source materials in what we believe are the most meaningful ways. We have also started using this database to teach students database literacy. To conclude this presentation, we invite your feedback and comments on how we can promote this dataset globally, making it meaningful and alive for people specializing in different regions.",
    "crumbs": [
      "Abstracts",
      "Rockefeller fellows as heralds of globalization: the circulation of elites, knowledge, and practices of modernization (1920–1970s): global history, database connection, and teaching experience"
    ]
  },
  {
    "objectID": "submissions/438/index.html",
    "href": "submissions/438/index.html",
    "title": "A handful of pixels of blood",
    "section": "",
    "text": "The 1980s marked the arrival of the home computer. Computing systems became affordable and were marketed to private consumers through state-supported programs and new economic opportunities (Haddon 1988; Williams 1976). Early models, such as the ZX Spectrum1, Texas Instrument TI-99/4A2, or the Atari3, quickly became popular in Europe and opened the door for digital technology to enter the home. This period also marks the advent of homebrew video game culture and newly emerging creative programming practices (Swalwell 2021; Alberts and Oldenziel 2014). As part of this process, these early programmers not only had to figure out how to develop video games but also were among the first to incorporate graphics into video games. This created fertile grounds for a new array of video game genres and helped popularize video games as a mainstream media.\nI’m researching graphics programming for video games from the 1980s and 1990s. The difference to other visual media lies in the amalgamation of computing and the expression of productive or creative intent by video game designers and developers. The specifics of video game graphics are deeply rooted in how human ideas must be translated into instructions that a computer understands. This necessitates a mediation between the computer’s pure logic and a playing person’s phenomenological experience. In other words, the video game image is a specific type of interface that needs to take care of a semiotic layer and offer functional affordances. I am interested in how early video game programmers worked with these interfaces, incorporating their own visual inspirations and attempting to work with the limited resources at hand. Besides critical source code analysis, I also extensively analyze formal aspects of video game images. For the latter, I depend on FAVR to properly describe and annotate images in datasets relevant to my inquiries. The framework explicitly deals with problems of analyzing video game graphics. It guides the annotation of images by their functional, material, and formal aspects and aids in analyzing narrativity and the rhetoric of aesthetic aspects (Arsenault, Côté, and Larochelle 2015).\nThe video game image also differs substantially from the image in animation or other software interfaces, to which they are often compared. Next to its interactivity, it also holds a double function of telling the game as well as offering the affordances to let the player participate. It is between animation and the user interface of the software, with added techno-visual dimensionalities such as resolution or frame rate. The concepts of the “ergodic animage” (Arsenault, Côté, and Larochelle 2015) and “algorithmic images” (Fizek 2022) aptly describe these video games-related aspects. The two terms imply that video game images are of a calculated nature and don’t represent reality but construct one and display the operations of software. Further, these images only work when players participate in them.\nClassical visual analysis is limited in its ability to deal with video game images due to their visual and material diversity, as well as a disciplinary vocabulary that is not a good fit for video game graphics (Arsenault, Côté, and Larochelle 2015). The same is true for the formal analysis of video game images through computer vision models, for example, towards object or image classifications. Neither general-purpose models nor such specialized user interfaces can deal with the visual diversity of video game images and interfaces. FAVR fills this gap by explicitly concentrating on what is displayed on the screen rather than what these images convey. For FAVR, the image on the screen becomes a specific type of interface at the intersection of the game’s rules and mechanics and their visual mediation.\nWhile the framework can identify different game modes and the different functionalities those screens can encompass, more intricate details can escape an analysis. FAVR distinguishes between tangible, intangible, and negative space, as well as agents, in-game and off-game elements, and interfaces. Whereas the aspect of space concerns the overall composition of the screen, the second set of attributes circumscribes the construction of the image. Intangible space, for example, is concerned with information relevant to gameplay but without the direct agency of the player. Examples are life bars or a display of the current score. As another example, off-game denotes decorative background elements. Being of a time-based and interactive nature, some of the relevant information only unfolds as animation or through player interaction. Further, not all visual mediations of the games’ operation are represented as expected in software interfaces or classic visual compositions.\nA simple example could be blood spurting from an agent, which can be any gameplay-relevant character on screen. The blood holds information relevant to the player, indicating that the character on screen got hurt and may prompt a change in play behavior. Whereas a life bar can represent the player’s character health, such indications are usually absent for enemies. Some video games also play with the distinction between in- and off-game planes. In Final Fight (Capcom, 1989, Arcade), our character walks from left to right in a raging city and, on the way, fights numerous enemies entering the screen from left and right. The off-game plane, the background, is composed of run-down houses and alleyways. At one point during the game, those houses’ doors start to open and spawn enemies as well. This mixes up the formerly established convention of what visual information is relevant for gameplay in terms of interactive and decorative elements.\nAnother relevant point regarding FAVR is its limitation to qualitative analysis and manual application. Since I am interested in a larger historical trajectory of video game images in the 1980s and 1990s, I need to leverage digital tools and computational methods to aid my research. I work with two image corpora in my research. A smaller corpus contains 1525 screenshots from 35 video games from Switzerland from 1987-1998. Acquiring a sufficient number of screenshots from old video games from Switzerland is difficult due to their low popularity. The corpus consists of video stills from Let’s Plays4 and screenshots from various video game databases. The second and larger corpus consists of 115’848 screenshots from 4316 video games and is solely sourced through the Mobygames database. Mobygames is one of the largest community-driven platforms for the collection of knowledge on video games. Being maintained mainly by amateurs and video game enthusiasts is not without problems (Pfister et al. 2023). There are open questions on accessing data, searchability, and, most importantly, completeness. Working with Mobygames makes it difficult to assess what will be missing from the dataset. Despite these shortcomings, the work of the community behind such database platforms is of immense value to video game research.\nTo leverage the potential of these corpora, I need to be able to apply FAVR in a formalized and digital method. To that end, I created a linked open ontology that derives and expands on FAVR (Demleitner 2023). It is based on CIDOC and can be applied in Tropy or similar image-annotation tools by providing templates. Other video game-related ontologies were not suitable for the tasks at hand. Most of the better-developed ontologies, such as Video Game Ontology (VGO), Digital Game Ontology (DGO), and Game Metadata and Citation Project (GAMECIP) are concentrating on describing the contents of a game and are mostly abandoned (Martino et al. 2023). Interestingly, both the VGO and VideOWL try to be of benefit to the industry and game developers. In turn, I’m mainly interested in the historic contextualization of video games and the practices of video game development.\nSo far, I was able to formalize the framework’s aspect on game modes and space, as well as create annotation templates that are building on those aspects. These were made for Tropy, a software that enables researchers to organize their visual material, properly describe the images with metadata, and make create annotations. The templates so far allow for the annotation of video game images regarding their overall composition of the screen, spaces as well as game modes. The screenshots provided above demonstrate the annotation of an in-game screenshot of Traps ‘n’ Treasures (Starbyte, 1993, Amiga). Such an annotation allows the comparison with other games, for example regarding the ratio of dynamic versus static image space. Such a ratio was an important factor in video game development, as dynamic image space needed more resources. The annotation can then be exported exportable as JSON5 and used in further analysis and digital methods.\nTo be able to analyze large quantities of video game images towards their functionality as interfaces, digital methods need to be leveraged. Computer vision (CV) models are of limited help regarding this inquiry. CV models are generally trained to extract semantic value, focusing primarily on object classification (Kurfess 2003) or segmentation tasks (Xu 2024). However, what is considered of semantic value typically does not include user interface elements, particularly in the specific context of video game images with their dual functionality. Image similarity cluster visualizations based on embeddings calculated using both classic convolutional neural network6 models like ResNet101 (He et al. 2015) and newer transformer7 models such as DINOv2 (Oquab et al. 2024) have shown [demleitnerThgieComingofageofthevideogameimageInitial2024] that these models are quite capable of recognizing what equals to modes in FAVR, although they lack the ability to properly annotate the images on that level. This limitation is likely due to the visual diversity present in video game images, where narrative elements and the visual mediation of game mechanics coexist on a wide spectrum. Visual Material annotated in Tropy with the FAVR ontology could potentially be used to train or fine-tune new models that are more adept at recognition in this domain.\nThe Framework for the Analysis of Visual Representation in Video Games is a welcomed vantage point for my research inquiry. After translating the framework into a linked open ontology, further work is needed to refine and expand it to encompass more subtle aspects of video game interfaces. Whereas the ontology developed so far works on a formal level, I have yet to research to what extent FAVR can be leveraged to applications of distant viewing of larger video game image corpora. Despite being implemented only in a limited form so far, the FAVR has proven to be a valuable tool in analyzing video game images towards their formal, discursive, and historical aspects.",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/438/index.html#references",
    "href": "submissions/438/index.html#references",
    "title": "A handful of pixels of blood",
    "section": "References",
    "text": "References\n\n\nAlberts, Gerard, and Ruth Oldenziel, eds. 2014. Hacking Europe: From Computer Cultures to Demoscenes. History of Computing. London: Springer. https://doi.org/10.1007/978-1-4471-5493-8.\n\n\nArsenault, Dominic, Pierre-Marc Côté, and Audrey Larochelle. 2015. “The Game FAVR: A Framework for the Analysis of Visual Representation in Video Games.”\n\n\nDemleitner, Adrian. 2023. “Thgie/Favr-Ontology: Alpha Release.” Zenodo. https://doi.org/10.5281/zenodo.10142313.\n\n\nFizek, Sonia. 2022. “Through the Ludic Glass: Making Sense of Video Games as Algorithmic Spectacles.” Game Studies 22 (2). http://gamestudies.org/2202/articles/gap_fizek.\n\n\nGerling, Winfried, Sebastian Möring, and Marco Mutiis. 2023. Screen Images. In-Game Photography, Screenshot, Screencast. https://doi.org/10.55309/c3ie61k5.\n\n\nHaddon, Leslie. 1988. “The Home Computer: The Making of a Consumer Electronic.” Science as Culture 1 (2): 7–51. https://doi.org/10.1080/09505438809526198.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” December 10, 2015. https://doi.org/10.48550/arXiv.1512.03385.\n\n\nKurfess, Franz J. 2003. “Artificial Intelligence.” In Encyclopedia of Physical Science and Technology (Third Edition), edited by Robert A. Meyers, 609–29. New York: Academic Press. https://doi.org/10.1016/B0-12-227410-5/00027-2.\n\n\nMartino, Simone De, Marianna Nicolosi-Asmundo, Stefano Angelo Rizzo, and Daniele Francesco Santamaria. 2023. “Modeling the Video Game Environment: The VideOWL Ontology.”\n\n\nOquab, Maxime, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, et al. 2024. “DINOv2: Learning Robust Visual Features Without Supervision.” February 2, 2024. https://doi.org/10.48550/arXiv.2304.07193.\n\n\nPfister, Eugen, Aurelia Brandenburg, Adrian Demleitner, and Lukas Daniel Klausner. 2023. “Warum wir es für eine gute Idee gehalten haben, eine DACH-Spieledatenbank aufzubauen.” In Game-Journalismus: Grundlagen – Themen – Spannungsfelder. Ein Handbuch, edited by Benjamin Bigl and Sebastian Stoppe, 307–16. Wiesbaden: Springer Fachmedien. https://doi.org/10.1007/978-3-658-42616-3_22.\n\n\nSwalwell, Melanie. 2021. Homebrew Gaming and the Beginnings of Vernacular Digitality. Cambridge: MIT. https://mitpress.mit.edu/9780262044776/homebrew-gaming-and-the-beginnings-of-vernacular-digitality/.\n\n\nWilliams, Richard. 1976. “Early Computers in Europe.” In Proceedings of the June 7-10, 1976, National Computer Conference and Exposition, 21–29. AFIPS ’76. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1499799.1499804.\n\n\nXu, Yuhang. 2024. “Application of Image Segmentation Algorithms in Computer Vision.” Frontiers in Computing and Intelligent Systems 7 (April): 17–20. https://doi.org/10.54097/gq1s6737.",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/438/index.html#media-list",
    "href": "submissions/438/index.html#media-list",
    "title": "A handful of pixels of blood",
    "section": "Media List",
    "text": "Media List\n\nFig. 1-2: Screenshots from Barbarian (1987) - MobyGames, accessed July 09, 2024\nFig. 3-4: Screenshots of Final Fight (Arcade) Playthrough - NintendoComplete - YouTube, accessed July 09, 2024\nFig. 5-6: Screenshots provided by the author\nBarbarian (Palace Software Inc, 1987, Amiga, DOS)\nFinal Fight (Capcom, 1987, Arcade)\nTraps ‘n’ Treasures (Starbyte, 1993, Amiga)",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/438/index.html#footnotes",
    "href": "submissions/438/index.html#footnotes",
    "title": "A handful of pixels of blood",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZX Spectrum, accessed May 13, 2024↩︎\nTI-99/4A, accessed May 13, 2024↩︎\nAtari 8-bit computers - Wikipedia, accessed July 12, 2024↩︎\nLet’s Play - Wikipedia, accessed July 09, 2024↩︎\nfavr-ontology/examples/ball-raider-1987-main-gameplay.json, accessed July 09, 2024↩︎\nConvolutional neural network - Wikipedia, accessed July 19, 2024↩︎\nTransformer (deep learning architecture) - Wikipedia, accessed July 19, 2024↩︎",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/453/index.html",
    "href": "submissions/453/index.html",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "",
    "text": "Over the past few decades, we have witnessed a major transformation in the digital resources and methodologies available, particularly in the field of Artificial Intelligence (AI), with significant implications for society and the economy. As it is stated in the White Paper The Digital Turn in the Sciences and Humanities by the German Research Foundation’s (DFG), the digital turn is bringing about three major changes in research: former analogue research practices are being realised with digital tools (transformative change); data-intensive technologies allow new research questions to be addressed (enabling change); digital technologies, especially AI methods, can even replace humans in parts of the research project (substitutive change).\nThis phenomenon can also be observed in the human and social sciences (HSS), and even in history, and is particularly striking in the area of open data publication. On the one hand, data can be deposited in well-known, dedicated repositories, such as Zenodo, Nakala, DaSCH or DANS, and a growing number of data journals (e.g. the Journal of Open Humanities Data) publish papers dedicated to contextualising data production in order to facilitate its reuse. On the other hand, directly accessible data are available in the form of relational databases that can be queried (e.g. the PRELIB project) or, using the RDF framework, in the form of Linked Open Data (e.g. the Sphaera project or the Geovistory collaborative platform). We can thus observe that the digital transformation of research practices in HSS (transformative change) is leading to the production and publication of an exponentially growing wealth of information, making it possible to address new research questions (enabling change), in particular by applying AI methodologies in the context of new disciplines known under the label of computational humanities (substitutive change).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#introduction",
    "href": "submissions/453/index.html#introduction",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "",
    "text": "Over the past few decades, we have witnessed a major transformation in the digital resources and methodologies available, particularly in the field of Artificial Intelligence (AI), with significant implications for society and the economy. As it is stated in the White Paper The Digital Turn in the Sciences and Humanities by the German Research Foundation’s (DFG), the digital turn is bringing about three major changes in research: former analogue research practices are being realised with digital tools (transformative change); data-intensive technologies allow new research questions to be addressed (enabling change); digital technologies, especially AI methods, can even replace humans in parts of the research project (substitutive change).\nThis phenomenon can also be observed in the human and social sciences (HSS), and even in history, and is particularly striking in the area of open data publication. On the one hand, data can be deposited in well-known, dedicated repositories, such as Zenodo, Nakala, DaSCH or DANS, and a growing number of data journals (e.g. the Journal of Open Humanities Data) publish papers dedicated to contextualising data production in order to facilitate its reuse. On the other hand, directly accessible data are available in the form of relational databases that can be queried (e.g. the PRELIB project) or, using the RDF framework, in the form of Linked Open Data (e.g. the Sphaera project or the Geovistory collaborative platform). We can thus observe that the digital transformation of research practices in HSS (transformative change) is leading to the production and publication of an exponentially growing wealth of information, making it possible to address new research questions (enabling change), in particular by applying AI methodologies in the context of new disciplines known under the label of computational humanities (substitutive change).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#a-paradigm-shift",
    "href": "submissions/453/index.html#a-paradigm-shift",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "A paradigm shift",
    "text": "A paradigm shift\nThis important transformation of historical research raises the question of a paradigm shift. This concept was used by Thomas Kuhn in 1962 in his book The Structure of Scientific Revolutions (Kuhn 1962) to describe the intellectual structure of disciplines and to analyse the ruptures that lead to scientific revolutions. There are two essential elements to be considered: on the one hand, the paradigm consists of all the shared methods, practices and achievements that form the basis and structure of a disciplinary community; on the other hand, it includes, in its ancient, original sense, the teaching practices applied during education with the aim of enabling the acquisition of the skills essential to the practice of a discipline. Since the purpose of scientific activity is the production of knowledge, the paradigm enables students to learn the methods and rules that are legitimate within a disciplinary community. The digital turn thus raises the question of the transformation of methods and forms of knowledge production in the historical sciences, as can be seen from the publications of a growing number of scholars (e.g. the Journal of Digital History).\nOn the basis of this analysis, it seems essential to introduce training in digital methodologies and tools into the standard disciplinary curriculum of history, and not just in optional Digital Humanities Minors. Since learning disciplinary tools is at the heart of the paradigm of a discipline, digital methodologies should be taught from the beginning of university studies, so that future generations of teachers, doctoral students, professors and researchers can make the transition to the new paradigm from within. This will enable to create a disciplinary community trained in the new methodologies, familiar with the issues from direct experience, and capable of defending the place of the historical sciences in the field of contemporary science and the digital society (Beretta 2023).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#masters-course-in-digital-methodology-for-historical-research",
    "href": "submissions/453/index.html#masters-course-in-digital-methodology-for-historical-research",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "Master’s course in digital methodology for historical research",
    "text": "Master’s course in digital methodology for historical research\nThese considerations stem not only from my work as a CNRS researcher who has spent the last fifteen years building collaborative information systems for research (symogih.org, ontome.net, geovistory.org)(Beretta 2024), in line with the vision that, as the DFG White Paper points out, “digital infrastructure is essential for research and must be built for long-term service”, but also from ten years of experience in teaching digital methodology at bachelor and master level in history, first at the University of Lyon 3 and for the last four years at the University of Neuchâtel, which currently offers courses in digital methodology in the master’s programmes in Historical Sciences and in Regional Heritage and Digital Humanities.\nBut at this point an essential question arises: what should be taught to history students to help them make the most of the digital transition and build a new paradigm? Looking at recent handbooks, e.g. (Antenhofer, Kühberger, and Strohmeyer 2023; Döring et al. 2022; Schuster and Dunn 2021), or at educational resources like the programminghistorian.org project, we can see a huge variety of approaches and areas of application of digital methods, and often the answer to the question depends on the own field of research and experience. In this sense, I will not provide a somewhat abstract review of the literature, and existing courses, but rather share some aspects of my own approach in the hope that they may be of some use or inspiration to others.\nMy teaching at Master’s level consists of a three-part programme: the first semester deals with understanding the research cycle in history, setting up an information system and discovering the semantic web; the second focuses on learning data analysis and visualisation methods using Python notebooks; the third is about applying the methods to the students’ own research agenda. This teaching programme has two objectives, which correspond to the first two components of the digital transformation mentioned in the DFG White Paper: to learn a methodology suitable for the manual collection of information from sources, according to the best practices of computer science (transformative change); to learn a pool of data analysis and visualisation methodologies, allowing the exploitation of the growing number of existing resources (enabling change). These courses therefore provide students with basic skills, particularly in data analysis, which they can apply directly to their Master’s thesis and, if they wish, continue on to computational research courses such as Machine Learning or Natural Language Processing (substitutive change).\nSince the aim of research is to produce knowledge, an analysis of the research process, conceptualised in terms of a research cycle, forms the basis of my courses. This choice underlines the iterative dimension that is specific to the scientific approach in general and also applies to the formulation and verification (or falsification) of hypotheses that is specific to the social sciences.\n\n\n\n\n\n\nCycle of knowledge production in historical disciplines\n\n\n\n\nFigure 1\n\n\n\nIn this context, knowledge is understood as the result of the analysis and interpretation of information. With regard to information it is at the heart of the scientific process and can be defined as a representation of reality (which is the only datum is the world we observe), and more precisely as an identification and representation of the objects in the world (people, organisations, artefacts, etc.), their characteristics (physical properties of objects, education and income levels of people, opinions, etc.) and their relationships in time and space (membership in organisations, exchange of messages or goods, journeys, etc.). Knowledge can thus be defined as an interpretation of the world represented in the information collected, and if the former is the result of the scientific activity and is generally published in the form of books or articles, the latter should be understood as a most accurate approximation of the facts in words, making the information reusable for new research when shared in the form of digital open data according to the FAIR principles.\nAs the diagram of the knowledge production cycle shows, all research must begin with the definition of a research agenda that fits within the horizon of existing knowledge, expressed in literature, and that defines the methodology that will be adopted and the research questions to be answered. Zotero seems to be the best tool for this task, not only for storing bibliographical references, but also for enriching them with your own notes and categories, and for connecting them to resources on the web, thus realising the first step of a digitally transformed research. On the basis of their line of inquiry, student must then select from the available mass of sources the relevant ones in order to gather the information that will be analysed and serve as a basis for knowledge. They will have to decide what information will be systematically retained and how it will be conceptualised and produced. This raises the issue of the conceptual model and the choice of digital storage technology, because while spreadsheets may be adequate if one is limited to systematically collecting a certain number of characteristics of a population of individuals of the same type, as soon as one wishes to inform about complex relationships between different objects (persons, organisations, artefacts, opinions, economic values, etc.) in space and time, it is essential to use a relational or graph-oriented database in order to capture the full wealth of the required information.\nThis is precisely the content of the teaching of the first semester and I propose to the students to follow the example of the teacher’s own GitHub repository in order to document, in a dedicated GitHub repository and wiki the progress of their research cycle. In other words, I’m adopting a kind of teaching by example, where the whole approach is documented in a sample project available on GitHub that can be imitated and applied to one’s own subject, while endeavouring to go through all the proposed steps by creating one’s own SQLite database, one’s own analyses in Python, etc.\nTo propose the simplest and most concrete use case, I adopt a proposopographical approach and invite students to search Wikipedia for the biographical records of a population that corresponds to their interests, for example political activists or fashion designers, while asking themselves some questions to which they would like to find answers. We then consider the Wikipedia biographical records for this population as sources and define a catalogue of information to be extracted that will lead to the creation of a conceptual model and an initial SQLite database. Students will thus acquire the basic elements for creating a simple, easy-to-manage information system, which will greatly facilitate the manual input of relatively complex information from the sources analysed (transformative change).\nSince it does not make sense to produce a lot of information manually in the context of this course, at this stage I take advantage of the DBPedia and Wikidata projects, which provide a wealth of information on the previously selected populations in the form of structured data published in RDF. Students will therefore learn how to retrieve this information using the SPARQL language and import it into their SQLite database for refinement, thus discovering the process of re-using existing data, which can be considerable in volume with thousands of individuals described and dozens of pieces of information about them (enabling change).\nThis step marks the transition to the second semester, which begins by learning basic skills in Python and using Jupyter notebooks. To be able to analyse the information collected, it must be simplified and coded. It is at this stage that the research questions are introduced and a range of tools are applied to the information collected in the form of digital data: univariate and multivariate statistical analysis, network analysis, spatial representation, etc. Students will discover a new notion of model, now in the statistical sense, that emerges from these analyses and has an eminently heuristic function, since the representations produced by analysis software always require critical discussion, contextualisation and interpretation. At the same time, these methods and digital tools make visible significant phenomena that would otherwise be impossible to see “with the naked eye”, given the considerable volume and complexity of the information collected on the Semantic Web.\nAt the end of the process, students formulate some possible answers to their research questions and document the results obtained in their repository wiki, accompanied by graphics resulting from the analysis. They thus complete the research cycle by producing new knowledge in response to their initial research agenda, publishing online not only the results of their investigations, but also the database, the Python notebooks and the discussion of the analyses that led to their conclusions, thus learning in practice to undertake a reproducible scientific approach. The third semester is devoted to accompanying students who wish to realise their Master’s thesis using the methods learned in the previous semesters. This is still an ongoing process in Neuchâtel, so in my paper I’ll present some results from the master’s theses written by students at Lyon 3 university.",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#results-and-discussion",
    "href": "submissions/453/index.html#results-and-discussion",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "Results and discussion",
    "text": "Results and discussion\nI observed in all these years that if the students invest some time in practising the exercises and follow the learning cycle in this kind of apprenticeship by example during the two semesters, they can achieve amazing results (e.g. Militant.e.s pour le droits des femmes and Fashion Designers). But at the same time I have to admit that the learning curve is steep, because in just one year students learn the basics of conceptual modelling, SQL, SPARQL, Python and the essential concepts of various data analysis methods. As well as versioning with GIT and putting data and notebooks online. On the one hand, a certain pedagogical investment is necessary, especially to support students who have less of a natural inclination towards digital technology. On the other hand, the more technical part of this method should be introduced at bachelor level, like GitHub versioning and Python. At the University of Neuchâtel, a brand new minor in Digital Humanities has been introduced in the bachelor’s programme, which will enable students who have taken it to benefit more from the master’s courses.\nAs far as the Master’s thesis is concerned, it seems that the conceptual modelling and the setting up of a database for the input of information extracted from sources are the most useful, while the venture into collecting data available on the web as a basis for the Master’s thesis does not yet seem attractive. However, there are exceptions, as shown by a work using the Refuge Huguenot database, which I will present in my paper. In conclusion, it seems that at the moment students that take this course can only reach the level of transformative change. But experience shows that it is only with the development of appropriate research infrastructure and the emergence of a wider community of digital disciplinary practices that we will be able to provide students with a context that will allow them to achieve the enabling and substitutive changes, and thus bring about an effective paradigm shift. It is up to the new generations to make this happen.",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#references",
    "href": "submissions/453/index.html#references",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "References",
    "text": "References\n\n\nAntenhofer, Christina, Christoph Kühberger, and Arno Strohmeyer. 2023. Digital Humanities in Den Geschichtswissenschaften. Erste Ausgabe. Stuttgart: utb GmbH. https://doi.org/10.36198/9783838561165.\n\n\nBeretta, Francesco. 2023. “Données Ouvertes Liées Et Recherche Historique : Un Changement de Paradigme.” Humanités Numériques, no. 7 (July). https://doi.org/10.4000/revuehn.3349.\n\n\n———. 2024. “Données Liées Ouvertes Et Référentiels Public : Un Changement de Paradigme Pour La Recherche En Sciences Humaines Et Sociales.” Arabesques, no. 112: 26–27. https://doi.org/10.35562/arabesques.3820.\n\n\nDöring, Karoline Dominika, Stefan Haas, Mareike König, and Jörg Wettlaufer. 2022. Digital History: Konzepte, Methoden Und Kritiken Digitaler Geschichtswissenschaft. 1st ed. Vol. 6. Studies in Digital History and Hermeneutics. Germany: De Gruyter.\n\n\nKuhn, Thomas S. 1962. The Structure of Scientific Revolutions. Chicago: University of Chicago Press.\n\n\nSchuster, Kristen, and Stuart E. Dunn. 2021. Routledge International Handbook of Research Methods in Digital Humanities. 1st ed. Routledge International Handbooks. London ; Routledge.",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/462/index.html",
    "href": "submissions/462/index.html",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "",
    "text": "The role of real estate in a premodern urban economy is generally underrated due to a lack of specific research and the disparity of sources. While aspects such as the commodification of real estate and its role as collateral in credit contracts have been studied, combining different aspects from various sources into one perspective remains challenging. The Historical Land Register of Basel, initiated in 1895 and developed over several decades1, offers a potential solution. It involved thorough research in Basel’s city archives, creating approximately 120,000 file cards with almost verbatim transcriptions of sources, organized by house and chronologically sequenced. This comprehensive register combines information from various corpora, providing an unparalleled wealth of data for the era. To extract relevant information from these handwritten cards, we use machine learning methods such as named entity recognition (NER) and event extraction on texts generated by handwritten text recognition (HTR) of the scanned cards. As this is work in progress, we focus on specific aspects of real estate’s role in Basel’s economy, particularly interests. Medieval Basel had a dense network of religious institutions with seigneurial rights on many houses, additionally acting as lenders in annuities. Post-Reformation, these institutions’ records were well-preserved in the city archives, dominating the Historical Land Register. We contrast this with descriptions of houses in civil court records, which often list beneficiaries or note the absence of interests. We will then explore the frequency of interest mentions for houses in connection with seizure procedures in the civil court, typically due to non-payment of interests. When combining different sources, we must carefully select and interpret results, considering possible biases like gaps in tradition, changing writing habits, and the specific methods used by the Land Register’s creators. Additionally, machine learning errors can distort findings. By acknowledging these biases, we can draw more accurate conclusions about historical developments and changes.\nAfter a short introduction on methodology, the following research questions will be addressed in this extended abstract:\n\nWhat can we discover about the interest burden on real estate?\nDid a higher burden of interest lead to an increased number of seizure procedures?\nWho made use of seizure procedures and how does this use relate to interest claims?",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#introduction",
    "href": "submissions/462/index.html#introduction",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "",
    "text": "The role of real estate in a premodern urban economy is generally underrated due to a lack of specific research and the disparity of sources. While aspects such as the commodification of real estate and its role as collateral in credit contracts have been studied, combining different aspects from various sources into one perspective remains challenging. The Historical Land Register of Basel, initiated in 1895 and developed over several decades1, offers a potential solution. It involved thorough research in Basel’s city archives, creating approximately 120,000 file cards with almost verbatim transcriptions of sources, organized by house and chronologically sequenced. This comprehensive register combines information from various corpora, providing an unparalleled wealth of data for the era. To extract relevant information from these handwritten cards, we use machine learning methods such as named entity recognition (NER) and event extraction on texts generated by handwritten text recognition (HTR) of the scanned cards. As this is work in progress, we focus on specific aspects of real estate’s role in Basel’s economy, particularly interests. Medieval Basel had a dense network of religious institutions with seigneurial rights on many houses, additionally acting as lenders in annuities. Post-Reformation, these institutions’ records were well-preserved in the city archives, dominating the Historical Land Register. We contrast this with descriptions of houses in civil court records, which often list beneficiaries or note the absence of interests. We will then explore the frequency of interest mentions for houses in connection with seizure procedures in the civil court, typically due to non-payment of interests. When combining different sources, we must carefully select and interpret results, considering possible biases like gaps in tradition, changing writing habits, and the specific methods used by the Land Register’s creators. Additionally, machine learning errors can distort findings. By acknowledging these biases, we can draw more accurate conclusions about historical developments and changes.\nAfter a short introduction on methodology, the following research questions will be addressed in this extended abstract:\n\nWhat can we discover about the interest burden on real estate?\nDid a higher burden of interest lead to an increased number of seizure procedures?\nWho made use of seizure procedures and how does this use relate to interest claims?",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#information-extraction",
    "href": "submissions/462/index.html#information-extraction",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Information Extraction",
    "text": "Information Extraction\nTo facilitate the automated extraction of information, our team manually annotated 640 documents according to the BeNASch annotations guidelines.2 BeNASch is a nested annotation system which represents information about entities, relations and events. Annotation work was done using the INCEpTION platform (Klie et al. 2018).\n\nEntity Annotation\nEntities are classified into the categories PER (person), LOC (location), ORG (organization) or GPE (geo-political entity) and consist of an outer span and the head-element which marks the core. Entities are annotated in a nested manner, meaning another entity may be marked inside an entity. Entities may also contain Attributes and Descriptors, further describing them. The annotation of quantitative values, such as dates and money, is also part of the entity annotation process.\n\n\n\n\n\n\nFigure 1: Example annotation of a strongly simplified house description.\n\n\n\n\n\nEvent Annotation\nAn event is usually represented by a trigger phrase, one or multiple actors and objects (roles) and sometimes modifiers which add additional information (e.g. the information why an interest must be paid). For example, a document describing a seizure procedure will usually have a trigger phrase like “gefröhnt” and the roles of a claimant, the seized object and the reason for the seizure, an event describing the missed interests. Annotation may then look like the following example:\n&lt;claimant&gt; The administrator of the Klingenthal monastery &lt;/claimant&gt; has &lt;trigger&gt; seized &lt;/seized&gt; &lt;seized_obj&gt; Hans Müllers house at the Viehmarkt &lt;/seized_obj&gt; because of &lt;reason&gt; missed dues, as aforementioned monastery gets 3 fl. yearly interest from it &lt;/reason&gt;.\nEvents are not only linked to the entity annotations by their roles, but descriptors and entity mentions can also imply an event. For example, a descriptor with class dues also implies a dues/interest event.\n\n\nAutomated Extraction\nWe model both, entity and event extraction, in a two-step sequence tagging tasks. We utilize the FlairNLP framework (Akbik, Blythe, and Vollgraf 2018) to train our models, splitting our annotated data in an 80/10/10 split into training, validation and test data. For both steps we finetuned the german-language contextual character embedding model provided by the Flair framework (Akbik et al. 2019) using all available text from the Historical Land Records.\nIn the first step, we use a model to annotate all information that can be represented as spans of text which includes entity mentions, descriptors and values, but also event trigger phrases and modifiers. We use the process described in (Prada Ziegler 2024) to facilitate nested sequence tagging, but extend this method by adding a prefix and suffix to each sample to inform which annotation level we’re currently on. In the second step, we apply the role detection. For this task, we generate one sample for each trigger in our training set. The sample only includes the text of the immediate annotation level where the trigger is nested within. E.g. in the seizure example above, this would be the full document, as the trigger is found on document level, but the dues event (in the DESC.DUES) in the entity annotation example would only include the text of the descriptor. We again include a prefix and suffix to mark the annotation level and pretag the text by inserting the recognized spans from step 1 by putting a prefix and suffix around them. To ensure the system focuses the correct trigger during training and inference, we set the prefix and suffix annotation for all other triggers in the same sample to “INACTIVE”. This enables our system to learn which entities have roles corresponding to which trigger.3\nAs our annotated dataset is very small, the extraction performance for many classes cannot be properly evaluated. For the classes relevant to this study, we did additional evaluation based on the data outside our ground truth, scores will be reported whenever relevant in the following chapters.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#analysis",
    "href": "submissions/462/index.html#analysis",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Analysis",
    "text": "Analysis\n\nMeasuring interest burden on real estate\nThe obvious indicator to see what amount of interest a property is burdened with are description of interests. We may find these in different places: In the form of lists kept by different institutions which received interest, as part of the description of properties when they are mentioned in the documents, most often when a property is sold, and whenever an annuity is established. The lists of interest payments are problematic to use as they take the perspective of the beneficiary of the interest and do not mention if that property is also burdened with interest by other institutions or persons. The documents tracking lending of annuities would work, but their recognition by our automated system is still lacking. The descriptions of properties are thus a good choice, they are well detected by our automated annotation and contain information about all beneficiaries.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file\nfile_path = './data/3_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path)\n\n# Plot the 'Description of Dues' line\nplt.plot(data['decade'], data['interest descriptions as part of house descriptions'], label='interest descriptions as part of house descriptions')\n\n# Plot the 'Due-Events at Document Level' line\nplt.plot(data['decade'], data['document level interest events'], label='document level interest events')\n\n# Add labels and title\nplt.xlabel('Decade')\nplt.ylabel('Count')\nplt.legend()\n\n# Add grid for better readability\nplt.grid(True)\n\n# Save the plot as an image file\nplt.savefig(\"./images/fig_3.png\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Comparison between frequency of descriptions of interest as part of house descriptions and other contexts, such as institutional records recording received interest. Note that even though received interests remain steady, interest descriptions in house descriptions vanish with time. Results are bucketed by decade.\n\n\n\n\n\nWe consider three ways to measure the burden of interest. First, we could use the absolute number of descriptions, but this number is unreliable, as there is a trend to not describe the interest anymore in later documents (see Figure 2). Second, we could try to use the monetary values mentioned in the descriptions. Here we encounter multiple problems: Numbers suffer from more HTR errors than other parts of the documents and are thus less reliable in general. Additionally a number of different currencies are in use, which would need conversion to a single value, as well as payments in kind. Finally our automated system cannot differentiate between different reasons for interest at the moment, so we wouldn’t know if a value is paid per year or only in case of an exchange of property ownership (“zu erschatz”). We settled using the number of beneficiaries to determine the burden of rent. Any entity found in a house description is classified as a beneficiary (we evaluated this to be true in 98% of cases based on 100 samples, with the true false positives being caused by errors in the named entity recognition process). Figure 3 shows the absolute numbers of organizations and persons recognized as receivers of interest in house descriptions over time.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file\nfile_path = './data/3_4_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path)\n\n# Group by 'date' and 'entitytype' and count the number of occurrences\ngrouped_data = data.groupby(['date', 'entitytype']).size().unstack(fill_value=0)\n\n# Apply rolling window of 11 years (5 years before, the current year, and 5 years after)\nrolling_window = 11\ngrouped_data_rolling = grouped_data.rolling(window=rolling_window, min_periods=1, center=True).mean()\n\n# Plot the data\n#plt.figure(figsize=(10, 6))\n\n# Plot each entity type\nfor entity_type in ['per', 'org', 'keine']:\n    if entity_type in grouped_data_rolling.columns:\n        plt.plot(grouped_data_rolling.index, grouped_data_rolling[entity_type], label=entity_type if entity_type != \"keine\" else \"No entity\")\n\n# Add labels and title\nplt.xlabel('Year')\nplt.ylabel('Number of Entities')\nplt.legend(title='Entity Type')\n\n# Show the plot\nplt.grid(True)\nplt.savefig(\"./images/fig_4.png\")  # Save the plot as an image file\nplt.show()  # Display the plot\n\n\n\n\n\n\n\n\nFigure 3: Entities mentioned in descriptions of interests within house descriptions by class (5-year gliding window).\n\n\n\n\n\n\n\nHow seizures relate to burden of interest\nSince the 1420ies, seizure procedures obtained their own series in the city court records. These procedures could be applied in case of arrears in real-estate-related annuities or other interests, allowing the claimant to take the house into his possession after confirming his claim on three occasions. It meant no automatism of confiscation, but rather a gradually increased pressure on house owners to pay their due – actual transfer of ownership was much rarer than the high number of seizure procedures would suggest. Our model is very reliable when it comes to extracting seizure events. In a sample of 100 automatically identified seizures, none of them were false positives. We did not conduct a large enough evaluation to calculate how many seizures we miss currently, but from smaller checks, we’re confident this number is very low.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file while skipping the first row of headers\nfile_path = './data/4_5_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path, skiprows=1, header=None)\n\n# Assign meaningful column names\ndata.columns = ['Decade_nice', 'Decade', 'no_of_seizures', 'no_of_docs_in_10_years', 'no_of_docs_with_desc_tax', 'no_of_entities_org_per', 'entities_desc_tax', 'entities_per_document', 'unused1', 'documents_per_decade', 'documents_with_desc_tax_per_decade', 'entities_per_decade', 'entities_desc_tax_2']\n\n# Plot the 'entities_desc_tax' line (column G, index 6)\nplt.plot(data['Decade'], data[\"entities_desc_tax\"], label='avg. entities in descriptions before seizure docs')\n\n# Plot the 'entities_desc_tax_2' line (column M, index 12)\nplt.plot(data['Decade'], data[\"entities_desc_tax_2\"], label='avg. entities in descriptions generally')\n\n# Add labels and title\nplt.xlabel('Decade')\nplt.legend()\n\n# Add grid for better readability\nplt.grid(True)\n\n# Save the plot as an image file\nplt.savefig(\"./images/fig_5.png\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Proportion of entities mentioned per interest description, comparison between houses in seizure procedures and all houses.\n\n\n\n\n\nSince seizure procedures were linked to interests, one could expect that interest descriptions would be more frequent in the years before the event. In fact, when comparing the 10 years before a seizure with all documents of the same decade, the relative number of entities mentioned in interest descriptions shows no difference. Thus, owing interests in itself was no reason for increased numbers of seizures. When looking at the number of entities mentioned in each interest description, one sees that in certain periods of time, this proportion was much higher for documents leading to seizure events than in the average document (see Figure 4). This is the case mainly for the time period between 1480 and 1540.\nIn the next part, we contrast the entities in the interest descriptions to the entities taking part in seizures as claimants. To distinguish organizations from persons as parties in seizure procedures, we must look closer at the claimants. Court records generally mention the representative who actually appeared in court. As actor in the event, this person is identified as claimant by our event recognition model. Thanks to our nested entity recognition, we can identify if there is an organizational affiliation with an accuracy of 83.4%. Most of the errors are based on the non-recognition of formulas like “innamen von” (representing) that should be possible to include in future algorithms. Figure 5 shows that surprisingly and in contrast to the interest descriptions presented above, persons were much more present as claimants than organizations, even if we account for a considerable proportion of unrecognized organizational affiliations. Thus, either there were people acting as claimants for institutions that are not recognisable in the text as such, or organizations made in fact fewer use of the seizure procedure. One possible explanation could be that seigneurial interests were generally very low, but accounted for a lot of organizations mentioned in interest descriptions. These low interests might not have presented a problem for house owners – or their non-payment did not justify going to court for the institutions. In order to clarify such questions, further research would be necessary.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file while skipping the first row of headers\nfile_path = './data/7_viz.csv'  # Update this with the actual file path if necessary\ndf = pd.read_csv(file_path)\n\nplt.plot(df['Decade'], df['none'], label='No claimant', color='green')\nplt.plot(df['Decade'], df['PER'], label='per', color='blue')\nplt.plot(df['Decade'], df['ORG'], label='org', color='orange')\n\n# Formatting\nplt.xlabel('Decade')\nplt.ylabel('Count')\nplt.legend(title='Type')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Entity types of claimants.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#conclusions",
    "href": "submissions/462/index.html#conclusions",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Conclusions",
    "text": "Conclusions\nThe relationship between interest burden and seizures proves to be investigable for the first time with our data. However, it is a complex relationship. The probability of a seizure increases with rising interest burden only during certain periods when seizures were infrequent. The many institutional creditors are reflected in the interest descriptions but not in the number of seizures. Further investigations need to determine whether institutions were generally more patient, whether this was due to lower interest burden, or whether our models fail to adequately capture institutions. This requires unambiguous identification of institutions, which still needs to be carried out.\nOn a methodological level, it becomes clear that successful recognition of entities and events allows for a large number of research questions based on a combination of different aspects that are brought together in the Historical Land Register. However, the recognition of events is still a work in progress, due to the often small number of annotated events in the training material. It needs to be evaluated and developed based on specific samples. As this process progresses, the quality of the analyses will enable a long-term perspective on the economic activities involving Basel properties.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#references",
    "href": "submissions/462/index.html#references",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "References",
    "text": "References\n\n\nAkbik, Alan, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. “FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP.” In NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), 54–59.\n\n\nAkbik, Alan, Duncan Blythe, and Roland Vollgraf. 2018. “Contextual String Embeddings for Sequence Labeling.” In COLING 2018, 27th International Conference on Computational Linguistics, 1638–49.\n\n\nKlie, Jan-Christoph, Michael Bugert, Beto Boullosa, Richard Eckart de Castilho, and Iryna Gurevych. 2018. “The INCEpTION Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation.” In Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, edited by Dongyan Zhao, 5–9. Santa Fe, New Mexico: Association for Computational Linguistics. https://aclanthology.org/C18-2002.\n\n\nPrada Ziegler, Ismail. 2024. “What’s in an entity? Exploring Nested Named Entity Recognition in the Historical Land Register of Basel (1400-1700).” Zenodo. https://doi.org/10.5281/zenodo.11500543.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#role-detection-column-format-example",
    "href": "submissions/462/index.html#role-detection-column-format-example",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Role Detection Column Format Example",
    "text": "Role Detection Column Format Example\n\n\n\n\n\n\n\n\n\nToken\nEntity Annotation\nRole Annotation\n\n\n\n\n[DESC]\nO\nO\n\n\n[B-DUE]\nB-tag:tr;t:due\nB-tag:tr;t:due\n\n\nzinst\nI-tag:tr;t:due\nO\n\n\n[E-DUE]\nI-tag:tr;t:due\nO\n\n\n[B-MONEY]\nB-tag:val;val:money\nB-tag:role;r:interest\n\n\n1sh\nI-tag:val;val:money\nO\n\n\n.\nI-tag:val;val:money\nO\n\n\n[E-MONEY]\nI-tag:val;val:money\nO\n\n\n[B-ORG]\nB-tag:ref;ent:org;sm:\nB-tag:role;r:beneficiary\n\n\nder\nI-tag:ref;ent:org;sm:\nO\n\n\nPresenz\nI-tag:ref;ent:org;sm:\nO\n\n\nuf\nI-tag:ref;ent:org;sm:\nO\n\n\nBurg\nI-tag:ref;ent:org;sm:\nO\n\n\n[E-ORG]\nI-tag:ref;ent:org;sm:\nO\n\n\n,\nO\nO\n\n\nsonstfrei\nO\nO\n\n\n[DESC]\nO\nO",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#footnotes",
    "href": "submissions/462/index.html#footnotes",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://dls.staatsarchiv.bs.ch/records/1016781.↩︎\nhttps://dhbern.github.io/BeNASch/↩︎\nSee the appendix for an example in column format.↩︎",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/428/index.html",
    "href": "submissions/428/index.html",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "",
    "text": "In 2121, nothing is as it once was: a nasty virus is keeping the world on tenterhooks – and people trapped in their own four walls. In the depths of the metaverse, contemporaries are searching for data to compare the frightening death toll of the current killer virus with its predecessors during the Covid-19 pandemic and the «Spanish flu». There is an incredible amount of statistical material on the Covid-19 pandemic in particular, but annoyingly, this is only available in obscure data formats such as .xslx in the internet archives. They can still be opened with the usual text editors, but their structure is terribly confusing and unreadable with the latest statistical tools. If only those digital hillbillies in the 2020s had used a structured format that not only long-outdated machines but also people in the year 2121 could read… Admittedly, very few epidemiologists, statisticians and federal officials are likely to have considered such future scenarios during the pandemic years. Quantitative social sciences and the humanities, including medical and economic history, but also memory institutions such as archives and libraries, should consciously consider how they can sustainably preserve the flood of digital data for future generations. Thus, the sustainable processing and storage of statistical printed data from the time of the First World War makes it possible to gain new insights into the so-called “Spanish flu” e. g. in the city of Zurich even today. The publications by the Statistical Office of the City of Zurich, which were previously only available in “analog” paper format, have been digitized by the Zentralbibliothek (Central Library, ZB) Zurich as part of Joël Floris’ Willy Bretscher Fellowship 2022/2023 (Floris (2023)). This project paper has been written in the context of this digitisation project, as issues regarding digital recording, processing, and storage of historical statistics have always occupied quantitative economic historians “for professional reasons”. The basic idea of this paper is to prepare tables with historical health statistics in a sustainable way so that they can be easily analysed using digital means. The aim was to capture the statistical publications retro-digitized by the ZB semi-automatically with OCR in Excel tables and to prepare them as XML documents according to the guidelines of the Text Encoding Initiative (TEI), a standardized vocabulary for text structures. To do this, it was first necessary to familiarise with TEI and its appropriate modules, and to apply them to a sample table in Excel. To be able to validate the Excel table manually transferred to XML, I then developed a schema based on the vocabularies of XML and TEI. This could then serve as the basis for an automated conversion of the Excel tables into TEI-compliant XML documents. Such clearly structured XML documents should ultimately be relatively easy to convert into formats that can be read into a wide variety of visualisation and statistical tools.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#introduction",
    "href": "submissions/428/index.html#introduction",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "",
    "text": "In 2121, nothing is as it once was: a nasty virus is keeping the world on tenterhooks – and people trapped in their own four walls. In the depths of the metaverse, contemporaries are searching for data to compare the frightening death toll of the current killer virus with its predecessors during the Covid-19 pandemic and the «Spanish flu». There is an incredible amount of statistical material on the Covid-19 pandemic in particular, but annoyingly, this is only available in obscure data formats such as .xslx in the internet archives. They can still be opened with the usual text editors, but their structure is terribly confusing and unreadable with the latest statistical tools. If only those digital hillbillies in the 2020s had used a structured format that not only long-outdated machines but also people in the year 2121 could read… Admittedly, very few epidemiologists, statisticians and federal officials are likely to have considered such future scenarios during the pandemic years. Quantitative social sciences and the humanities, including medical and economic history, but also memory institutions such as archives and libraries, should consciously consider how they can sustainably preserve the flood of digital data for future generations. Thus, the sustainable processing and storage of statistical printed data from the time of the First World War makes it possible to gain new insights into the so-called “Spanish flu” e. g. in the city of Zurich even today. The publications by the Statistical Office of the City of Zurich, which were previously only available in “analog” paper format, have been digitized by the Zentralbibliothek (Central Library, ZB) Zurich as part of Joël Floris’ Willy Bretscher Fellowship 2022/2023 (Floris (2023)). This project paper has been written in the context of this digitisation project, as issues regarding digital recording, processing, and storage of historical statistics have always occupied quantitative economic historians “for professional reasons”. The basic idea of this paper is to prepare tables with historical health statistics in a sustainable way so that they can be easily analysed using digital means. The aim was to capture the statistical publications retro-digitized by the ZB semi-automatically with OCR in Excel tables and to prepare them as XML documents according to the guidelines of the Text Encoding Initiative (TEI), a standardized vocabulary for text structures. To do this, it was first necessary to familiarise with TEI and its appropriate modules, and to apply them to a sample table in Excel. To be able to validate the Excel table manually transferred to XML, I then developed a schema based on the vocabularies of XML and TEI. This could then serve as the basis for an automated conversion of the Excel tables into TEI-compliant XML documents. Such clearly structured XML documents should ultimately be relatively easy to convert into formats that can be read into a wide variety of visualisation and statistical tools.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#data-description",
    "href": "submissions/428/index.html#data-description",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Data description",
    "text": "Data description\nA table from the monthly reports of the Zurich Statistical Office serves as an example data set. The monthly reports were digitised as high-resolution pdfs with underlying Optical Character Recognition (OCR) based on Tesseract by the Central Library’s Digitisation Centre (DigiZ) as part of the Willi Bretscher Fellowship project. They are available on the ZB’s Zurich Open Platform (ZOP, Statistisches Amt der Stadt Zürich (1919)), including detailed metadata information. They were published by the Statistical Office of the City of Zurich as a journal volume under this title between 1908 and 1919, and then as «Quarterly Reports» until 1923. The monthly reports each consist of a 27-page table section with individual footnotes, and conclude with a two-page explanatory section in continuous text. For this study, the data selection is limited to a table for the year 1914 and the month of January (Statistisches Amt der Stadt Zürich (1919)). In connection with Joël Floris’ project, which aims at obtaining quantitative information on Zurich’s demographic development during the «Spanish flu» from the retro-digitisation project, it was obvious to focus on tables with causes of death. The corresponding table number 12 entitled «Die Gestorbenen (in der Wohnbev.) nach Todesursachen und Alter» («The Deceased (in the Resident Pop.) by Cause of Death and Age») can be found on page seven of the monthly report. It contains monthly data on causes of death, broken down by age group and gender, as well as comparative figures for the same month of the previous year. The content of this table is to be prepared below in the form of a standardized XML document with an associated schema that complies with the TEI guidelines.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#methods-for-capturing-historical-tables-in-xml",
    "href": "submissions/428/index.html#methods-for-capturing-historical-tables-in-xml",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Methods for capturing historical tables in XML",
    "text": "Methods for capturing historical tables in XML\nThe source of inspiration for this project paper was a pioneering research project originally based at the University of Basle. In the research project, the annual accounts of the city of Basle from 1535 to 1610 were digitally edited (Calvi (2015)). Technical implementation was carried out by the Center for Information Modeling at the University of Graz. Based on a digital text edition prepared in accordance with the TEI standard, the project manages to combine facsimile, web editing in HTML, and table editing via an RDF (Resource Description Framework ) and XSLT (eXtensible Stylesheet Language Transformations ) in an exemplary manner. The edition thus allows users to compile their own selection of booking data in a “data basket” for subsequent machine-readable analysis. In an accompanying article, project team member Georg Vogeler describes the first-time implementation of a numerical evaluation and how “even extensive holdings can be efficiently edited digitally” (Vogeler 2015). However, as mentioned, the central basis for this is XML processing of the corresponding tabular information based on the TEI standard. This project is based on the April 2022 version (4.4.0) of the TEI guidelines (Burnard (2022)). They include a short chapter on the preparation of tables, formulas, graphics, and music. And even the introduction to Chapter 14 is being rather cautious with regard to TEI application for table formats, warning that layout and presentation details are more important in table formats than in running text, and that they are already covered more comprehensively by other standards and should be prepared accordingly in these notations. On asking the TEI-L mailing list whether it made sense to prepare historical tables with the TEI table module, the answers were rather reserved (https://listserv.brown.edu/cgi-bin/wa?A1=ind2206&L=TEI-L#24). Only the Graz team remained optimistic that TEI could be used to process historical tables, albeit in combination with an RDF including a corresponding ontology. Christopher Pollin also provided github links via TEI to the DEPCHA project, in which they are developing an ontology for annotating transactions in historical account books.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#table-structure-in-tei-xml",
    "href": "submissions/428/index.html#table-structure-in-tei-xml",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Table structure in TEI-XML",
    "text": "Table structure in TEI-XML\nBasically, the TEI schema treats a table as a special text element consisting of line elements, which in turn contain cell elements. This basic structure was used to code Table 12 from 1914, which I transcribed manually as an Excel file. Because exact formatting including precise reproduction of the frame lines is very time-consuming, the frame lines in the project work only served as structural information and are not included as topographical line elements as TEI demands. Long dashes, which correspond to zero values in the source, are interpreted as empty values in the TEI-XML. I used the resulting worksheet as the basis for the TEI-XML annotation, in which I also added some metadata. I then had to create an adapted local schema as well as a TEI header, before structuring the table’s text body. Suitable heading (“head”) elements are the title of the table, the table number as a note and the «date» of the table. The first table row contains the column headings and is assigned the role attribute “label” accordingly. The third-last cell of each row contains the row total, which I have given the attribute “ana” for analysis and the value “#sum” for total, following the example of the Basle Edition. The first cell of each row again names the cause of death and must therefore also be labelled with the role attribute “label”. The second last row shows the sum of the current monthly table, which is why it is given the “#sum” attribute for all respective cells. Finally, the last line shows the total for the previous year’s month. It is therefore not only marked with the sum attribute, but also with a date in the label cell. A potential confounding factor for later calculations is the row “including diarrhea”, which further specifies diseases of the digestive organs but must not be included in the column total. Accordingly, it is provided with another analytical attribute called “#exsum”. As each cell in the code represents a separate element, the »digitally upcycled table 12 in XML format ultimately extends over a good 550 lines of code, which I’m happy to share on request.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#challenges-and-problems",
    "href": "submissions/428/index.html#challenges-and-problems",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Challenges and problems",
    "text": "Challenges and problems\nAn initial problem already arose during the OCR-based digitisation. The Central Library (ZB)’s Tesseract-based OCR software, which specializes in continuous text, simply failed to capture the text in the tables. I therefore first had to transcribe the table by hand, which is error-prone. In principle, however, it is irrelevant in TEI in which format the original text was created. The potential for errors when transferring Excel data into the “original” XML is also high, especially if the table is complex and/or detailed. Ideally, i. e. with a clean OCR table, it ought to be possible to export OCR content in pdfs to XML. When speaking with the ZB’s DigiZ, they confirmed not being happy with OCR quality anymore, and are considering improvement with regard to precision. Due to the extremely short instructions for table preparation in TEI, I underestimated the variety of different text components that TEI offers. The complexity of TEI is not clear from the rough overview of the individual chapters and their introductory descriptions. This only became clear while adjusting table 12 to TEI standards. By becoming accustomed to TEI, its limitations regarding table preparation also became more evident: It is fundamentally geared towards structuring continuous text rather than text forms, where the structure or layout also indicates meaning, as is the case with tables. The conversion of the sample table into XML and the preparation of an associated TEI schema, which is reduced to the elements present in the sample document, yet remains valid with the TEI standard, proved to be time-consuming code work. Thus, both the sample XML and the local schema each comprise over 500 lines of code – and this basically for only a single – though complex – table with a few metadata. In addition, the extremely comprehensive and complex TEI schema on which my XML document is based is not suitable for implementation in Excel. As a result, I had to prepare an XML table schema that was as general as possible, which may be used to convert the Excel tables into XML in the future, thus reducing error potential of the XML conversion.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#ideas-for-project-expansion",
    "href": "submissions/428/index.html#ideas-for-project-expansion",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Ideas for Project Expansion",
    "text": "Ideas for Project Expansion\nBecause, as mentioned, the OCR output of the tables in this case is not usable, it should now be crucial for any digitisation project to achieve high-quality OCR of the retro-digitised tables. Table recognition is definitely an issue in economic history research, and there are several open source development tools around on Git-Repositories, which yet have to set a standard, however. Ideally, the tables recognized in this way would then provide better text structures in the facsimile. With the module for the transcription of original sources, TEI offers extensive possibilities for linking text passages in the transcription with the corresponding passages in the facsimiles. Such links could ideally be used as training data for text recognition programs to improve their performance in the area of table recognition. Other TEI elements that lend structure to the table, such as the dividing lines and the long dashes for the empty values, could also serve as such structural recognition features. Additional important TEI elements such as locations and gender would further increase the content of the TEI XML format. Detailed metadata, as e.g. provided by the retro-digitized version of the ZOP, can be easily integrated into the TEI header area “xenodata”. Finally, in view of the complex structure of the tables, it is essential to understand and implement XSLT (eXtensible Stylesheet Language Transformation) for automated structuring, and as a basis for RDF used e.g. by the Graz team.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#conclusion",
    "href": "submissions/428/index.html#conclusion",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Conclusion",
    "text": "Conclusion\nSo far, tables seem to have had a shadowy existence within the Text Encoding Initiative (TEI) – or, as TEI pioneer Lou Burnard remarked in the TEI mailing list on behalf of my question whether TEI processing of tables made sense: “Tables are tricky”. The main reason for this probably lies in the continuous text orientation of existing tools and users, who are also less interested in numerical formats. In principle, however, preparation according to the TEI standard offers the opportunity to think conceptually about the function of tabularly structured data and to make changes, e.g. in serial sources such as statistical tables, comprehensible. The clearly structured text processing of TEI could provide a basis for improving the still rather poor quality of text recognition programs when recording tables. And a platform-independent, non-proprietary data structure such as XML would be almost indispensable for the sustainable long-term archiving of “digitally born” statistics, which have experienced a boom in recent years, and especially during the pandemic. After all, our descendants should also be able to access historical statistics during the next one.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#references",
    "href": "submissions/428/index.html#references",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "References",
    "text": "References\n\n\nBurnard, Syd, Lou und Bauman. 2022. P5: Guidelines for Electronic Text Encoding and Interchange, Version 4.4.0. https://tei-c.org/release/doc/tei-p5-doc/en/html/index.html.\n\n\nCalvi, Meili, Sonja. 2015. “Jahrrechnungen Der Stadt Basle 1535 Bis 1610 – Digital.” Edited by Susanna Burghartz. 2015. http://gams.uni-graz.at/context:srbas.\n\n\nFloris, Joël. 2023. “Die Spanische Grippe in Zürich Erzählen.” Willy-Bretscher-Fellow 2022/23. Juni 2023. &lt;. November 2023. https://www.zb.uzh.ch/de/zuerich/die-spanische-grippe-zuerich-erzaehlen.\n\n\nStatistisches Amt der Stadt Zürich. 1919. Monats-Berichte Des Statistischen Amtes Der Stadt Zürich 1918. Zurich. https://doi.org/https://doi.org/10.20384/zop-871.\n\n\nVogeler, Georg. 2015. “Warum Werden Mittelalterliche Und Frühneuzeitliche Rechnungsbücher Eigentlich Nicht Digital Ediert?” In Grenzen Und Möglichkeiten Der Digital Humanities, edited by Constanze Baum and Thomas Stäcker. Sonderband Der Zeitschrift Für Digitale Geisteswissenschaften, 1. https://doi.org/10.17175/sb001_007.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/443/index.html",
    "href": "submissions/443/index.html",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "",
    "text": "The Impresso project pioneers the exploration of historical media content across temporal, linguistic, and geographical boundaries. In its initial phase (2017-2020), the project developed a scalable infrastructure for Swiss and Luxembourgish newspapers, featuring a powerful search interface. The second phase, beginning in 2023, expands the focus to connect media archives across languages and modalities, creating a Western European corpus of newspaper and broadcast collections for transnational research on historical media.\nWe introduce Impresso 2 and review the evolution from the first to the second project. We also discuss the specific challenges to connecting newspaper and radio, our efforts to adapt text mining and exploration tools to the affordances of historical material derived from different modalities, and our approach to conducting comparative and data-driven historical research using semantically enriched sources, accessible through both graphical and API-based interfaces.",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/443/index.html#introduction",
    "href": "submissions/443/index.html#introduction",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "",
    "text": "The Impresso project pioneers the exploration of historical media content across temporal, linguistic, and geographical boundaries. In its initial phase (2017-2020), the project developed a scalable infrastructure for Swiss and Luxembourgish newspapers, featuring a powerful search interface. The second phase, beginning in 2023, expands the focus to connect media archives across languages and modalities, creating a Western European corpus of newspaper and broadcast collections for transnational research on historical media.\nWe introduce Impresso 2 and review the evolution from the first to the second project. We also discuss the specific challenges to connecting newspaper and radio, our efforts to adapt text mining and exploration tools to the affordances of historical material derived from different modalities, and our approach to conducting comparative and data-driven historical research using semantically enriched sources, accessible through both graphical and API-based interfaces.",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/443/index.html#media-monitoring-of-the-past-the-impresso-projects",
    "href": "submissions/443/index.html#media-monitoring-of-the-past-the-impresso-projects",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "Media Monitoring of the Past: The Impresso Projects",
    "text": "Media Monitoring of the Past: The Impresso Projects\nAt the intersection of natural language processing, history and design, the Impresso projects focus on the processing, enrichment and exploration of large-scale historical media sources, with the aim of transforming the accessibility and usability of media archives for historical research.\nThe objectives of the first project (2017-2020) were to improve text mining tools for historical texts, to enrich historical newspapers with new information layers in the form of semantic annotations, and to integrate such data into historical research workflows by means of a newly developed user interface. Impresso 1 compiled and semantically enriched a corpus of digitised Swiss and Luxembourg newspapers, and designed a scalable infrastructure and user interface, that together form the Impresso app (Düring, Bunout, and Guido 2024; Ehrmann 2021; Ehrmann et al. 2020; Romanello et al. 2020). Beyond simple browsing and often unsatisfactory keyword searches, this interface exploits new opportunities offered by semantic enrichments such as word embeddings, named entities, topic modelling and text reuse for content search and discovery, as well as comparative and critical perspectives on the available data (Düring et al. 2021, 2023; Düring, Bunout, and Guido 2024). Its creation has been guided by the principles of co-design (where all team members actively contribute through continuous push-pull interaction), generosity (offering multiple complementary access points to the collection) and transparency (providing information on e.g. tools, document processing, and data quality). Impresso 1 aimed to harness machine reading to support historical scholarship, and its interface has helped popularise the use of text mining-based enrichment for the retrieval and exploration of newspaper articles - now a de facto standard.\nTwo key observations form the basis of the second Impresso project (2023-2027). First, although newspaper digitisation and online accessibility has outpaced that of broadcast archives over the past two decades, the digitisation of radio collections has recently caught up, making it easier for scholars to select them as sources and opening up new processing opportunities. Second, despite considerable progress in facilitating exploration of digitised sources, particularly for newspapers, existing digital portals still fall short of meeting the needs of historical research. This deficit arises for two main reasons. First, current frameworks for exploring digitised media archives remain fragmented, confined to digital archive silos with country-based institutional portals, and digital media silos, where enrichments and exploration capabilities are typically limited to one language, one modality and one media type. Even with enhanced search capabilities, the search horizon remains narrow. Second, portals offer only passive exploration of static collections, whereas historical research proceeds through iterative processes of association and comparison of multiple objects of study. Furthermore, as the digital transformation increasingly permeates all phases of historical research, historians are calling for new methods to critically analyse data, tools and interfaces.\nBuilding on the achievements of Impresso 1, Impresso 2 envisions a comprehensive connection between media archives, aiming to enable the first-ever joint exploration of historical newspaper and radio content across temporal, linguistic, and national boundaries1. The aim is not merely to juxtapose collections and provide full-text search across them, but to enrich and connect these sources through multiple layers of semantic enrichment represented in a shared multilingual vector space, and to design appropriate, meaningful and transparent exploration capabilities for historical research from a transmedia and transnational perspective. These efforts are guided by original research on historical media ecosystems making use of this newly available data and of data-driven research methods.\nTo achieve this goal, Impresso 2 collaborates with 21 European partners, including national libraries, archives, newspapers, cultural heritage institutions, and international research networks.",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/443/index.html#challenges-in-enriching-and-integrating-historical-digitised-newspaper-and-radio-archives",
    "href": "submissions/443/index.html#challenges-in-enriching-and-integrating-historical-digitised-newspaper-and-radio-archives",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "Challenges in Enriching and Integrating Historical Digitised Newspaper and Radio Archives",
    "text": "Challenges in Enriching and Integrating Historical Digitised Newspaper and Radio Archives\nAs the mass media of their time, historical newspapers and radio broadcasts offer a daily account of the past and are key to the study of historical media ecosystems. Since the 19th century in print and the 1920s on air, these media have disseminated news, opinion, and entertainment, reported on events, and offered insights into the daily lives of past societies.\n\n\n\n\n\n\nFigure 1\n\n\n\nThey have both shaped and been shaped by their social, cultural, and political environments. Until now, these sources have mostly been studied in isolation, resulting in a plethora of parallel national histories (Fickers 2018). Although there has been a ‘transnational turn’ toward broader geographical and temporal perspectives (Badenoch, Fickers, and Henrich-Franke 2013; Fickers and Johnson 2012; Cronqvist and Hilgert 2017), in media history, transnational and transmedia perspectives are rare, particularly when focusing on the distribution of content rather than institutional histories.\nHow can we accurately connect large collections of newspapers and radio, provide effective means for their exploration in historical research, and put content-based transmedia history into practice? Impresso 2 undertakes a multi-dimensional approach to integrating historical newspaper and broadcasts, focusing on four interconnected areas (see also Figure 1):\n\nLaying the foundation stone by compiling and making available an unprecedented corpus of Western European digitised print and broadcast media.\nEnriching and connecting historical sources by transforming noisy and heterogeneous sources into semantically enriched and structured data, ultimately connected in a common vector space.\nConducting original historical research, which advances understanding of historical media ecosystems through various case studies, while also defining methods for data-driven analysis and identifying diverse user needs for data and interface design.\nDesigning and implementing new interfaces by creating different entry points to the data and its enrichments.\n\nIn pursuing these objectives, Impresso 2 faces a set of unique challenges arising from the central issue of aligning and connecting newspaper and radio archives.\n\n\n\n\n\n\nFigure 2\n\n\n\nIn many ways familiar sources, digitised radio and newspapers have however never been aligned and connected before. How to proceed? Their integration requires a careful examination of the nature of these media objects, their preservation by cultural heritage institutions, specific characteristics as archival records and legal boundaries which determine access to them. While many questions remain unanswered at this stage, we provide a brief overview of each project focus area, primarily focusing on the key issues raised by integrating radio and newspapers: how to document and contextualise the corpus, how to align radio and newspaper historical documents, how to integrate their content cross-lingually, how to analyse and compare, and how to explore and interact with the data2.\n\nThe Impresso Corpus: A Silo-Breaking, Transmedia and Transnational Corpus\nThe foundation of Impresso 2 is the creation of a large corpus of print and broadcast media collections across several countries. Building on the newspapers collected in the first Impresso project, the corpus expands to a geographically and historically coherent set of neighbouring countries, encompassing both newspaper and radio archives from each. To begin, let us review the core characteristics of the two media sources. Newspapers were disseminated and are preserved in print, while radio broadcasts, originally transmitted as audio signals, are preserved through audio recordings or typescripts (the text read by the speaker). Newspapers were typically published daily, with one issue per day, whereas radio broadcasts followed a highly variable rhythm, documented in radio schedules available in dedicated magazines, unpublished internal listings, and newspapers (see Figure 2). From a digitised archive perspective, newspaper materials consist of facsimiles and their transcriptions obtained via optical character recognition (OCR), whereas radio materials include facsimiles and OCR for typescripts and radio schedules, and audio recordings and transcripts generated through automatic speech recognition ASR for broadcasts. These materials encompass different modalities, such as text and image for newspapers, and text and sound for radio.\nData Acquisition and Sharing Framework\nAcquiring such a large and diverse corpus is a lengthy and complex endeavour with several obstacles: collections include various data elements (metadata, facsimiles, audio records, transcripts) with differing copyright statuses and are held by institutions across multiple countries, each with its own jurisdiction and legal constraints based on data owners’ preferences and institutional policies. We aim to make these sources accessible to researchers for operations such viewing, searching, and exporting. To address these issues, we have developed a dual approach: operational, by implementing a rigorous organisation to conduct dialogue and facilitate data exchange with our partners; and legal, by designing a modular data sharing and access framework that respects copyright and institutional constraints while maximising research opportunities through differentiated access. We believe that this operational and legal basis will help to break down institutional barriers.\nRich Contextual Information for Historical Research\nThe practical acquisition of the corpus also provides an opportunity to deepen our understanding of the sources, which is essential for their use. Although radio and newspaper archival records come with standard metadata, this information is often heterogeneous and varies significantly in content, quantity and quality across collections and institutions. Additionally, there are other sources of contextual knowledge, including unspoken or unwritten information. Two ongoing initiatives aim to further document the production and preservation of these archives to provide rich contextual information for historical research, all the more important in this multi-source context.\nThe first seeks to leverage the information contained in newspaper directories, following the approach outlined in (Beelen et al. 2022). As a starting point for the Swiss context, we are focusing on extracting semi-structured information from the Swiss Press Bibliography published by Fritz Blaser in 1956 (Blaser 1956). This bibliography documents in great detail about 480 newspapers and around 1,000 periodicals published in Switzerland between 1803 and 1958. It offers rich insights into the origins and history of Swiss newspapers, which can be used to enhance the documentation of newspapers in the Impresso corpus and interface, as well as support the study of the newspaper ecosystem in Switzerland. A similar approach will be used to trace the development of radio programmes through radio schedules and magazines.\nThe second initiative focuses on radio, adopting an oral history perspective to gain a better understanding not only of radio archives, but also of each archive custodian. What important aspects about these archives might we be unaware of? The ’Oral History of Radio Archivists” (OHRA) addresses this by conducting semi-structured interviews of Impresso audio partners.These interviews explore topics such as archival preservation and documentation policies, digitisation priorities, and the evolution of data quality over time, with the aim of providing complementary narrative descriptions of radio archives.\n\n\nEnriching and Connecting Historical Media Collections\nA critical step towards Impresso’s vision is the application of text and image processing techniques to the corpus. We aim to enrich and connect media sources through multiple layers of semantic enrichment, ultimately represented and connected in a common vector space. This endeavour involves three main steps; we discuss here only the first and the last.\nSource consolidation. Sources come in various digital formats and exhibit varying levels of refinement, both in terms of OCR and ASR quality and in the granularity of document layout analysis and article segmentation. We harmonise historical document data by establishing a consistent data representation and ensuring uniform characterisation of the material across all collections.\nFirst, we aim to define an appropriate and consistent data representation framework for both radio and newspaper digital materials. By ‘representation’, we refer to how data or information is effectively encoded and structured in a machine-readable format. The world of digitised newspapers is well understood, with a clear and consistent structure: a title consists of issues, which include pages containing articles and images. Each issue’s content is organised into sections, a framework that intersects with journalistic genres, exhibits distinct characteristics in both layout and content, and evolves over time. In contrast, digitised radio broadcasts present a more complex and heterogeneous landscape, lacking a shared definition of what constitutes a ‘standard’ broadcasting unit. There are varying levels of content organisation and an uneven distribution of material over time. Drawing from concrete evidence from files on disk, we carefully inventory all source elements for both media types, refining the terminology for newspapers and radio components – an often challenging process. With this groundwork, we then explore how to align collections’ structure and compositional units of both media sources. This alignment design, developed in parallel with data acquisition and in collaboration with archive partners, also informs and influences the rendering of sources in the interface.\nSecond, we elevate the corpus to a consistent and higher-quality level, through the assessment and optimisation of OCR and ASR quality, along with the homogenisation of content item segmentation and classes. The objective in this regard is to create “bridges” between radio broadcast and newspaper content types, enabling the retrieval of specific categories–such as editorials, sports-related (sub)sections, or radio schedules–across a set of titles, languages and a specific period in all digitised material.\nCross-lingual connection of sources. After enriching historical sources with semantic information to facilitate content-related search facets and support the exploration and comparison of people, locations, keyphrases, topics, and semantic classes across time periods, languages, and media channels (the second step not discussed here), a crucial task is establishing meaningful connections at the content level across both media, both mono- and cross-lingually. This process shifts focus away from structure, concentrating instead on content and its enrichments, with the goal of computing effective vector representations. This is relatively easy to achieve on a monolingual basis, but becomes more challenging cross-lingually, where the goal is to compute meaningful similarities of multilingual representations across languages.\n\n\nUnlocking the Dynamics of Historical Media Ecosystems\nThe focus of historical research activities within Impresso 2 is on examining influence—encompassing actors, themes, and formats—within a transnational media ecosystem. In our context, influence refers to the ability of individuals, groups, organisations or even texts (e.g. books, articles) to shape, direct, or alter narratives, imagery, content, opinions, behaviours, or outcomes related to a particular subject, issue, or topic represented by and through the media. Several case studies, which are both conceptually and methodologically complementary, investigate influences from various perspectives: external to the media, within the media ecosystem, within individual institutions, and across different content formats. This transnational and transmedia research aims to revisit and compare media autonomy, examining interactions between different media forms, such as radio and newspapers, beyond their competitive aspects. A central challenge is extend the historian’s traditional skill of making meaningful comparisons from incomplete and diverse sources to a large-scale, multilingual and transmedia corpus interconnected and enriched with semantic annotations. To this end, we are developing a comparative framework that leverages our data, tools, and interfaces.\n\n\nInterfaces\nTo facilitate research on our data, we are developing two complementary research-oriented user interfaces: the Impresso web app, a powerful graphical user interface, and the Impresso data lab, built around a user-oriented API. While the search interface offers a more traditional entry point to explore sources, enable close reading and the compilation of research datasets, the data lab is designed for automating access and enabling computational analysis. Our efforts focus on providing an easy programmatic access to the corpus and its enrichments through a public API and the Impresso Python library, allowing users to annotate external documents with Impresso tools and import external annotations into the web app (annotation and import services), and ensuring transparency with comprehensive documentation, including datasets, models notebook galleries, and tutorials. To further inform the design of the Impresso Datalab, we are surveying current approaches and realisation around data labs for computational humanities research (Beelen, During, and Guido 2024).",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/443/index.html#references",
    "href": "submissions/443/index.html#references",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "References",
    "text": "References\n\n\nBadenoch, Alexander, Andreas Fickers, and Christian Henrich-Franke, eds. 2013. Airy Curtains in the European Ether. Broadcasting and the Cold War. Baden-Baden: Nomos.\n\n\nBeelen, Kaspar, Marten During, and Daniele Guido. 2024. “Surveying Cultural Heritage Data Labs.” Reykjavík, Iceland. https://www.conftool.org/dhnb2024/index.php?page=browseSessions&presentations=show&search=during.\n\n\nBeelen, Kaspar, Jon Lawrence, Daniel C S Wilson, and David Beavan. 2022. “Bias and Representativeness in Digitized Newspaper Collections: Introducing the Environmental Scan.” Digital Scholarship in the Humanities, July, fqac037. https://doi.org/10.1093/llc/fqac037.\n\n\nBlaser, Fritz. 1956. “Bibliographie der Schweizer Presse : mit Einschluss des Fürstentums Liechtenstein = Bibliographie de la presse suisse = Bibliografia della stampa svizzera / bearb. von Fritz Blaser.” In Bibliographie der Schweizer Presse mit Einschluss des Fürstentums Liechtenstein = Bibliographie de la presse suisse = Bibliografia della stampa svizzera. Bern: [Schweizerische Nationalbibliothek].\n\n\nCronqvist, Marie, and Christoph Hilgert. 2017. “Entangled Media Histories.” Media History 23 (1): 130–41. https://doi.org/10.1080/13688804.2016.1270745.\n\n\nDüring, Marten, Estelle Bunout, and Daniele Guido. 2024. “Transparent Generosity. Introducing the Impresso Interface for the Exploration of Semantically Enriched Historical Newspapers.” Historical Methods: A Journal of Quantitative and Interdisciplinary History, June, 35–55. https://www.tandfonline.com/doi/full/10.1080/01615440.2024.2344004.\n\n\nDüring, Marten, Roman Kalyakin, Estelle Bunout, and Daniele Guido. 2021. “Impresso Inspect and Compare. Visual Comparison of Semantically Enriched Historical Newspaper Articles.” Information 12 (9): 348. https://www.mdpi.com/2078-2489/12/9/348.\n\n\nDüring, Marten, Matteo Romanello, Maud Ehrmann, Kaspar Beelen, Daniele Guido, Brecht Deseure, Estelle Bunout, Jana Keck, and Petros Apostolopoulos. 2023. “Impresso Text Reuse at Scale. An Interface for the Exploration of Text Reuse Data in Semantically Enriched Historical Newspapers.” Frontiers in Big Data 6. https://www.frontiersin.org/articles/10.3389/fdata.2023.1249469.\n\n\nEhrmann, Maud. 2021. “Explorer La Presse Numérisée: Le Projet Impresso.” Revue Historique Vaudoise, 159–73.\n\n\nEhrmann, Maud, Matteo Romanello, Simon Clematide, Phillip Benjamin Ströbel, and Raphaël Barman. 2020. “Language Resources for Historical Newspapers: The Impresso Collection.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, edited by Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, et al., 958–68. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.121.\n\n\nFickers, Andreas. 2018. “&lt;&lt;Hybrid Histories&gt;&gt;: Versuch Einer Kritischen Standortbestimmung Der Mediengeschichte.” Annali Dell’Istituto Storico Italo-Germanico in Trento/Jahrbuch Des Italienisch-Deutschen Historischen Instituts in Trient 1 (44): 117–31.\n\n\nFickers, Andreas, and Catherine Johnson. 2012. Transnational Television History: A Comparative Approach. Transnational Television History a Comparative Approach. London: Routledge. https://doi.org/10.4324/9780203723579.\n\n\nRomanello, Matteo, Maud Ehrmann, Simon Clematide, and Daniele Guido. 2020. “The Impresso System Architecture in a Nutshell.” EuropeanaTech Insights. EuropeanaTech Insights. https://pro.europeana.eu/page/issue-16-newspapers#the-impresso-system-architecture-in-a-nutshell.",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/443/index.html#footnotes",
    "href": "submissions/443/index.html#footnotes",
    "title": "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImpresso is the result of the collaboration of an interdisciplinary team composed of computational linguists, designers and historians. In addition to the presenters, the team includes: Marten Düring (co-PI, UniLu), Simon Clematide (co-PI, UZH), Ferdaous Affan (UniLu), Emanuela Boros (EPFL), Kaspar Beelen (London University), Estelle Bunout (UniLu), Pauline Conti (EPFL), Daniele Guido (Luxembourg University), Andrianos Michail (UZH), Arthur Michelet (UNIL), Juri Opitz (UZH). More on the team here.↩︎\nIt should be noted that the project has only recently started and all areas of work are in progress.↩︎",
    "crumbs": [
      "Abstracts",
      "Impresso 2: Connecting Historical Digitised Newspapers and Radio. A Challenge at the Crossroads of History, User Interfaces and Natural Language Processing."
    ]
  },
  {
    "objectID": "submissions/429/index.html",
    "href": "submissions/429/index.html",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "",
    "text": "Part of the national Lab In Virtuo project (2021-2024), the Techn’hom Time Machine project, initiated in 2019 by the Belfort-Montbéliard University of Technology, aims to study and digitally restore the history of an industrial neighborhood, with teacher-researchers but also students as co-constructors (Gasnier 2014; 2020, 293). The project is thus located at the interface of pedagogy and research. The Techn’hom district was created after the Franco-Prussian War of 1870 with two companies from Alsace: the Société Alsacienne de Constructions Mécaniques, nowadays Alstom; and the Dollfus-Mieg et Compagnie (DMC) spinning mill, in operation from 1879 to 1959. The project aims to create a “Time Machine” of these industrial areas, beginning with the spinning mill. We seek to restore in four dimensions (including time) buildings, machines with their operation, but also document and model sociability and know-how, down to the gestures and feelings. The resulting “Sensory Realistic Intelligent Virtual Environment” should allow both researchers and general public to virtually discover places and “facts” taking place in the industry, but also interact with them or even make modifications.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#introduction",
    "href": "submissions/429/index.html#introduction",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "",
    "text": "Part of the national Lab In Virtuo project (2021-2024), the Techn’hom Time Machine project, initiated in 2019 by the Belfort-Montbéliard University of Technology, aims to study and digitally restore the history of an industrial neighborhood, with teacher-researchers but also students as co-constructors (Gasnier 2014; 2020, 293). The project is thus located at the interface of pedagogy and research. The Techn’hom district was created after the Franco-Prussian War of 1870 with two companies from Alsace: the Société Alsacienne de Constructions Mécaniques, nowadays Alstom; and the Dollfus-Mieg et Compagnie (DMC) spinning mill, in operation from 1879 to 1959. The project aims to create a “Time Machine” of these industrial areas, beginning with the spinning mill. We seek to restore in four dimensions (including time) buildings, machines with their operation, but also document and model sociability and know-how, down to the gestures and feelings. The resulting “Sensory Realistic Intelligent Virtual Environment” should allow both researchers and general public to virtually discover places and “facts” taking place in the industry, but also interact with them or even make modifications.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#study-and-training-areas",
    "href": "submissions/429/index.html#study-and-training-areas",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Study and training areas",
    "text": "Study and training areas\nThe project is carried out within a technology university and, as such, is designed to include the participation of engineering students. They can apply and develop skills previously covered in a more basic way in their curriculum. This constitute for students an investment in the acquisition of skills that can subsequently be reused in their professional lives as engineers. In the current state, four main axes exist concerning inclusion of students in the Techn’hom Time Machine project:\n\nModeling of industrial buildings on Revit;\nMachine modeling on Catia;\nKnowledge engineering with the construction of a data model, initially as a relational database, having evolved into an RDF base based on standard ontologies;\nIntegration of those elements in the same virtual environment on Unity. Historical sources are crucial in all axes since many artifacts no longer exist, have been heavily modified and/or are inaccessible. Modeling is based on handwritten or printed writings, plans, iconography, and surviving heritage. This imposes a disciplinary opening for engineering students, untrained in the manipulation and analysis of such sources, and who may feel distant from issues linked to human and social sciences.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#project-progress",
    "href": "submissions/429/index.html#project-progress",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Project progress",
    "text": "Project progress\nTo date, thirty two students were included in the project. Each of the four axes was allocated between four and twelve students depending on opportunities and needs. In addition to the scientific contribution, student reports make it possible to evaluate their point of view on this training, all critical perspective retained.\n\nModeling: the software question\nThis axis has currently involved twelve students, and has led to the complete or partial modeling of six machines. It implies to reverse engineering machines with very partial data, on software designed for rendering of much more recent mechanisms. Students are assigned to work on small projects whose results are not necessarily directly usable. This offers the advantage of an exploratory and critical approach, by having a student take over the project of a previous one. Students were thus responsible for creating the model, but also for defining the software used. The first machine modeled, a twisting machine, was the subject of two successive works, linked to a change in modeling software. The first student used Blender, directing his work “on the optimization of models rather than on precision” and “took the initiative to abandon coherence”, offering “parts very close to the base material from a visual point of view but absolutely not reliable from a measurement point of view” (Bogacz 2019, 11–12). The following year, a second group was tasked of restoring consistency in this model, but realized that their colleague’s choices prevented such an achievement: pieces were too inaccurate, and conversion to a kinematic CAD model was impossible (Castagno and Vigne 2020, 11, 13). They therefore remade the model on Catia, without realistic texture. The team of another machine proposed another solution: on Catia, they “‘imagined’ missing parts”, paying attention to their mechanical coherence, while using Keyshot to obtain a more visually attractive final result (Paulin and Chambon 2020, 15–16). This questioning also occurred with integration of buildings and machines on Unity: models produced by specialized software are each quite efficient, but too heavy and ill-optimized to be all integrated in the same simulation. Students working on this topic thus have to take and reduce models in order to optimize performance, losing a part of the precision (Bozane 2022, 4–6, 10). Freedom left to students in technical solutions thus made it possible, by authorizing research and free experimentation, to identify configurations most likely to meet the needs of the project as a whole.\n\n\nWhich data model?\nSimilarly, tests “distribution” between students provided insights as to the appropriate type of data model. The Techn’hom Time Machine project was initially supposed to rely on a “classic” relational database. The first student to work on setting up said database quickly realized that a historical database involves “a certain complexity in its design”, necessitating a table for abstract concepts “most difficult to define”, and a table for specifying types of links between actors, but without informing in advance all possible types of relationships (Garcia 2020, 20, 23). In short, the student realized that, for a system as complex as a human society, a relational database quickly shows its limits. In fact, even if this first student still managed to create a relational database, the next two underlined its complexity: “the number of tables in the database makes reading difficult” (Ruff 2022, 7), and it was difficult to “precisely complete [it]” (Marais 2020, 9, 16). A fourth student, tasked to take up the previous work to refine it and make a functional application, concluded with the support of teacher-researchers that this database simply did not allowed to describe precisely enough a historical reality, and pointed the need to use an RDF graph database (Echard 2023, 15–16, 21). This solution, actually adopted, therefore comes once again from a series of works allowing a self-critique of the entire project, helping to define effective solutions.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#reflective-feedback-from-students",
    "href": "submissions/429/index.html#reflective-feedback-from-students",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Reflective feedback from students",
    "text": "Reflective feedback from students\nBeyond these contributions to the scientific project, this program also aims to offer training to students. The point that emerges most clearly from students’ reports, before any technical consideration or skills acquisition, relates to discovery of human sciences and their methodologies.\n\nDiscovering human sciences\nAlmost all of the students emphasize an initial dismay when faced with historical sources, lacking quantity, precision and conciseness of the information expected in an engineering context. Apart from a few immediately relevant sources, the mass of additional documentation, necessary to understand machines operation and context, is much more confusing and time-consuming to analyze, while offering mediocre quality of information. Students have “access to a lot of documents but little precision” (Bogacz 2019, 6, 8), and historical documents often “do not provide as much information as [they] hoped” (Castagno and Vigne 2020, 4). Moreover, students note that, even with good sources, machines “remain much more complex” than diagrams, and no blueprints, which “does not allow the direct connection and understanding of each piece” (Paulin and Chambon 2020, 8). The same goes for buildings, with damaged or partial plans, forcing to “make measurements on the plan to approximate distances” (Le Guilly 2022, 9).\nDespite this initial blockage, students developed solutions - starting with awareness that historical models can never “exactly” reproduce past reality. The most important resource consisted of seeking by themselves complementary sources, like archive originals (Marchal 2021, 5), old films (Castagno and Vigne 2020, 4), or “observations made on site” for buildings (Le Guilly 2022, 10). For machines, two other valuable sources could be mobilized, via contacts obtained by supervising teacher-researchers: dialogue with former workers about machines functioning and details (Bogacz 2019, 7; Paulin and Chambon 2020, 9); and visits in still-working spinning mills. Those experiences allowed them, according to their feedback, to better understand machines, operations but also context, “allowing [them] to take a step back from the project” (Bogacz 2019, 7). On the contrary, students working in the midst of the Covid pandemic, regretted not having been able to have the same experience (Paulin and Chambon 2020, 20). Direct contact with historical elements also include an emotional aspect highlighted by the students: “It was both a very interesting and very pleasant moment. Being able to see with our own eyes the machine that we were trying to reproduce computationally was a very enriching experience”; “The fact of visualizing in real life a machine that we had been modeling for several months is truly incredible” (Bogacz 2019, 7, 16).\nThis need to delve into sources implied for students the discovery, through practice, of the ins and outs of human sciences research. Typically, with data modeling, working from real data brings a certain advantage: working from “concrete cases […] helped us to understand how to articulate [several] ontologies and thus develop a strategy to combine them effectively into a coherent whole” (Echard 2023, 23, 32). Likewise, for buildings, sources comparison led students to perceive inconsistencies, and thus “note the importance of reading all the archives and not just a few because errors may be present” (Pic 2020, 3–4). Some also emphasize “difficulty of exploiting numerous bibliographic resources” in terms of synthesis capacities and working time (Bogacz 2019, 6; Castagno and Vigne 2020, 15), but also the pleasure of “learning to read archives” (Paulin and Chambon 2020, 20). The novelty of the practice compared to classic engineering curriculum is well summed up by one of the teams: “This type of task requires patience and a methodology completely different from what we have habit of doing. The difficulty or even the impossibility of finding the desired information taught us to put ourselves in the shoes of a historian who must at certain times make hypotheses in order to continue his work.” (Castagno and Vigne 2020, 15).\n\n\nProject managing\nWhatever the students’ specific project, it generally appeared to be a first in their training, positioning them as researchers over several months. This induced a “complete autonomy” (Garcia 2020, 8) underlined by all reports, often before competence gains. One, those project was “the most significant project he had to carry out”, “learned the management” of his organization (Bogacz 2019, 16). Another “learned to manage a project in [his] free time” (Le Guilly 2022, 10), and a third “learned to work efficiently and manage projects independently” (Echard 2023, 9, 40). Faced with complex and non-linear projects, students emphasize the “need to do a lot of research to use the right method to work correctly”, and to propose solutions on their own (Marchal 2021, 9–10, 27). The gross volume of work is finally underlined, projects requiring “time to understand the documents, research into software functionalities as well as a considerable investment” (Pic 2020, 16). Participation in the project can appear as “a first professional experience (…) The experience gained during the internship is immense” (Garcia 2020, 39).\nBeyond each individual work, some students also develop reflection on the overall project. In particular, they suffered from a lack of communication with their predecessor on the same subject, “making the task more difficult”, leading to risk of “wasting (…) time understanding what the other had already understood” (Castagno and Vigne 2020, 15). This experience lead to an awareness of the importance of good communication or documentation. Students therefore suggested organizing “video conferences between old and new groups”, and that “each group [should] bring together important documents in a separate file” during project transitions. They applied the lesson to their own report, by “explaining as best as possible what [they] had understood”, with concrete recommendations (Castagno and Vigne 2020, 15–16).",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#conclusion",
    "href": "submissions/429/index.html#conclusion",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Conclusion",
    "text": "Conclusion\nStudents involvement in the Techn’hom Time Machine project leads to bidirectional enrichment. The project benefits from the possibility of distributed work and multiple proposal strengths, making it possible to test several options in parallel on a given subject. Students deepen their knowledge of diverse software, while introducing themselves to human sciences and project management. Gain in technical skills is often implied in reports, obviously being an integral part of expectations of any engineering school project. Acquisition of more fundamental knowledge can be identified, with discovery of some entirely new technologies. An interest in the historical dimension is also mentioned, as well as human contacts with researchers and workers. Finally, the very fact of participating in a digital humanities project, atypical in itself, appears as a source of satisfaction.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#references",
    "href": "submissions/429/index.html#references",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "References",
    "text": "References\n\n\nBogacz, Florent. 2019. “Projet de Restitution Historique En 3D. Continu à Retordre à Anneaux Au Mouillé.”\n\n\nBozane, Marius. 2022. “Implémentation En VR Du Projet Techn’hom Time Machine.”\n\n\nCastagno, Javourez, Nathan, and Pierre Vigne. 2020. “Continu à Retordre – Rapport de Projet.”\n\n\nEchard, Noé. 2023. “Ontologie Et Base de Données Pour Le Projet Techn’hom Time Machine.”\n\n\nGarcia, Gabriel. 2020. “Quelle Base de Données Pour Techn’hom Time Machine ?”\n\n\nGasnier, Marina. 2014. “Territorialisation Urbaine Et Processus de Patrimonialisation : Le Cas de Techn’hom à Belfort (Franche-Comté).” Annales de Géographie 699: 1168–92.\n\n\n———. 2020. “Techn’hom Time Machine : Un Patrimoine Industriel Augmenté.” Artefact 12: 293–99.\n\n\nLe Guilly, Erwann. 2022. “Reconstitution de Bâtiment Sur Revit.”\n\n\nMarais, Maxence. 2020. “Mémoire Techn’hom Time Machine.”\n\n\nMarchal, Jean-Baptiste. 2021. “Techn’hom Time Machine.”\n\n\nPaulin, Antoine, and Rémi Chambon. 2020. “Modélisation d’un Dévidoir.”\n\n\nPic, Pierre. 2020. “Rapport de Projet : Reconstitution 3D Du Poste de Transformation de l’usine DMC de Belfort.”\n\n\nRuff, Guillaume. 2022. “TTM Front End.”",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/473/index.html",
    "href": "submissions/473/index.html",
    "title": "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”",
    "section": "",
    "text": "For some time now, there has been a desire to elevate film to the status of a scholarly publication, and thus recognize it as a research publication in its own right. Never before has this dream seemed so concrete: analog film collections are being extensively digitized and made reliably referenceable via permanent links.\nA recent example of large-scale digital (online) publication of scientific films is the so-called AV portal of the Technische Informationsbibliothek Hannover (TIB), the German National Library of Science and Technology. Since 2004, the TIB has acted as a registration agency for digital object identifiers (DOI) for non-textual scientific objects such as data, software, videos, etc.1 In 2020, it was named Library of the Year by the German Library Association for its “pioneering role in shaping digital change”.2 The video collection on the AV portal has grown exponentially over the last ten years, especially in terms of talks and lectures, which currently comprise 35,000 of 45,000 contributions and have click figures in the thousands. However, 4,800 of the films – and thus a tenth of the current TIB holdings – were published by the Institut für den wissenschaftlichen Film Göttingen (IWF). The IWF was founded in the 1950s and was a leading national and international institute in the production, distribution, collection and conception of scientific films. For its flagship project, the Encyclopaedia Cinematographica (EC), films were published from 1952 until the 1990s, and archived and distributed until the institute’s liquidation in 2011. The film, photo and text collections as well as the contracts with the authors/scientists/filmmakers were then transferred to the TIB.\nMore than two-thirds of the IWF’s films are now available online, 1,500 films are not, as they are subject to “licensing restrictions”. They can only be viewed after contacting the TIB. The most popular films – not only in the IWF inventory, but in the AV portal as a whole – are films about the sexual behavior of animals – bonobos, domestic donkeys or Camargue horses – with a click lead of tens of times over other topics. In addition to films, the AV portal also lists formal, technical and content-related metadata.\nFirst of all, there is a gap between the rich digital present of the AV portal and the analog history of the IWF collection. For example, there is no information on the provenance of the collection on the AV portal. On the overview page of the holdings of the former IWF Wissen und Medien gGmbH, there is no mention of the National Socialist predecessor institution, the Reichsstelle für den Unterrichtsfilm (RfdU), later Reichsanstalt für Film und Bild in Wissenschaft und Unterricht (RWU). However, it is listed as the publisher of several hundred films. The overview of the IWF only mentions the amount of films, the ongoing clarification of rights, the transfer to the TIB in 2012 and information about accessibility.3 The impression that emerges is that what interests the TIB about the scientific films is the opportunity to take on a pioneering role in the digital transformation, but not the history of the films, of the collection and of the institutions. Second, although the digitization and accessibility of the film collection online is a crucial prerequisite for its study, library interfaces like the AV portal of the TIB not always provide the most suitable tools for research of such complex materials. In the online interface, the films can hardly be experienced as a cohesive collection, but rather as individual and itemized entries. Meaningful relations between films, e.g. whether they are part of a series of similar films, are not well represented in the data. Searching within the library catalog often tends to remain on the edges of results, making it difficult to gain a broader or more condensed view of the collection that would allow for relevant insights into its logics and structure.\nThus, researching large collections, especially film collections, is logistically and conceptually challenging, as well as technically, ethically, and legally demanding. The challenging layering of media, histories, technologies, and institutional agendas requires not only different “literacies” but also specifically designed research tools.\nThe SNSF-funded research project “Visualpedia. ‘Atlas Encyclopaedia Cinematographica’ and the Visual Science and Technology Studies” (2022–2026)4 is dedicated to the question of how such a collection, entangled in the logics of archival digitization and digital library organization, can be appropriately activated and researched. The project is guided by the question of how scholars can not only research about visual artifacts, but also research with them, which is based on Peter Galison’s Visual STS first and second order approach (Galison 2014). On a meta-level, “Visualpedia” is also an experiment in collaborative research practices. In addition to a historical reappraisal of the institution and the collection, the team is developing various custom browser-based digital interfaces to activate the collection and reconfigure it in ways that facilitate its study more appropriately. It is important to note that our goal is neither to create a “better library catalog” with a particular “end user” in mind, nor to provide a streamlined interface for the library that licenses the films, nor is it to speed up the digital transformation of film archives. Rather, our aim is to slow it down, or at least to make it a little more thoughtful and a little more difficult by acknowledging the historical, medial, ethical, and legal complexities that surround this collection.\nThe first interface developed in the project, titled “E-EC Explorer” (working title) provides an overview of the roughly 3000 EC films in the TIB library catalog, that so far has been missing for an extensive study of the collection. The interface is based on the publicly available data provided by TIB that we scraped from the website of the AV portal. Entries are arranged in a scalable grid and include metadata for each film, preview images if available, and links to the original entries in the library catalog. Films can be filtered by basic categories, like the thematic section of the collection, with more filters and sorting capabilities currently in development. This relatively straightforward visual reconfiguration of the collection already provides a much better sensibility for the structure and aesthetics of the film encyclopedia as a whole. The interface also indicates the accessibility and copyright status of each entry, thus implicitly generating a mapping of the digitization logics of TIB.\nA second interface prototype, “E-EC Shuffle”, provides a more subjective view on the collection from “within” the material, complementary to the overview of the “Explorer”. Here, short clips from two randomly selected EC films play side by side, with films changing in alternating fashion after every few seconds. The selection is limited to films that fall under a Creative Commons license. This setup is meant to allow for a “sampling” of the large-scale collection to convey a sense of its filmic contents, aesthetics, and techniques, for instance. This parallel view also serves to test and hypothesize about possible projection situations that may have been historically intended for the presentation of EC films.\nA further group of interfaces in the project each focuses on a particularly relevant subset or series of films, oriented along the three thematic sections of the collection: biology, technical sciences and ethnology. Here, selected films are arranged in compact overviews and capable to be played back simultaneously to allow for a concentrated comparative view. These interfaces are intended for a more detailed study of selected films that may exemplify the encyclopedic ambitions, aesthetic tendencies or historic conceptions of the EC as well as serve as visual arguments (in the sense of VSTS) to evaluate research hypotheses within the project.\nThe interfaces’ overall aim is to also highlight the need for more context-specific research tools, especially when working with complex and rich-media materials like large film collections. They can show that the design of basic but tailored visual and organizational media operations (e.g. generating overview, filtering, selective and synchronous views) can already vastly open up and activate such materials for research and allow for more appropriate configurations that facilitate insights and argumentation.\nCurrently, the interfaces developed are mostly internal tools that help us sharpen research questions, develop designs, and deal with the restrictive legal issues surrounding the materials. During the development, we presented them to experts from various fields (such as science and museum studies) and adapted them according to their reactions. Further important steps in the evaluation process of the interfaces will be the feedback at the Digital History Conference and in the course of the art exhibition “String Figures / Fadenspiele. A Research Exhibition” at the Museum Tinguely Basel (November 2024 – March 2025),5 where some of the interfaces will be made accessible to the public. It is also planned to evaluate the more finalized versions of the interfaces outside the project, e.g. with people from the source communities, other EC experts, members of the TIB, or other researchers from large film collections.\nTeam members\nMoritz Greiner-Petter, Mario Schulze, Sarine Waltenspül",
    "crumbs": [
      "Abstracts",
      "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”"
    ]
  },
  {
    "objectID": "submissions/473/index.html#references",
    "href": "submissions/473/index.html#references",
    "title": "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”",
    "section": "References",
    "text": "References\n\n\nGalison, Peter. 2014. “Visual STS.” In Visualization in the Age of Computerization, edited by Annamaria Carusi, Aud Sissel Hoel, Timothy Webmoor, and Steve Woolgar, 197–225. New York: Routledge.",
    "crumbs": [
      "Abstracts",
      "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”"
    ]
  },
  {
    "objectID": "submissions/473/index.html#footnotes",
    "href": "submissions/473/index.html#footnotes",
    "title": "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnonymous, “TIB vergibt millionsten Digital Object Identifier. Bibliothek ist zweiterfolgreichstes DataCite-Mitglied nach Registrierungszahlen,” in: Nachrichten Informationsdienst Wissenschaft, 20/4/2015, https://nachrichten.idw-online.de/2015/04/20/tib-vergibt-millionsten-digital-object-identifier (accessed 20/12/2023).↩︎\nFrank Mentrup quoted after https://www.bibliotheksverband.de/bibliothek-des-jahres#BibliothekdesJahres2020 (accessed 28/11/2023).↩︎\nSee https://www.tib.eu/de/recherchieren-entdecken/sondersammlungen/iwf-medienbestand (accessed 4/12/2023).↩︎\nSee https://www.unilu.ch/fakultaeten/ksf/institute/seminar-fuer-kulturwissenschaften-und-wissenschaftsforschung/wissenschaftsforschung/forschung/visualpedia-atlas-encyclopaedia-cinematographica-and-the-visual-science-and-technology-studies/ (accessed 13/8/2024).↩︎\nCurated by Mario Schulze and Sarine Waltenspül, see https://www.tinguely.ch/en/exhibitions/exhibitions/2024/fadenspiele.html (accessed 13/8/2024).↩︎",
    "crumbs": [
      "Abstracts",
      "Digital Film Collection Literacy – Critical Research Interfaces for the “Encyclopaedia Cinematographica”"
    ]
  },
  {
    "objectID": "submissions/480/index.html",
    "href": "submissions/480/index.html",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "",
    "text": "To start with some definitions, the term “flora” – in the sense of a document – denotes a directory in which the plant species of a specific area are systematically listed, often together with a description and additional information. “Herbarium” refers to a collection of preserved (usually dried and pressed) plants or fungi for scientific purposes; an individual botanical object in a herbarium collected at a specific place and time is called a “specimen” (Wagenitz 2003).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#introduction",
    "href": "submissions/480/index.html#introduction",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "",
    "text": "To start with some definitions, the term “flora” – in the sense of a document – denotes a directory in which the plant species of a specific area are systematically listed, often together with a description and additional information. “Herbarium” refers to a collection of preserved (usually dried and pressed) plants or fungi for scientific purposes; an individual botanical object in a herbarium collected at a specific place and time is called a “specimen” (Wagenitz 2003).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#floras-and-herbaria-in-the-history-of-knowledge",
    "href": "submissions/480/index.html#floras-and-herbaria-in-the-history-of-knowledge",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Floras and herbaria in the history of knowledge",
    "text": "Floras and herbaria in the history of knowledge\nBoth the flora and the herbarium are regarded as decisive innovations in the development of botany into an independent scientific discipline from the mid-16th century onwards. Together with the botanical garden, which is functionally related and established in the same period (Rieppel 2016; Findlen 2006a), only the invention of the herbarium made it possible to work out regional floras based on systematic empirical fieldwork (Flannery 2023; Müller-Wille 2019). Paula Findlen summarised the motivation behind this crucial invention: “The more naturalists observed nature in situ, the more they realized that limited contact with specimens did not yet yield enough knowledge to describe and compare medicinal herbs. They needed to take nature home” (Findlen 2006b, 447; see Sunderland 2016). Findlen stands for the history of knowledge or the renewed history of science that firstly emphasises the shift from finalised knowledge to the act of its production, secondly shows an increased interest in the everyday intellectual life of small groups, circles or networks and thirdly focuses on practices and material cultures of knowledge (Müller-Wille, Carsten, and Sommer 2017; Förschler and Mariss 2017; Holenstein, Steinke, and Stuber 2013). In this perspective and with the catchy phrase “collecting as knowledge”, the creation of a natural history collection, such as a herbarium, is seen as knowledge production (Heesen and Spary 2001). The activity of collecting expresses not only the fact that dispersed natural objects are brought together in a single location, but also that the forms of representation associated with them, such as illustrations, descriptions, lists and publications, are included in the repositories, where they are available for comparison, retracing and synoptic synthesis (Klemun 2017, 235).\nThe precise structures and functions of floras and herbaria with their spatial relations between local and global can only be understood in the context of the “collaborative knowledge culture of botany”. Therefore the interplay of the three central resources on which botany depended in early modern times has to be reconstructed: living and dried plants, relevant specialised literature (e.g. floras) and correspondence (Dietz 2017a, 2017b). The nexus of correspondence, plant transfer and collection policy was first reconstructed by Emma Spary using the example of the network of André Thouin (1747–1824), director of the Jardin du Roi in Paris (Spary 2000, 49–98). The analysis of such networks draws attention to the extensive transfer of dried plants and seeds as the basis of knowledge production (Dauser et al. 2008). A wide variety of methods has been used to correspond efficiently, to save time and to avoid loss of information. First and foremost is the use of reference catalogues. This means that lists of transmitted or desired plant species could simply be referred to the numbers that had been assigned to the species in an published flora (Dietz 2017a, 96–99). Secondly, network analyses show that natural history owes its existence not only to the outstanding figures, but also developed through the participation of thousands of amateurs working locally (Klemun 2017, 239). Correspondence networks not only serve to improve understanding of herbaria and floras, it is also possible to go in the opposite direction: herbaria themselves can serve as a source for social network analyses by systematically evaluating the collectors of the individual specimen (Siracusa et al. 2020; Groom, O’Reilly, and Humphrey 2014).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#digitisation-of-herbaria-from-a-botanical-perspective",
    "href": "submissions/480/index.html#digitisation-of-herbaria-from-a-botanical-perspective",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Digitisation of herbaria from a botanical perspective",
    "text": "Digitisation of herbaria from a botanical perspective\nThe value of herbaria has long been recognized in the fields of taxonomy, systematics and biogeography. Moreover, in recent decades they have proven to be fundamental for dealing with the biology of climate change, biodiversity, phenology, conservation and biological invasions. Given the high scientific and cultural value of herbarium collections, many efforts to make them more accessible have already been made in the last 20 years. Digitization is an essential first step in the process of transforming this vast amount of data associated with physical specimen into flexible digital data formats that allow information to be re-categorized according to variable criteria (Roma-Marzio et al. 2023, 108; see generally Andraschke and Wagner 2020). Building on centuries of research based on herbarium specimens collected over time and around the globe, which are freely accessible and aggregable, a “new era” of discovery, synthesis and prediction using digitized collection data is postulated (James et al. 2018; Nelson and Ellis 2018). Digitization and online availability of specimen facilitates the rapid exploration and dissemination of accurate biodiversity data on an unprecedented scale: “The emerging ‘herbarium of the future’ (or the ‘global metaherbarium’) will be the central element guiding the exploration, illumination, and prediction of plant biodiversity change in the Anthropocene” (Davis 2023, 412). It should be borne in mind that collections are usually associated with various distortions that need to be characterised and mitigated to make data usable. Most common are taxonomic and collector biases, which can be understood as the effects of particular recording preferences of key collectors on the overall taxonomic composition of the biological collections to which they contribute (Siracusa et al. 2020; Davis 2023, 421; Jaroszynska et al. 2023). In order to capture such phenomena so that they can be taken into account in the data analysis, precise knowledge of the entire context in which a herbarium was created is required. This is exactly the aim of the approach described above under history of knowledge. Obviously, there is a bridge here between the research interests of the natural sciences and the humanities. An overview published in 2024 shows that the topic of accessibility and digitization of herbaria as “archives of biodiversity” has also gained new relevance in Switzerland in recent years. Apart from two major exceptions, the Platter-Herbarium and Les Herbiers de Rousseau, there have been no attempts to do this in an interdisciplinary context (Stämpfli 2024). Additionally there is a lack of including the interaction with the functionally linked correspondence networks and contemporary floras. For this reason, the experience with historical plants gained on hallerNet, on which we pursued an interdisciplinary approach to the interaction between different types of entities (letters, species, specimens, reviews), may be of general interest.",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#historical-plants-on-the-data-and-edition-platform-hallernet",
    "href": "submissions/480/index.html#historical-plants-on-the-data-and-edition-platform-hallernet",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Historical plants on the data and edition platform hallerNet",
    "text": "Historical plants on the data and edition platform hallerNet\nThe data and edition platform hallerNet opens up historical networks of knowledge in Switzerland in their interconnectedness. The platform is institutionally supported by the Albrecht von Haller Foundation of the Burgergemeinde Bern and the Historical Institute and the Institute for the History of Medicine at the University of Bern. The basis of the currently around 128,000 data objects is formed by extensive prosopographical and bibliographical data that has been compiled in a relational database (FAUST) since the early 1990s as part of three SNSF projects at the University of Bern. A transformation project (2016–2019) transferred this extensively interlinked data into a XML data structure compliant to the Text Encoding Initiative (TEI) and thus turned it into “reusable research data” based on the FAIR data criteria (Dängeli and Stuber 2020). The platform nowadays contains, among others, around 46,000 publications, 31,000 persons and 1,200 institutions, which are systematically linked to 9,000 edited reviews and 5,000 edited letters. In our context, the 4,955 plant entities currently on hallerNet are at the centre. Starting point were the 1,737 species of flowering plants mentioned in Haller’s Swiss Flora Historia Stirpium (Haller 1768), which were systematically referenced in the aforementioned relational database to Haller’s first edition of the Swiss Flora Enumeratio (Haller 1742), to Linné’s Species plantarum (Linné 1753) and to the current nomenclature. This concordance between Haller’s and Linné’s nomenclature, compiled by Luc Lienhard with reference to Johan Rudolf Suter’s Flora Helvetica (Suter 1802), not only makes Haller’s Swiss flora accessible, but does also provide access to pre-Linnaean botany in general. On this basis, we transformed the botanical data, which was originally divided into four different data types, for the new XML structure into a generic data model based on today’s plant entities (InfoFlora, Global Biodiversity Information Facility GBIF), and treat their (historical) names as name variants. In this way, entities become flexibly adaptable for other historical floras that are partly or completely outside Haller’s and Linné’s nomenclature. At the same time, the data model structured according to today’s nomenclature facilitates reference to current issues in historical ecology. The following summary of realized, initiated or planned expansions illustrates this double advantage.\n\nEcological data: The diverse ecological information in Haller’s Historia on habitat, frequency, typical altitudinal range and specific localities is of far above-average quality for the 18th century (Lienhard 2005; Drouin and Lienhard 2008). hallerNet systematically records a total of 7,545 locality details, whereby the 1,920 different localities have been georeferenced for the most part as kilometre squares with their corner points and additionally linked to the neighbouring municipalities (‘populated places’) in order to appear in the hallerNet place register. Haller’s extraordinary data is thus available in a flexible structure whose exploration is only just beginning (Lienhard 2008, 2000). Historical biodiversity research, which is high on the agenda of environmental history (Goethem and Zanden 2019), has a wealth of source material at its disposal. Using appropriate methods of analysis, this will massively extend its temporal scope beyond the Swiss state of research (Jaroszynska et al. 2023; Wang et al. 2023; Stöckli et al. 2012; Lachat 2010).\nCollectors and correspondents: Haller’s Historia also often includes the collector to whom Haller owes the information. These 109 people are all curated on hallerNet and systematically referenced for each species (1,342 times in total). The network is to be further expanded by systematically labelling the plants mentioned in Haller’s botanical correspondence, some of which has already been edited on hallerNet and most of which are made accessible via the International Image Interoperability Framework (IIIF). How great the potential of the correspondence is, both for the reconstruction of the ‘knowledge culture’ and for the supplementation and specification of the location data, is demonstrated by some analyses already available (Favre 2021; Hächler 2008; Lienhard 2005).\nBook references and reviews: For his extensive information on the plant species of his Swiss flora, Haller also uses a vast amount of historical data from his predecessors. For example synonyms and place references, which have already been added to hallerNet. Together with the links to Hallers other botanical publications (Steinke and Profos 2004, 186–95), to the numerous botanical publications in Hallers personal library (Monti 1983–1994, integrated in hallerNet) and to Hallers countless botanical reviews in the Göttingische Gelehrten Anzeigen, all of which are available in edited form on hallerNet, the integral process of botanical knowledge production could be precisely reconstructed (see Dietz submitted; Lienhard 2005; Drouin and Lienhard 2008).\nUseful plants: 656 species or varieties are listed in a total of eleven systematic catalogues in the context of the Bernese Economic Society, which was presided over by Haller. In this catalogues, the Latin-universal plant names were consistently linked to the dialectal-regional plant names. On hallerNet, 755 actions are linked to them, most of which obtained from the meetings of the Economic Society (Stuber and Lienhard 2007; Stuber 2008). In addition, Haller’s Swiss Flora contains information on the medicinal or economic use of more than a quarter of all the flowering plants. This reveals a wide range of interferences with medicine, agriculture, forestry and economy (Dauser and Stuber submitted; Gerber-Visser and Stuber 2019; Stuber 2018; Boscani Leoni and Stuber 2017).\nHerbaria: The digitisation of Haller’s herbaria is one of the most intruding unfulfilled postulates in the study of Haller’s botany and beyond. Haller’s main herbarium, which after being sold by his heirs to Emperor Joseph II was first sent to Pavia and later to Paris by Napoleon, is now in the Muséum National d’Histoire Naturelle and comprises more than 10,000 specimens in a total of 60 volumes (including 8 volumes of cryptogams) (Margéz, Aupic, and Lamy 2006; Zoller 1958a); further there is a smaller herbarium by Haller in Göttingen (Zoller 1958b). Due to the fact that the current location of Haller’s herbaria is not in Switzerland, its digitization is not in scope of being supported by the ongoing SwissCollNet project, the national initiative for the digitization of natural history collections (Frick, Stieger, and Scheidegger 2019). As part of SwissCollNet, however, the lichen herbarium of Jean-Frédéric Chaillet (1747–1839), which is kept in the Neuchâtel herbarium (NEU), has now been edited on hallerNet as sub-project Lichens of the Enlightenment led by Jason Grant. This is consequent in terms of content, as Chaillet operated as a direct Swiss successor to Haller and referred to him wherever possible. At the same time, it represents a milestone for hallerNet, as the platform data structures for Herbaria could be developed. The centrepiece are the 943 lichen specimens, which firstly contain the transcribed original information on the label. Secondly, they are linked to the original scan via IIIF and additionally with positional accuracy, as there are several specimens sticked on one herbarium page. Thirdly, they are assigned to the species entities, which point to authority data (GBIF, Index Fungorum, SwissLichens). This species entities also contain the data from historical floras, in this case all from the manuscript flora by Chaillet and, where already listed, from Haller’s Historia. The information from historical floras is often the decisive key to relate the objects in a herbarium to present-day taxonomic databases. The assignment of source terms to standardised data is thus presented transparently on the platform, which is particularly essential for a period in which botanical nomenclature is still very unstable. Additionally, the structure of the data follows Darwin Core standard which facilitates the connection to other systems such as the Neuchâtel Herbarium, the emerging SwissCollNet database or the global Index Herbariorum (Vust et al. in prep.).\nPlant lists: The connectivity of hallerNet is also demonstrated by Meike Knittel’s ongoing guest edition of plant lists in the circle of the Zurich botanist Johannes Gessner (1709–1790), the Naturforschende Gesellschaft and the botanical garden, which document the actual exchange of seeds and list a total of 1,829 individual actions (see Knittel in print).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#conclusion",
    "href": "submissions/480/index.html#conclusion",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Conclusion",
    "text": "Conclusion\nReconstructing the whole interaction in which floras and herbaria interplayed, difficulties arise in integrating digital approaches to historical correspondence networks (e.g. Edmondson and Edelstein 2019) with digitization methods for floras and herbaria, which are located in different scientific disciplines. The challenge for the data and edition platform hallerNet is therefore to find interdisciplinary solutions. With tools, methods and workflows of the digital humanities, traceable relations between text, scans and structural data are determined in an innovative way. That allows to rely today’s botanical authority data systematically to the historical information such as changing plant names, specimens, locality information and plant collectors. For the interoperability of the data, the orientation towards the Darwin Core standard is mandatory, for the sustainable editorial quality the TEI guidelines. Originally developed in the natural sciences, the FAIR data principles became a standard in the humanities (especially for GLAM institutions), and thus serve as an overarching guideline; in particular, FAIR guarantees the sustainable handling of data, which therefore remains ‘reusable’ for future generations of users because the traces of the normalization and flexibilization processes can be traced in detail. With this integration of different disciplinary standards and different types of sources, hallerNet could become a dynamic and cross-collection instrument for the interdisciplinary research of historical plants and biodiversity in Switzerland in the period before 1850. The current transformation of hallerNet into the national collaborative platform République des Lettres, which began running in 2024 with the support of the Data Science Lab of the University of Bern, will further strengthen this potential.",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#references",
    "href": "submissions/480/index.html#references",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "References",
    "text": "References\n\n\nAndraschke, Udo, and Sarah Wagner, eds. 2020. Objekte im Netz. Wissenschaftliche Sammlungen im digitalen Wandel. Bielefeld.\n\n\nBoscani Leoni, Simona, and Martin Stuber, eds. 2017. Wer das Gras wachsen hört. Wissensgeschichte(n) der pflanzlichen Ressourcen vom Mittelalter bis ins 20. Jahrhundert. Jahrbuch für Geschichte des ländlichen Raumes 14.\n\n\nDängeli, Peter, and Martin Stuber. 2020. “Nachhaltigkeit in langjährigen Erschliessungsprojekten. FAIR-Data Kriterien bei Editions- und Forschungsplattformen zum 18. Jahrhundert.” xviii.ch, Schweizerische Zeitschrift für die Erforschung des 18. Jahrhunderts / Revue suisse d’études sur le XVIIIe siècle 11: 34–51. https://doi.org/10.24894/2673-4419.0000410.1515/9783486712339.\n\n\nDauser, Regina, Stefan Hächler, Michael Kempe, Franz Mauelshagen, and Martin Stuber, eds. 2008. Wissen im Netz. Botanik und Pflanzentransfer in europäischen Korrespondenznetzen des 18. Jahrhunderts. Berlin.\n\n\nDauser, Regina, and Martin Stuber. submitted. “Pflanzliche Ressourcen – Interferenzen zwischen Botanik und Ökonomischer Aufklärung.” In Korrespondenz und Kritik. Albrecht von Haller als paradigmatische Figur der entstehenden Scientific Community, edited by Martin Stuber, Bernhard Metz, and Hubert Steinke. Göttingen.\n\n\nDavis, Charles C. 2023. “The Herbarium of the Future.” Trends in Ecology & Evolution 38 (5): 412–23. https://doi.org/10.1016/j.tree.2022.11.015.\n\n\nDietz, Bettina. submitted. “Orte der Kritik in der Botanik des 18. Jahrhunderts.” In Korrespondenz und Kritik. Albrecht von Haller als paradigmatische Figur der entstehenden Scientific Community, edited by Martin Stuber, Bernhard Metz, and Hubert Steinke. Göttingen.\n\n\n———. 2017a. Das System der Natur. Die kollaborative Wissenskultur der Botanik im 18. Jahrhundert. Köln u.a.\n\n\n———. 2017b. “Kollaboration in der Botanik des 18. Jahrhunderts. Die partizipative Architektur von Linnés System der Natur.” In Verfahrensweisen der Naturgeschichte in der Frühen Neuzeit. Akteure Tiere Dinge, edited by Silke Förschler and Anne Mariss, 93–108. Köln.\n\n\nDrouin, Jean-Marc, and Luc Lienhard. 2008. “Botanik.” In Albrecht von Haller. Leben, Werk, Epoche, edited by Hubert Steinke, Urs Boschung, and Wolfgang Pross, 292–314. Göttingen.\n\n\nEdmondson, Chloe, and Dan Edelstein, eds. 2019. Networks of Enlightenment. Digital Approaches to the Republic of Letters. Oxford.\n\n\nFavre, Madline. 2021. “Réseaux, pratiques et motivations des acteures locaux de la recherche botanique en milieu alpin. Le cas du Valais entre 1750 et 1810.” In Histoire naturelle et montagnes – Storia naturale e montagne – Naturgeschichte und Berge, edited by Simona Boscani Leoni, Anne-Lise Head, and Luigi Lorenzetti, 32–49. Histoire des Alpes / Storia delle Alpi / Geschichte der Alpen 26.\n\n\nFindlen, Paula. 2006a. “Anatomy Theaters, Botanical Gardens, and Natural History Collections.” In Early Modern Science, edited by Katharina Park and Lorraine Daston, 272–89. The Cambridge History of Science 3. Cambridge.\n\n\n———. 2006b. “Natural History.” In Early Modern Science, edited by Katharina Park and Lorraine Daston, 435–68. The Cambridge History of Science 3. Cambridge.\n\n\nFlannery, Maura C. 2023. In the Herbarium: The Hidden World of Collecting and Preserving Plants. New Haven.\n\n\nFörschler, Silke, and Anne Mariss, eds. 2017. Akteure, Tiere, Dinge : Verfahrensweisen der Naturgeschichte in der Frühen Neuzeit. Köln.\n\n\nFrick, Holger, Pia Stieger, and Christoph Scheidegger. 2019. “SwissCollNet – a National Initiative for Natural History Collections in Switzerland.” Biodiversity Information Science and Standards 3: e37188. https://doi.org/10.3897/biss.3.37188.\n\n\nGerber-Visser, Gerrendina, and Martin Stuber. 2019. “Naturgeschichte Helvetiens als Projekt der Ökonomischen Aufklärung. Jakob Samuel Wyttenbachs Forschungsbericht (1788) und Albrecht Höpfners Magazin (1787-1789).” In Politische, gelehrte und imaginierte Schweiz. Kohäsion und Disparität im Corpus helveticum des 18. Jahrhunderts, edited by André Holenstein, Claire Jaquier, Timothée Léchot, and Daniel Schläppi, 225–52. Genève.\n\n\nGoethem, Thomas van, and Jan Luiten van Zanden. 2019. “Who Is Afraid of Biodiversity? Proposal for a Research Agenda for Environmental History.” Environment and History 25: 613–47. https://doi.org/10.3197/096734018X15254461646440.\n\n\nGroom, Quentin, C. O’Reilly, and T. Humphrey. 2014. “Herbarium Specimens Reveal the Exchange Network of British and Irish Botanists, 1856–1932.” New Journal of Botany 4, 2: 95–103. https://doi.org/10.1179/2042349714Y.0000000041.\n\n\nHächler, Stefan. 2008. “Avec une grosse boete de plantes vertes. Pflanzentransfer in der Korrespon-denz Albrecht von Hallers (1708-1777).” In Wissen im Netz. Botanik und Pflanzentransfer in europäischen Korrespondenznetzen des 18. Jahrhunderts, edited by Regina Dauser, Stefan Hächler, Michael Kempe, Franz Mauelshagen, and Martin Stuber, 201–18. Berlin.\n\n\nHaller, Albrecht von. 1742. Enumeratio methodica stirpium Helvetiae indigenarum. Gottingae: ex officina academica Abrami Vandenhoek.\n\n\n———. 1768. Historia stirpium indigenarum Helvetiae inchoata. Bernae: sumptibus Societatis typographicae.\n\n\nHeesen, Anke te, and Emma C. Spary, eds. 2001. Sammeln als Wissen. Göttingen.\n\n\nHolenstein, André, Hubert Steinke, and Martin Stuber, eds. 2013. Scholars in Action. The Practice of Knowledge and the Figure of the Savant in the 18th Century. 2 vols. Leiden; Boston.\n\n\nJames, Shelley A, PS. Soltis, L. Belbin, AD. Chapman, G. Nelson, DL. Paul, and M. Collins. 2018. “Herbarium Data: Global Biodiversity and Societal Botanical Needs for Novel Research.” Applications in Plant Sciences 6, 2: e1024. https://doi.org/10.1002/aps3.1024.\n\n\nJaroszynska, Francesca, Christian Rixen, Sarah Woodin, Jonathan Lenoir, and Sonja Wipf. 2023. “Resampling Alpine Herbarium Records Reveals Changes in Plant Traits over Space and Time.” Journal of Ecology 111 (2): 338–55. https://doi.org/10.1111/1365-2745.14062.\n\n\nKlemun, Marianne. 2017. “Gärten und Sammlungen.” In Handbuch Wissenschaftsgeschichte, edited by Marianne Sommer, Staffan Müller-Wille, and Carsten Reinhard, 235–44. Stuttgart.\n\n\nKnittel, Meike. in print. Blühende Beziehungen. Botanische Praktiken im Zürich des 18. Jahrhunderts.\n\n\nLachat, Thibault, ed. 2010. Wandel der Biodiversität in der Schweiz seit 1900: ist die Talsohle erreicht? Bern u.a.\n\n\nLienhard, Luc. 2000. “Haller et la découverte botanique des Alpes.” In Une cordée originale. Histoire des relations entre science et montagne, edited by Jean-Claude Pont and Jan Lacki, 96–119. Chêne-Bourg.\n\n\n———. 2005. “La machine botanique. Zur Entstehung von Hallers Flora der Schweiz.” In Hallers Netz. Ein europäischer Gelehrtenbriefwechsel zur Zeit der Aufklärung, edited by Martin Stuber, Stefan Hächler, and Luc Lienhard, 371–410. Basel.\n\n\n———. 2008. “Wegränder, Wiesen, Sümpfe – Flora und Lebensräume.” In Berns goldene Zeit. Das 18. Jahrhundert neu entdeckt, edited by André Holenstein, Daniel Schläppi, Dieter Schnell, Hubert Steinke, Martin Stuber, and Andreas Würgler, 56–59. Bern.\n\n\nLinné, Carl von. 1753. Species plantarum. Stockholm: Laurenz Salvi.\n\n\nMargéz, Marlène, Cécile Aupic, and Denis Lamy. 2006. “La restauration de l’herbier Haller du Muséum national d’Histoire naturelle.” Support tracé 5: 354–60.\n\n\nMonti, Maria Teresa, ed. 1983–1994. Catalogo del Fondo Haller della Biblioteca Nazionale Braidense di Milano. 13 vols. Milano.\n\n\nMüller-Wille, Staffan. 2019. “Botanik.” In Enzyklopädie der Neuzeit Online. https://doi.org/10.1163/2352-0248_edn_COM_248430.\n\n\nMüller-Wille, Staffan, Reinhardt Carsten, and Marianne Sommer. 2017. “Wissenschaftsgeschichte und Wissensgeschichte.” In Handbuch Wissenschaftsgeschichte, edited by Marianne Sommer, Staffan Müller-Wille, and Carsten Reinhard, 2–18. Stuttgart.\n\n\nNelson, Gil, and Shari Ellis. 2018. “The History and Impact of Digitization and Digital Data Mobilization on Biodiversity Research.” Phil. Trans. R. Soc. B 374: 2017039. https://doi.org/10.1098/rstb.2017.0391.\n\n\nRieppel, Lukas. 2016. “Museums and Botanical Gardens.” In A Companion to the History of Science, edited by Bernard Lightman, 238–51. Oxford u.a.\n\n\nRoma-Marzio, Francesco, S. Maccioni, D. Dolci, G. Astuti, N. Magrini, F. Pierotti, R. Vangelisti, L. Amadei, and L. Peruzzi. 2023. “Digitization of the Historical Herbarium of Michele Guadagno at Pisa (PI-GUAD).” PhytoKeys 234: 107–25. https://doi.org/10.3897/phytokeys.234.109464.\n\n\nSiracusa, Pedro C., Jr. Gadelha, M. R. Luiz, and Arthur Ziviani. 2020. “New Perspectives on Analysing Data from Biological Collections Based on Social Network Analytics.” Sci Rep 10, 3358. https://doi.org/10.1038/s41598-020-60134-y.\n\n\nSpary, Emma. 2000. Utopias Garden : French Natural History from the Old Regime to Revolution. London.\n\n\nStämpfli, Remo. 2024. “Pflanzen im Netz: Die Möglichkeiten des Digitalen bei der Erschliessung und Vermittlung von Herbarien und Herbarbelegen.” In Informationswissenschaft: Theorie, Methode Und Praxis, 8 (1):123–44. https://doi.org/10.18755/iw.2024.8.\n\n\nSteinke, Hubert, and Claudia Profos, eds. 2004. Bibliographia Halleriana. Verzeichnis der Schriften von und über Albrecht von Haller. Basel.\n\n\nStöckli, Veronika, S. Wipf, C. Nilsson, and C. Rixen. 2012. “Using Historical Plant Surveys to Track Biodiversity on Mountain Summits Plant.” Ecology and Diversity 4: 415–25.\n\n\nStuber, Martin. 2008. “Kulturpflanzentransfer im Netz der Oekonomischen Gesellschaft Bern.” In Wissen im Netz. Botanik und Pflanzentransfer in europäischen Korrespondenznetzen des 18. Jahrhunderts, edited by Regina Dauser, Stefan Hächler, Michael Kempe, Franz Mauelshagen, and Martin Stuber, 229–69. Berlin.\n\n\n———. 2018. “Vom Simmental bis Spitzbergen. Albrecht von Haller als europäischer Vermittler regionaler Kultur und Ökonomie.” In Zwischen den Kulturen. Mittler und Grenzgänger vom 17. bis 19. Jahrhundert, edited by Joachim Eibach and Claudia Opitz- Belakhal, 165–89. Hannover.\n\n\nStuber, Martin, and Luc Lienhard. 2007. “Nützliche Pflanzen. Systematische Pflanzenverzeichnisse von Wild- und Kulturpflanzen im Umfeld der Oekonomischen Gesellschaft.” In Nützliche Wissenschaft und Ökonomie im Ancien Régime. Akteure, Themen, Kommunikationsformen, edited by André Holenstein, Martin Stuber, and Gerrendina Gerber-Visser, 65–106. Cardanus. Jahrbuch für Wissenschaftsgeschichte 7.\n\n\nSunderland, Mary E. 2016. “Specimens and Collections.” In A Companion to the History of Science, edited by Bernard Lightman, 488–99. Oxford u.a.\n\n\nSuter, Johann Rudolf. 1802. Helvetiens Flora, worinn alle im Hallerischen Werke enthaltenen und seither neu entdeckten Schweizer Pflanzen nach Linné’s Methode aufgestellt sind. 2 Bde. s.l.\n\n\nVust, Mathias, Edouard Di Maio, Christian Forney, and Martin Stuber. in prep. “Les herbiers historiques, défi pour les humanités numériques - l’herbier des lichens de J.-F. Chaillet comme projet pilote.” In Usages, pratiques et fonctions des herbiers historiques. Actes Colloque Ascona.\n\n\nWagenitz, Gerhard. 2003. Wörterbuch der Botanik. 2. Aufl. Heidelberg. Berlin.\n\n\nWang, Jessica, Markus Fischer, Stefan Eggenberg, and Katja Rembold. 2023. “The Impact of Climate Change on Plant Distribution and Niche Dynamics over the Past 250 Years in Switzerland.” Bauhinia 29: 101–12.\n\n\nZoller, Heinrich. 1958a. “A l’occasion du 250e anniversaire de Albrecht von Haller: quelques remarques sur son oeuvre botanique et ses collections.” Bulletin de Museum national d’histoire naturelle série 2, t. 30, no 3: 305–12.\n\n\n———. 1958b. “Albrecht von Hallers Pflanzensammlungen in Göttingen, sein botanisches Werk und sein Verhältnis zu Carl von Linné.” Nachrichten der Akademie der Wissenschaften in Göttingen 2, Mathematisch-physikalische Klasse, Nr. 10: 217–51.",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/458/index.html",
    "href": "submissions/458/index.html",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "",
    "text": "In our 2021 paper, “Network of Words: A Co-Occurrence Analysis of Nation-Building Terms in the Writings of Liang Qichao and Chen Duxiu,” (Journal of Historical Network Research, 5:154-185), we created a word co-occurrence network to plot the relationship between selected words, in order to compare different interpretations of terms such as “democracy,” “constitutionalism,” and “citizen” in the writings of two famous Chinese intellectuals of early Republican China.\nWe now propose to re-examine the writings of Chen Duxiu (1879-1942), the co-founder of the Chinese Communist Party, by using the innovative Large Language Models (LLMs) LLMA and ChatGPT.(Bian et al. 2021) We intend to leverage ChatGPT to detect changes in Chen’s thinking about the theory and practice of Marxism and Communism. Through embedding and prompt engineering, we plan to extract topic sentences, generate summary statements and estimate topic opinions on these summaries. Our work introduces two key innovations:\n\nthe utilization of advanced AI methodologies grounded in extensive language models, as opposed to traditional statistical techniques that often relied on oversimplified assumptions, and\nan extension of our analysis to sentences rather than individual words, allowing for richer contextual understanding.\n\nA brief biography of Chen Duxiu is as follows: As a young boy Chen chafed against the traditional preparation to study for the all-important civil service exam. He deplored the ineffectiveness of gaining governmental positions through rote memorization of ancient classics. Surprisingly he placed first in the entry level of this exam. Soon Chen left to study in Japan, where he first encountered Western democratic philosophies such as those of John Stuart Mills, Jean Jacques Rousseau and Montesquieu. He concluded that the only way to save China was to overthrow the dynasty. Returning to China in 1903 he joined an assassination squad and published newspapers rallying his countrymen to fight foreign imperialism and to overthrow the dynasty. One of his journals, called Xin Qingnian 新青年 [New Youth], whose contributors ultimately consisted of some of the most respected and celebrated scholars and intellectuals of the time, made Chen a celebrated public intellectual. Together these authors pushed through the national language reform, denounced the restrictive Confucian ethos, and advocated scientific thinking, democracy, and individual freedom. With the disappointing outcome from the Treaty of Versailles in 1919, whereby China’s hope to recover German occupied Shandong peninsula was dashed, Chen and many of his colleagues became disillusioned with Western style democracy. Within two years, Chen co-founded the Chinese Communist Party (CCP) with the help of Russian Comintern agents and turned his attention to political activism. Politics proved to be treacherous, however, and Chen was scapegoated for the failure of Comintern policies in China and ousted from the CCP in 1929. He was jailed by Chiang Kai-shek, head of the Nationalist Party (GMD), from 1932-1937, and released from prison at the onset of the Resist Japan war. Distancing himself from both the CCP and the GMD, Chen became a political pariah whose writings few dared to publish. Undaunted, he continued to comment on the state of Chinese politics and died in penurious circumstances in 1942.\nWhat were Chen’s final views on Western democracy, capitalism, and communism? In his youthful optimism, he declared France to have gifted humanity with three powerful concepts: human rights, evolutionary theory, and socialism.(C. Duxiu, n.d.) Caught in the power struggle between Stalin and Trotsky, Chen lost his political leadership. Tragically he also lost two of his sons at the hands of the GMD. Did the vicissitudes of life affect his thinking? We turn to his essays to find out.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#introduction",
    "href": "submissions/458/index.html#introduction",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "",
    "text": "In our 2021 paper, “Network of Words: A Co-Occurrence Analysis of Nation-Building Terms in the Writings of Liang Qichao and Chen Duxiu,” (Journal of Historical Network Research, 5:154-185), we created a word co-occurrence network to plot the relationship between selected words, in order to compare different interpretations of terms such as “democracy,” “constitutionalism,” and “citizen” in the writings of two famous Chinese intellectuals of early Republican China.\nWe now propose to re-examine the writings of Chen Duxiu (1879-1942), the co-founder of the Chinese Communist Party, by using the innovative Large Language Models (LLMs) LLMA and ChatGPT.(Bian et al. 2021) We intend to leverage ChatGPT to detect changes in Chen’s thinking about the theory and practice of Marxism and Communism. Through embedding and prompt engineering, we plan to extract topic sentences, generate summary statements and estimate topic opinions on these summaries. Our work introduces two key innovations:\n\nthe utilization of advanced AI methodologies grounded in extensive language models, as opposed to traditional statistical techniques that often relied on oversimplified assumptions, and\nan extension of our analysis to sentences rather than individual words, allowing for richer contextual understanding.\n\nA brief biography of Chen Duxiu is as follows: As a young boy Chen chafed against the traditional preparation to study for the all-important civil service exam. He deplored the ineffectiveness of gaining governmental positions through rote memorization of ancient classics. Surprisingly he placed first in the entry level of this exam. Soon Chen left to study in Japan, where he first encountered Western democratic philosophies such as those of John Stuart Mills, Jean Jacques Rousseau and Montesquieu. He concluded that the only way to save China was to overthrow the dynasty. Returning to China in 1903 he joined an assassination squad and published newspapers rallying his countrymen to fight foreign imperialism and to overthrow the dynasty. One of his journals, called Xin Qingnian 新青年 [New Youth], whose contributors ultimately consisted of some of the most respected and celebrated scholars and intellectuals of the time, made Chen a celebrated public intellectual. Together these authors pushed through the national language reform, denounced the restrictive Confucian ethos, and advocated scientific thinking, democracy, and individual freedom. With the disappointing outcome from the Treaty of Versailles in 1919, whereby China’s hope to recover German occupied Shandong peninsula was dashed, Chen and many of his colleagues became disillusioned with Western style democracy. Within two years, Chen co-founded the Chinese Communist Party (CCP) with the help of Russian Comintern agents and turned his attention to political activism. Politics proved to be treacherous, however, and Chen was scapegoated for the failure of Comintern policies in China and ousted from the CCP in 1929. He was jailed by Chiang Kai-shek, head of the Nationalist Party (GMD), from 1932-1937, and released from prison at the onset of the Resist Japan war. Distancing himself from both the CCP and the GMD, Chen became a political pariah whose writings few dared to publish. Undaunted, he continued to comment on the state of Chinese politics and died in penurious circumstances in 1942.\nWhat were Chen’s final views on Western democracy, capitalism, and communism? In his youthful optimism, he declared France to have gifted humanity with three powerful concepts: human rights, evolutionary theory, and socialism.(C. Duxiu, n.d.) Caught in the power struggle between Stalin and Trotsky, Chen lost his political leadership. Tragically he also lost two of his sons at the hands of the GMD. Did the vicissitudes of life affect his thinking? We turn to his essays to find out.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#methodology",
    "href": "submissions/458/index.html#methodology",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Methodology",
    "text": "Methodology\nThe central objective is to compare the efficacy of our earlier co-occurrence network methodology with the novel ChatGPT approach. We seek the best method to detect Chen’s ideological evolution over time. We used the corpus of Chen’s writing, consisting of 892 articles and 1,347,699 characters.(C. Duxiu 1914--1940) From this collection we selected fifteen articles that were salient in his thoughts on political theory, written in the years 1914-1940.(C. Duxiu 1914--1940) The analysis was performed using various Python libraries and tools. Transformers were used for loading pre-trained language models, FAISS for efficient similarity search and clustering, Jieba for Chinese text tokenization, and Docx for handling Word documents. Numpy, Pandas, and Sklearn were used for numerical operations and data handling, and Langchain was used for managing document schemas and prompts.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#preprocessing-and-text-tokenization",
    "href": "submissions/458/index.html#preprocessing-and-text-tokenization",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Preprocessing and Text Tokenization",
    "text": "Preprocessing and Text Tokenization\nThe Chinese text was tokenized using Jieba, which segmented the text into meaningful tokens. Sentence Detection: A custom function called “chinese_sentence_detector “was implemented to detect sentences based on punctuation marks specific to Chinese (e.g.,”。“,”！“,”？“).",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#embedding-and-similarity-analysis",
    "href": "submissions/458/index.html#embedding-and-similarity-analysis",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Embedding and Similarity Analysis",
    "text": "Embedding and Similarity Analysis\nWe loaded the Colossal-LLaMA-2-7b-base model. Sentences from the documents were embedded using the loaded model. The embeddings were stored in a NumPy array. Next, we computed pair-wise cosine similarity scores between the embeddings of all sentences.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#query-processing",
    "href": "submissions/458/index.html#query-processing",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Query Processing",
    "text": "Query Processing\nThe question: “Is this article in favor of communism or capitalism?” [这篇文章是支持共产主义还是支持资本主义?”] was embedded using the same model and tokenizer. The FAISS index was used to search for the top 5 sentences most similar to the query. The indices and distances of the closest matches were retrieved.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#output-generation",
    "href": "submissions/458/index.html#output-generation",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Output Generation",
    "text": "Output Generation\nThe sentences retrieved from the similarity search provided the context for answering the query. The following steps illustrate the workflow for a specific set of documents:\n\nDefine the volume pattern for document selection.\nEmbed the sentences in the document using the pre-trained model.\nConstruct the FAISS index for efficient search.\nPerform query embedding and similarity search.\nRetrieve and save the top similar sentences as the answer to the query.\n\nFor the second part of the analysis, we employed a combination of natural language processing (NLP) techniques and the GPT-4 API to classify political opinions in textual documents. The primary objective was to determine whether the texts supported communism or capitalism. We used Python for automating the analysis process, integrating various tools and libraries to achieve this.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#question-design",
    "href": "submissions/458/index.html#question-design",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Question Design",
    "text": "Question Design\nWe formulated a question to extract the political sentence from the text. The question posed to the GPT-4 model was: “Is this article in favor of communism or capitalism?” [这篇文章是支持共产主义还是支持资本主义?”]",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#api-request",
    "href": "submissions/458/index.html#api-request",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "API Request",
    "text": "API Request\nThe openai.ChatCompletion.create method was utilized to interact with the GPT-4 API. We constructed a prompt with the following messages:\n\nSystem Message: Provided context to the model, indicating that it was functioning as a helpful assistant.\nUser Messages: Included the embedded document content, followed by instructions for providing a score on a scale from 0 to 4 (where 0 indicates strong support for communism and 4 indicates strong support for capitalism) and a detailed explanation within 200 to 300 words.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#response-extraction",
    "href": "submissions/458/index.html#response-extraction",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Response Extraction",
    "text": "Response Extraction\nThe response from the API was parsed to extract the generated content, which included the classification score and explanatory text. In summary, our method leveraged the GPT-4 API to analyze and classify political opinions in text documents. By embedding the documents and querying the model with specific questions and instructions, we automated the classification process and systematically stored the results for subsequent review.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#results",
    "href": "submissions/458/index.html#results",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Results",
    "text": "Results\nThe summaries of the fifteen articles produced by Llama were perfunctory and inconclusive. At times Llama changed the order of sentences in an effort to answer the query, but it often generated a text that was more a random collection of sentences than a cohesive answer to the question.\nChatGPT4 produced a much more systematic and accurate summary for each article. However, we detected five articles that variously missed the intent of the essay, or forced fit the answer, or gave a wrong numerical coding. Of the remaining ten articles, ChatGPT4 best answered the question “Is this article in support communism or capitalism?” when the essay consistently argued for one point of view, such as in “Shehui zhuyi piping” [A critique of socialism] and “Makesi xueshuo” [A study of Marxism]. Where ChatGPT4 ran into problems is when the writing was nuanced and contained multiple points of view. Because of its effort to answer the question, it sometimes force fit the answer by implying connections that did not exist. We briefly describe these five issues below.\n\nMissing the context and the main argument of the article\nIn his 1914 article, “Aiguo xin yu zijue xin” [Patriotism and self-awareness], Chen famously shocked his readers by concluding that a nation which did not care for its people should not expect support from its citizens, and that such a nation should be allowed to die. Lacking such context, ChatGPT4 picked up on Chen’s description of an ideal nation but did not underscore the novelty of his perspective. It coded the article as -1 (mildly in support of communism). The article never mentioned communism, and at the same time, the command was to rate articles from 0 to 4, not -2 to 2.\n\n\nTrying too hard to answer the question\nIn the 1915 article, “Fa lanxi ren yu jinshi wenming” [The French and contemporary civilization], Chen praised socialism and mentioned French socialists such as Henri de Saint-Simon, Gracchus Babeuf and Charles Fourier. It concluded that Chen was in favor of socialism but not rejecting capitalism. It erroneously stated that socialism was the same as communism, which did not appear in the text.\n\n\nInaccurate coding\nIn the 1916 article, “Tan zhengzhi” [Talk about politics], Chen strongly attacked the bourgeois exploitation of the working class, which was accurately picked up by ChatGPT4. However, it gave the article a +1 code, meaning it was somewhat in favor of capitalism. This is surprising since it concluded that the article was in favor of communism. Similarly in the 1924 article, “Women de huita” [Our response], Chen was defending the reason why the CCP was part of the GMD (an ill-fated decision ordered by Stain), and strongly argued for class struggle between the bourgeoisie and the proletariat. ChatGPT4 recognized this interpretation but surprisingly rated the article as +1 (mildly in support of capitalism).\n\n\nMissed by a wide margin\nHistorians often cited the conflicting tone of “Gei Xiliu de xin” [A letter to Xiliu], one of Chen’s last essays, as indicative of his ambivalence about communism. He railed against Stalin’s dictatorship, the lack of freedoms of speech, assembly, publishing, the right to strike, and the right to an opposition party in Russia. He wrote that in his opinion, bourgeois democracy and proletarian democracy are similar in nature but differed only as a matter of degrees.(chen Duxiu 1940) Bourgeois democracy, Chen explained, was the culmination of a struggle by millions of people over six hundred years, and that gifted the world with the three big principles of science, democratic system and socialism. “We must recognize” he stated, “that the incomplete democracy of England, France and the United States deserve to be protected.” Chen created a table comparing the freedoms people enjoyed in a capitalist democracy versus those under Stalin and Hitler, and asked, “Would any Communist dare curse bourgeois democracy?” ChatGPT4 picked up the anti-fascist sentiment but missed Chen’s ambivalence toward bourgeois democracy. Instead it described the article as Chen’s way to use the struggle between fascist and Western democratic countries to advance the causes of the proletariat. We found this conclusion to be off the mark and as an attempt to force the conclusion.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#conclusion",
    "href": "submissions/458/index.html#conclusion",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "Conclusion",
    "text": "Conclusion\nChatGPT 4 is able to detect strong and consistent sentiments in articles, but would force fit its answer and rationalize connections that did not exist. It does less well when multiple viewpoints are expressed, and it does not know the historical context. Llamas is unable to perform the analysis at all.\nIn conclusion, the use of ChatGPT 4 for analyzing historical texts presents both significant advantages and notable challenges. ChatGPT 4 has demonstrated its capability to detect strong and consistent sentiments in articles, providing systematic and accurate summaries in many cases. However, it occasionally forces connections that do not exist, particularly in texts with nuanced or multiple viewpoints. This limitation highlights the importance of historical context, which the model sometimes misses, leading to misinterpretations of the larger point. Conversely, the Llama 2-7b model was unable to perform the analysis adequately, emphasizing there may be a need for larger models. Our study underscores the need for continuous refinement in leveraging AI methodologies for historical text analysis. Future work should focus on enhancing the model’s contextual understanding and its ability to handle complex, multi-faceted arguments, thereby improving its overall efficacy in historical and ideological analysis.\nThrough our analysis, we gained deeper insights into the evolution of Chen Duxiu’s thoughts. In his early writings, Chen was a fervent supporter of Western democratic ideals. However, his disillusionment with Western democracy, especially after the Treaty of Versailles, led him to co-found the Chinese Communist Party and embrace Marxist ideologies. Despite this shift, Chen’s later writings revealed a nuanced perspective. He criticized Stalin’s dictatorship and acknowledged the merits of bourgeois democracy, indicating an ambivalence toward both Western and Communist ideologies. This complexity in Chen’s thought underscores the importance of a refined approach in analyzing historical texts, which current AI models are still striving to fully achieve.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/458/index.html#references",
    "href": "submissions/458/index.html#references",
    "title": "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History",
    "section": "References",
    "text": "References\n\n\nBian, Zhengda, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang You. 2021. “Colossal-AI: A Unified Deep Learning System for Large-Scale Parallel Training.” arXiv Preprint arXiv:2110.14883.\n\n\nDuxiu, Chen. 1914--1940. “Fifteen Articles : 愛國心與自覺心[patriotism and Self-Awareness],吾人最後之覺悟[my Final Awakening], 談政治[talk about Politics], 社会主义批评 [a Critique of Socialism], 马克思学说 [a Study of Marxism], 资产阶级的革命与革命的资产阶级 [Bourgeois Revolution and the Revolution’s Bourgeoisie], 我们的回答 [Our Response], 给戴季陶的一封信 [a Letter to Dai Jitao], 孙中山之民主主义中之民族主义是不是国家主义? [Is the People’s Principle in Sun Yatsen’s Democracy Nationalism?], 世界革命与中国民族解放运动 [World Revolution and the Liberation of Chinese People], 关于中国革命问题致中共中央信 [a Letter to CCP Central on the Question of Chinese Revolution] 谁能救中国？怎样救中国？[who Can Save China? How to Save China?], 我们不要害怕资本主义 [We Do Not Need to Be Afraid of Capitalism], 给西流的信 [a Letter to Xiliu].” CDXZZXB, 1914--1940.\n\n\n———. 1914--1940. “The Six Volume Set of Chen Duxiu Zhuzuo Xuanbian.” Zhuzuo Xuanbian CDXZZXB, 1914--1940.\n\n\nDuxiu, chen. 1940. “给西流的信 [a Letter to Xiliu].” CDXZZXB.\n\n\nDuxiu, Chen. n.d. “Fa Lanxi Ren Yu Jinshi Wenming, 法兰西人与近世文明, the French and Contemporary Civilization.” Zhuzuo Xuanbian CDXZZXB.",
    "crumbs": [
      "Abstracts",
      "Efficacy of Chat GPT Correlations vs. Co-occurrence Networks in Deciphering Chinese History"
    ]
  },
  {
    "objectID": "submissions/469/index.html",
    "href": "submissions/469/index.html",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "",
    "text": "In our presentation, we will explore how Digital Humanities tools can be used to analyse the concept of development from a historiographical perspective. We will begin with a brief introduction to the topic, followed by an overview of our primary sources. The core of our presentation will focus on the methodology, where we will justify our choice of Structural Topic Modelling over other techniques like Hierarchical Clustering on Principal Components. Finally, we will present the results of our analysis and some remarks.\nThe concept of development — and its practical implications — has been controversial since its inception, both in academia and the political arena. Created in the post-WWII period as a universal goal, it soon met opposition, especially in ‘underdeveloped’ countries that had little say in the development policies imposed on them. Consequently, the concept has undergone continuous redefinition.(Sachs 2008)\nFrom the outset, governmental and non-governmental actors have been involved in the development process. Among the non-state actors, philanthropic foundations are particularly significant. However, despite their importance, the way these foundations conceptualize development has received less academic attention than other aspects of their activities. This is true for the Rockefeller Foundation,1 a key player in international public health, (Birn and Fee 2013) global food and agriculture policies, (Smith 2009) the development of various academic disciplines, (Tournès 2007; Fisher 1983, 1999; Schneider and Picard 1999) and the configuration of the international order after WWII. (Tournès 2014)",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/469/index.html#introduction",
    "href": "submissions/469/index.html#introduction",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "",
    "text": "In our presentation, we will explore how Digital Humanities tools can be used to analyse the concept of development from a historiographical perspective. We will begin with a brief introduction to the topic, followed by an overview of our primary sources. The core of our presentation will focus on the methodology, where we will justify our choice of Structural Topic Modelling over other techniques like Hierarchical Clustering on Principal Components. Finally, we will present the results of our analysis and some remarks.\nThe concept of development — and its practical implications — has been controversial since its inception, both in academia and the political arena. Created in the post-WWII period as a universal goal, it soon met opposition, especially in ‘underdeveloped’ countries that had little say in the development policies imposed on them. Consequently, the concept has undergone continuous redefinition.(Sachs 2008)\nFrom the outset, governmental and non-governmental actors have been involved in the development process. Among the non-state actors, philanthropic foundations are particularly significant. However, despite their importance, the way these foundations conceptualize development has received less academic attention than other aspects of their activities. This is true for the Rockefeller Foundation,1 a key player in international public health, (Birn and Fee 2013) global food and agriculture policies, (Smith 2009) the development of various academic disciplines, (Tournès 2007; Fisher 1983, 1999; Schneider and Picard 1999) and the configuration of the international order after WWII. (Tournès 2014)",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/469/index.html#primary-sources",
    "href": "submissions/469/index.html#primary-sources",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "Primary sources",
    "text": "Primary sources\nWe chose as primary sources the Foundation’s Annual Reports for two reasons. The first one is quantitative. The Annual Reports were published annually from 1915 to 2016, with the 1913 and 1914 reports issued jointly in 1915. With this extensive temporal coverage, the Foundation lends itself as an excellent observatory to study the evolution of the concept of development before and after the emergence of this concept.\nThe second reason is qualitative. The main objective of annual reports is to communicate the activities of the Foundation, its financial operations, its priorities, its vision of the issues it faces, and a self-assessment of its own actions in the past and those to be adopted in the future. Although the structure of the annual reports has changed over time, the content has remained stable. The Foundation presents with them a summary of its activities but also presents a narrative that seeks to communicate the reasoning and justification behind the Foundation’s activities. In this sense, the annual reports are a showcase in which the Foundation displays, promotes and justifies its values.\nMoreover, since these reports are public, they serve two functions. The first is purely functional. The reports inform the reader of the Foundation’s activities, its financial state, and other relevant details. The second function is symbolic. As Peter Goldmark Jr. (president of the Foundation from 1988 to 1997) noted, philanthropic foundations lack the three disciplines American life has: the test of the markets, the test of the elections and the press that analyses every move. (Rockefeller Foundation 1998, 3) Therefore, the Foundation uses the annual reports as a form of self-evaluation, as a way to make itself accountable to the public and to offer a promotion and justification of the values that guide its activities. (Rockefeller Foundation 1955, 3)",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/469/index.html#methodology-and-its-twists-and-turns",
    "href": "submissions/469/index.html#methodology-and-its-twists-and-turns",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "Methodology and its twists and turns",
    "text": "Methodology and its twists and turns\nConfronted with the enormous amount of reports to be analysed and inspired by the working paper “Bankspeak” by Moretti and Pestre, (Moretti and Pestre 2015) we undertook a quantitative analysis of the language used in this reports. Then, guided by the results of this analysis we interpreted the activities and institutions in which the Foundation was involved to reconstruct the evolution of its concept of development.\nWe began our quantitative analysis by importing the PDF reports into R using the ‘tidy’ principle (Silge and Robinson 2016, 1) and then performing the necessary text cleaning to reduce the size of the corpus. This increased the efficiency and effectiveness of the analysis.(Gurusamy and Kannan 2014) We then proceeded with the analysis itself.\nInitially, we employed basic text analysis techniques, namely counting the most frequent words per year and per period and using the TF-IDF. These techniques yielded promising results but were insufficient. Although the Foundation had the same objective throughout the period – “to promote the well-being of mankind throughout the world” – ,(Rockefeller Foundation 1915, 7; 1964, 3; 2014, 3) it used different words in absolute and relative terms to describe and justify its activities.\nHowever, in terms of visualisation, precision and displaying temporal dynamics, the capabilities of these two techniques are worse than those of Hierarchical Clustering on Principal Components (HCPC) and Structural Topic Modelling (STM). Moreover, the former techniques are unable to create clusters and topics, unlike the latter two.\nWe continued with the HCPC, using only nouns, as this part of speech is the most suitable for analysing topics.(Suh 2019, 2) This technique confirmed the findings of the absolute frequency analysis and the TF-IDF. That is, there is structure in the use of words by the Foundation, as reflected both in the biplot created by the Correspondence Analysis (CA) necessary to perform the HCPC and in the final clusters. In the biplot in Figure 1, the documents are organised in a temporal manner and, being together with each other, this indicates that they favour and avoid the same words regardless of the number of words in each document.(Bécue-Bertaut 2019, 18–19) Specifically, we observed that the Foundation used more frequently terms such as ‘infection’ or ‘hookworm’ and less frequently terms such as ‘resilience’ or ‘climate’ at the beginning of the period. Furthermore, when clustering after the CA and analysing the words contained in each cluster, it is observed that the Foundation, over time, diversifies the topics in which it engages, following a chronological trend. However, the visualisation of the clusters does not significantly enhance our understanding of the matter.\n\n\n\n\n\n\nFigure 1: Top 25 contributors to the two first dimensions\n\n\n\nDespite offering us greater certainty regarding the temporal structure of the language used, the HCPC does not possess the precision of the next technique we employed: the Structural Topic Modelling with temporal metadata. In a CA with two dimensions, the closer a word, report, or cluster is to the origin of coordinates, the lower its explanatory power, as it represents a smaller percentage of the variance. In our case, there is one cluster almost at the origin of coordinates and two others not far from the central values of one or the other dimension.\nNext, we employed the STM using also only nouns. As a topic modelling technique, the STM seeks to discover latent topics assumed to be generated by the corpus to be analysed, and the researcher must define the number of topics. Since there is no ‘correct’ number of topics for a corpus, we followed Roberts et al.’s methodology.(2014) Thus, we quantitatively measured semantic coherence2 and topic exclusivity3 by standardising these scores and choosing a number of topics that balances them well.\n\n\n\nTable 1: Table with the topical content\n\n\n\n\n\n\nOnce we chose the number of topics, we obtained two lists of nouns associated with each topic, as shown in Table 1. One list groups the nouns most likely to appear in each topic (Highest Prob list), while the other groups those that are frequent and exclusive (FREX list). These lists allow us to discover the central topics without our prior biases. We then named each topic using both lists and analysed the most representative reports for each topic. Therefore, this approach is a mixture of the methods suggested by Roberts et al.(2014, 1068) and Grajzl & Murrell.(2019, 10)\n\n\n\n\n\n\nFigure 2: Topical prevalence of the topics correlated with time\n\n\n\nSubsequently, we calculated the frequency of each topic in our corpus, as shown in Figure 2, and deduced the importance of each topic for the Foundation’s promotion of development.\n\n\n\n\n\n\nFigure 3: Topics and their correlation with time metadata\n\n\n\nFurthermore, by using STM with temporal metadata, we identified which topics the Foundation addressed in its annual reports and their statistical relation to time. This approach enabled us to observe how the frequency of particular topics changed over time. The distribution of these topics over time is illustrated in Figure 3.\nFinally, using the Highest Prob and FREX lists Table 1, and the most prominent reports of each topic, we examined the activities and institutions in which the Foundation was involved to reconstruct the development concept and the ideas underpinning it.",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/469/index.html#results-and-conclusion",
    "href": "submissions/469/index.html#results-and-conclusion",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "Results and conclusion",
    "text": "Results and conclusion\nThis approach provided an innovative way to understand the main topics in which the Foundation was involved in its promotion of development. Despite having the ultimate goal “to promote well-being of mankind throughout the world”, before the coining of the concept of development, the Foundation was already engaged in activities later considered development-related. These activities were strongly influenced by the political, epistemic, and economic context. Thus, we observed how the first layer of meaning of development – health – was gradually joined by economic, social, cultural, and finally, environmental layers.\nHowever, this methodology proved inefficient in analysing the role of the self-help mentality and the market-oriented mentality. To address this, we had to perform a close reading to conclude the centrality of both in the Foundation’s thinking, especially in the 21st century. Indeed, throughout its existence, the Foundation sought to ensure that the actors it helped to develop became autonomous agents who could solve their problems without recourse to third parties. Furthermore, we observed how the importance of these actors in the development process also changed over time. At the beginning of the period, the Foundation conceived of the State as the primary catalyst for development. By the end of the period, it advocated development involving the State, private enterprise, civil society, and individuals. As the State’s credibility as a guarantor of rights and provider of welfare-related services wanes, the Foundation encourages individuals to find their own means to cope with the risks present in contemporary society without waiting for help from the State.\nThis limitation of STM revealed the importance of working hypotheses created through a sound bibliographical review and the hermeneutical work of the historian, despite the use of new methodologies. It was only through the insights gained from the bibliographical review that we anticipated a change in the role of different political actors in the development arena and recognised the significance of the self-help and market-oriented mentality in the Foundation’s development concept. When interpreting the STM results, we found that we could not answer these questions solely with the digital tools. Consequently, we had to conduct a close reading to address these issues, highlighting the critical role of hermeneutical work both in analysing the results of Digital Humanities tools and in the close reading exercise.",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/469/index.html#footnotes",
    "href": "submissions/469/index.html#footnotes",
    "title": "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom now on referred to as the Foundation↩︎\nWords more likely to appear in a topic are more likely to appear together within documents↩︎\nWords more likely to appear in one topic are less likely to appear in another↩︎",
    "crumbs": [
      "Abstracts",
      "Develop Yourself! Development according to the Rockefeller Foundation (1913 – 2013)"
    ]
  },
  {
    "objectID": "submissions/405/index.html",
    "href": "submissions/405/index.html",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "",
    "text": "Data-driven approaches bring extensive opportunities for research to analyze large volumes of data, and gain new knowledge and insights. This is considered especially beneficial for implementation in the humanities and social sciences (Weichselbraun et al. 2021). Application of data-driven research methodologies in the field of history requires a sufficient source base, which should be accurate, transparently shaped and large enough for robust analysis (Braake et al. 2016). Web archives preserve valuable resources that can be drawn upon to analyze the development of the websites and even the whole domains through the decades, and provide access to them (Brügger 2018). At first glance, the volumes of data captured are impressive and suggest the opportunity for big data research practices. For example, the Web Crawls collection of the Internet Archive alone includes 80.2 PB of data (Internet Archive, n.d.c). At the same time, the web-archived collections expose a set of other characteristics relevant to big data and this can be challenging for their efficient use. Such features include, for instance, a high level of velocity, exhaustive in scope and diverse in variety (Kitchin 2014), which require addressing and resolving specific issues. This research focuses on museums’ presence on the web, describes opportunities for implementation of data-driven research, and identifies challenges faced by researchers. In particular, in the paper the opportunity to extract data, to investigate the complexity of structure of the archived websites, and to analyze the content are addressed. At the same time, the findings are relevant to other studies devoted to the use of the archived web in computational research in the humanities.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#introduction",
    "href": "submissions/405/index.html#introduction",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "",
    "text": "Data-driven approaches bring extensive opportunities for research to analyze large volumes of data, and gain new knowledge and insights. This is considered especially beneficial for implementation in the humanities and social sciences (Weichselbraun et al. 2021). Application of data-driven research methodologies in the field of history requires a sufficient source base, which should be accurate, transparently shaped and large enough for robust analysis (Braake et al. 2016). Web archives preserve valuable resources that can be drawn upon to analyze the development of the websites and even the whole domains through the decades, and provide access to them (Brügger 2018). At first glance, the volumes of data captured are impressive and suggest the opportunity for big data research practices. For example, the Web Crawls collection of the Internet Archive alone includes 80.2 PB of data (Internet Archive, n.d.c). At the same time, the web-archived collections expose a set of other characteristics relevant to big data and this can be challenging for their efficient use. Such features include, for instance, a high level of velocity, exhaustive in scope and diverse in variety (Kitchin 2014), which require addressing and resolving specific issues. This research focuses on museums’ presence on the web, describes opportunities for implementation of data-driven research, and identifies challenges faced by researchers. In particular, in the paper the opportunity to extract data, to investigate the complexity of structure of the archived websites, and to analyze the content are addressed. At the same time, the findings are relevant to other studies devoted to the use of the archived web in computational research in the humanities.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#data-driven-research-of-the-museums-web-presence",
    "href": "submissions/405/index.html#data-driven-research-of-the-museums-web-presence",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "Data-Driven Research of the Museums’ Web Presence",
    "text": "Data-Driven Research of the Museums’ Web Presence\nProliferation of digital technologies and the World Wide Web have profoundly impacted museums, transforming their functions and engagement practices. To comprehend these changes, a thorough examination of museum websites and their historical evolution is essential. The focus of this research is on the application of a data-driven approach to the history of the Metropolitan Museum of Art (MET) and the National Museum of Australia (NMA). The cases were selected from two publicly available web archives – the Internet Archive (Internet Archive, n.d.b) and the Australian Web Archive, Trove (Trove, n.d.a) – that are the oldest web archives which means they preserve the historical web from the same starting point, making it possible to compare their infrastructures and their use in research. When working with web archives, the main strategies to apply data-driven methods to research the websites’ history refer to obtaining data from the general pool or using special collections. In the first scenario, scholars obtain data captured during the general crawling process which is not specifically curated. In this case, there is a large chance that the website is not crawled systematically and in terms of the full depth of hierarchy of the pages. The second scenario assumes more rigorous preservation practices that result in a more comprehensive dataset. Analyzing the MET and the NMA, the researcher may use different approaches to obtaining data. Studying the MET’s history on the web, the researcher can search for data from the Internet Archive and also from the special collection devoted to the MET in the Archive-It project (Archive-It, n.d.b). However, the special collection was only initiated in 2019. To study the previous years the researcher needs to necessarily apply to the general pool of the Internet Archive. To investigate the web presence of the NMA scholars may mainly observe the data from the Internet Archive and from Trove as there is no special collection of the preserved web devoted to the NMA. Both web archives offer open APIs to obtain large datasets suitable for data-driven research (Internet Archive, n.d.a; Trove, n.d.b). However, even obtaining data is challenging. The statistics of the Internet Archive show that the preserved version of the MET museum website (The Metropolitan Museum of Art, n.d.b) was saved 20,519 times between November 11, 1996, and July 28, 2023, including 10,867,395 captures of text/HTML files, corresponding to 6,559,761 URLs (Wayback Machine, n.d.a). The statistics for the National Museum of Australia on the Internet Archive show 353,134 captures of text/HTML files, which relates to 175,999 URLs (Wayback Machine, n.d.b). At the same time, the attempt to download all the timestamps using the Wayback Machine Downloader returns only 1,342,067 files for the MET website. The same is relevant for the NMA website obtained from the Internet Archive. The stage of obtaining the dataset requires more attention to the APIs and gaining reliable data. Difficulties related to obtaining data and building datasets have been addressed partially by creating research infrastructures to work with web archived materials. The Internet Archive introduced several initiatives to collect, store and provide access to preserved materials and process data such as Archive-It (Archive-It, n.d.a) and ARCH (Archives Research Compute Hub (Archives Research Compute Hub (ARCH), n.d.)). The GLAM Workbench (GLAM Workbench, n.d.c) has been created to analyze materials from the Australian Web Archive, the Internet Archive and several other web archives. Initially focused on Australia and New Zealand digital platforms the GLAM Workbench suggests a range of solutions based on the use of Jupyter notebooks for exploration and usage of data from GLAM institutions including web archival data. These infrastructures support researchers in finding solutions of various problems in obtaining and processing data, opening them up to wide opportunities to explore the archived web. Regarding the topic of museums on the web, the GLAM Workbench is particularly valuable because some examples in the notebooks have already focused on the Australian web domain and the code from the notebooks can be easily transformed for addressing the topic related to the NMA’s web presence. Using these research infrastructures is beneficial also for solving some technical issues related to the limited capacity of personal computers to address large amount of data (Robertson 2022). Data-driven approaches require not only obtaining information in as complete as possible form but also assessment of the available data, which can be considered as a step in source criticism. Analysis of the distribution of data can be based on a URL analysis of data preserved on the web archives. The URL analysis serves as a necessary step in the source assessment because its study can reveal the temporal distribution of the data, identify the gaps, trace the regularity of crawling and updating the website, specify the distribution of the file formats, and identify other characteristics related to the complexity of the websites and their changes over time. URL-string after crawling becomes a part of the identification of information in the web archive, being enriched with a timestamp. URLs collected from the web archive provide a series of characteristics such as protocol, domain and subdomain names, path, timestamps, and parameters which are all valuable resources of information. Some of these parameters are more important for tracing the technical side of the website’s history. For example, tracing the protocols http and https give us an idea about accepting of security measures and technological upgrades. Some other characteristics provide valuable information to study the content of the website. Identification of the subdomains contributes to our understanding complexity and segmentation of the museum’s website and the presentation of information to the website’s visitors. Often the subdomains have been used for specific projects within the museum’s activities and can be studied separately from the ‘main’ website due to their own structure and content. Both of the considered case studies through their history included the subdomain structures and experimented over the years to find more suitable approaches to the complexity of the website. Subdomains could have a different design and non-overlapping content so that they can be located as a large web structure within the museums’ activities. The analysis of the subdomains facilitates reconstruction of their life cycle, sustainability and use in comparison with the main domain. Building the network of the domain and the subdomains serves as a way to identify the important webpages through the most connected nodes (web pages) and the edges (hyperlinks). In this regard, the main website performs as a metastructure that encompasses various substructures (such as subdomains). Therefore, data-driven approaches are helpful in the analysis of functional segmentation, autonomy and integration processes within such a museum’s web universe. The challenge in the identification of the subdomains refers not only to the inherent incompleteness of preserved data but also to the deficient methods of obtaining data from the web archives. Our experience shows that the API of the Internet Archive deriving the millions of URLs from the same domain returns errors and collapses the processing. Also, we are able to get some subdomains of the metmuseum.org website from the Archive-It platform (The Metropolitan Museum of Art Web Archive, 2024) but the MET preserves the data systematically only from 2019 and for this reason identification of the subdomains from the past perspective can be challenging and researchers need to seek better ways to achieve access this data. The GLAM Workbench has a notebook in relation to obtaining subdomains. The code can also be adjusted to any web domain for searching on the Internet Archive (and some other web archives). Regarding the research of the NMA and subdomains, the GLAM workbench suggests a highly powerful tool to consider the NMA’s website as a part of the gov.au domain (GLAM Workbench, n.d.a). The sub-subdomains of nma.gov.au website can be analyzed around the main museum’s website and at the same time the museum’s website can be considered in connection with other websites from the gov.au domain. Such an approach has a strong potential for discoveries related to positioning of the museums’ website along with the other 1825 third-level domains of the governmental segment, identifying the unique webpages and other characteristics. The archived web is a complex resource that encompasses a large amount of heterogeneous data (Brügger and Finnemann 2013). The single webpage may include various formats of information and analysing the whole website requires finding the appropriate methods. Deducing complexity and investigation data separated according to formats is a widely accepted method of analysis of the website’s content. Textual analysis is a subversion of such research on the websites when only the texts are taken for the study. Building a corpus of texts from the museum websites preserved on the Danish Web Archive gave insights into the development of the Danish museums on the web and the identification of the attributes specific to the museum clusters (Skov and Svarre 2024). Separating the content according to the formats and selecting the particular type of data for the analysis, may appear to be a simple task. However, building a corpus is a very complex task which requires defining appropriate approaches to how to obtain data and what type of data to include in the corpus. There are many pitfalls to consider: the transfer of dynamic web content to the static version on the web archive inescapably changes the nature of the data and requires decisions on how to shape the dataset for analysis (Brügger 2010). Moreover, not all the textual data represent the same level of data. Another issue is the multiplication of data when the same page has been crawled and preserved on the web archive several times. The Internet Archive and the GLAM Workbench suggest different solutions in this regard. The Internet Archive provides users with unique identifiers (‘digest’) of every captured URL. If the content on the same URL has been changed, the hash sum and subsequently the identifier will vary as well. It helps to treat the web pages differently if their content is diverse. At the same time, the significance of the changes cannot be assessed from a distance. The Glam Workbench suggests a code published on the Jupyter Notebook to harvest textual data from the required archived webpages (GLAM Workbench, n.d.b). At the same time, the obtaining of data is possible by the lists of the URLs, which aids in treating the large amount of webpages automatically. Textual data analysis is a well-established sphere of computational humanities. However, the complexity of the website is significantly greater than the text only. Analysis of images can be beneficial for many reasons. One such task is to reveal the selection processes in publishing images on the websites in general and in the digital collections in particular. Art museums had to identify the priorities in publishing pictures and develop specific strategies for that. We do not know much about selection processes, especially in the early years of the web, and how these solutions evolved due to the influences of political and cultural events, movements and actions. Data-driven research is able to identify and highlight these trends. To analyze the currently published collections some museums suggest open APIs for obtaining metadata about the museum objects. Both the MET and the NMA provide open access to their collections through the open API (The Metropolitan Museum of Art, n.d.a; The National Museum of Australia, n.d.). Access to the metadata of the publicly available objects is provided on the digital platforms. At the same time, the metadata is limited to information about the objects and does not include metadata regarding their web presence, including the date of the first publication online. In this regard, the timestamps from the web archives can be considered as a valuable resource to analyze the publishing processes from a historical perspective. At the same time, in the web archive the preserved pictures are disconnected from the metadata about the image and this gap requires finding specific solutions to connect the image and metadata to make the discoveries easier. Apart from the texts and images, the websites incorporate other formats of data and their use in the research is more problematic for analysis. The museums represented on the web multimedia content including videos, animation, conducted podcasts, etc. All of this and other content is valuable for our understanding of their evolution on the web. At the same time, these types of content are very challenging for web archiving (Müller-Budack et al. 2021), so the specific methodologies should be developed for their systematic preservation and then for the subsequent analysis, including data-driven practices.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#conclusion",
    "href": "submissions/405/index.html#conclusion",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "Conclusion",
    "text": "Conclusion\nWeb archives provide wide opportunities for the implementation of data-driven research in the analysis of museums’ web presence. The archived websites require a thorough source criticism and evaluation of the available data for gaining new insights into the evolution of museums’ activities online. Studying the cases of the MET and the NMA is possible via a large amount of data preserved on the Internet Archive and Trove. However, the robust analysis is challenging due to various factors. Researchers need to investigate new ways to obtain the data from the web archives, identify incompleteness and biases, to evaluate data and diversity of the file formats, and to select the best approaches to address them. Analysis of web content remains challenging and requires the development of innovative solutions to exploit data-driven research. At the same time, some of the issues can be gradually resolved based on the developing tools and digital research infrastructures, first of all, ARCH and the GLAM Workbench. Ultimately, data-driven research on the museums’ web presence has a great potential for new discoveries but at the same time, it is a complex endeavor.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#references",
    "href": "submissions/405/index.html#references",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "References",
    "text": "References\n\n\nArchive-It. n.d.a. “Archive-It.” https://www.archive-it.org/.\n\n\n———. n.d.b. “The Metropolitan Museum of Art Web Archive.” https://archive-it.org/collections/13135.\n\n\nArchives Research Compute Hub (ARCH). n.d. “Archives Research Compute Hub (ARCH).” https://github.com/internetarchive/arch.\n\n\nBraake, Serge ter, Antske Fokkens, Niels Ockeloen, and Chantal van Son. 2016. “Digital History: Towards New Methodologies.” In Computational History and Data-Driven Humanities, edited by Bojan Bozic, Gavin Mendel-Gleason, Christophe Debruyne, and Declan O’Sullivan, 23–32. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-46224-0_3.\n\n\nBrügger, Niels. 2010. “Website History: An Analytical Grid.” In Web History, edited by Niels Brügger, 29–59. New York: Peter Lang.\n\n\n———. 2018. The Archived Web: Doing History in the Digital Age. The MIT Press.\n\n\nBrügger, Niels, and Niels Ole Finnemann. 2013. “The Web and Digital Humanities: Theoretical and Methodological Concerns.” Journal of Broadcasting & Electronic Media 57 (1): 66–80. https://doi.org/10.1080/08838151.2012.761699.\n\n\nGLAM Workbench. n.d.a. “Exploring Subdomains in the Whole of Gov.au.” https://glam-workbench.net/web-archives/exploring-govau-subdomains/.\n\n\n———. n.d.b. “Harvesting Collections of Text from Archived Web Pages.” https://glam-workbench.net/web-archives/harvesting-text/.\n\n\n———. n.d.c. “Web Archives, GLAM Workbench.” https://glam-workbench.net/web-archives/.\n\n\nInternet Archive. n.d.a. “API Information.” https://help.archive.org/help/api-information/.\n\n\n———. n.d.b. “Internet Archive.” https://archive.org/.\n\n\n———. n.d.c. “Web Crawls.” https://archive.org/details/web.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm Shifts.” Big Data & Society 1 (1). https://doi.org/10.1177/2053951714528481.\n\n\nMüller-Budack, Eric, Kader Pustu-Iren, Sebastian Diering, Matthias Springstein, and Ralph Ewerth. 2021. “Image Analytics in Web Archives.” In The Past Web, edited by Demidova Gomes D., 203–24. Cham: Springer. https://doi.org/10.1007/978-3-030-63291-5_11.\n\n\nRobertson, Stephen. 2022. “The Properties of Digital History.” History and Theory 61 (4): 86–106. https://doi.org/10.1111/hith.12286.\n\n\nSkov, Mette, and Tanja Svarre. 2024. “A Diachronic Cluster Analysis of Danish Museum Websites.” Internet Histories 8 (1–2): 188–203. https://doi.org/10.1080/24701475.2024.2313292.\n\n\nThe Metropolitan Museum of Art. n.d.a. “Collection API.” https://metmuseum.github.io/.\n\n\n———. n.d.b. “The Metropolitan Museum of Art.” https://www.metmuseum.org/.\n\n\nThe National Museum of Australia. n.d. “Museum API.” https://www.nma.gov.au/about/our-collection/museum-api.\n\n\nTrove. n.d.a. “Trove.” https://trove.nla.gov.au/.\n\n\n———. n.d.b. “Using the API.” https://trove.nla.gov.au/about/create-something/using-api.\n\n\nWayback Machine. n.d.a. “Summary Www.metmuseum.org.” http://web.archive.org/details/https://www.metmuseum.org/.\n\n\n———. n.d.b. “Summary Www.nma.gov.au.” http://web.archive.org/details/https://www.nma.gov.au/.\n\n\nWeichselbraun, Albert, Philipp Kuntschik, Vincenzo Francolino, Mirco Saner, Urs Dahinden, and Vinzenz Wyss. 2021. “Adapting Data-Driven Research to the Fields of Social Sciences and the Humanities.” Future Internet 13 (3): 59. https://doi.org/10.3390/fi13030059.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/468/index.html",
    "href": "submissions/468/index.html",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "",
    "text": "Digital tools like the online portal and the Video Essays in Rural History series of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA) have greatly facilitated the use of films as sources and the publication of audiovisual media as means of communication. This significantly enhances the source base of historical studies of the 20th century and therefore enables scholars to include new perspectives in their research. It furthermore enables researchers to reach new audiences by communicating the results of their studies in audiovisual formats.\nThis presentation will first introduce the relevance of films in rural history and the role that the agricultural sector played in film history. It will then present the research infrastructure of the Archives of Rural History and the European Rural History Film Association. The presentation then concludes with reflections on the use of films as sources and means of communication in historical studies.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#introduction",
    "href": "submissions/468/index.html#introduction",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "",
    "text": "Digital tools like the online portal and the Video Essays in Rural History series of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA) have greatly facilitated the use of films as sources and the publication of audiovisual media as means of communication. This significantly enhances the source base of historical studies of the 20th century and therefore enables scholars to include new perspectives in their research. It furthermore enables researchers to reach new audiences by communicating the results of their studies in audiovisual formats.\nThis presentation will first introduce the relevance of films in rural history and the role that the agricultural sector played in film history. It will then present the research infrastructure of the Archives of Rural History and the European Rural History Film Association. The presentation then concludes with reflections on the use of films as sources and means of communication in historical studies.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#agriculture-in-films-films-in-agriculture",
    "href": "submissions/468/index.html#agriculture-in-films-films-in-agriculture",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Agriculture in Films – Films in Agriculture",
    "text": "Agriculture in Films – Films in Agriculture\nThe agricultural sector was one of the pioneers when it came to producing moving pictures. Film production outside the United States really started after World War I. The films made about rural Europe were used by organisations for educational purposes as well as for advertising products and for teaching the rural population new values and techniques. While in France the government funded a rural cinema campaign in the interwar period, in Switzerland it were mainly the agricultural organisations (often in cooperation with state institutions) which promoted the film as a medium of communication. And women farmers used the new medium to present their work on the farms from their own perspective. A crucial period in the development of the rural film production are the 1960s, when significant changes took place both in the structures and in the actors involved. Up to the 1960’s, agricultural films were almost exclusively so-called commercial or, more precisely, commissioned films. These films were commissioned by state departments, agricultural organisations or scientific institutions for specific purposes – but the films were often used for a variety of purposes. The producers normally were film production companies producing feature or cinema films as well. Indeed, most of them could not have survived from the risky feature-film business alone if they had not had a halfway steady income from their commercial activities, that is: producing commissioned films. Quite often these commissioned films – whether agricultural or otherwise – were shown as supporting films (Vorfilme) immediately before a feature film was shown in the cinema. The practice of broadcasting a commissioned film with an industrial, tourist or agricultural content as a supporting film for a feature film furthermore contributed to a better acceptance of the latter category as a form of art in the feuilleton of “respectable” papers where feature films for a long time in the 20th century were judged as “low-culture”.\nRural films up to the 1960’s can, broadly speaking, be divided into two categories: feature films under the cultural heading and commissioned films produced for industrial, tourist and agricultural clients. Exactly because agricultural films were regarded as part of the economic, not the cultural world, they were not judged as sophisticated enough and culturally valuable enough to be preserved for the future by the existing film archives. This attitude only changed significantly in the 1960/70s, when the so-called author-director films began their remarkable career. Intellectuals influenced by the student movement of the late 1960s began to look at agriculture, especially the peasantry in remote or mountain areas, from new perspectives. They literally produced new pictures, pictures their audience often did not associate with the rural world at all. The author-directors called themselves “documentary” film makers, convinced to “show nothing but the reality”.\nA second element that was crucial for the development and broadening of the independent film makers was the rise and breakthrough of television. TV provided a new outlet for the author- director film. It became, in addition to the state, an important financial support for the filmmakers. And it opened up for them a new, pre-dominantly urban audience that began to be interested in the peasant-mountain world for a variety of reasons.\n\n\n\nFig. 1: Milk transport with a handcart and a horse-drawn cart, shown in a remarkable split screen. Film still from the last of the three Swiss milk films (1923–1929), entitled Wir und die Milch (1929).1",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#the-arherhfa-research-infrastructure",
    "href": "submissions/468/index.html#the-arherhfa-research-infrastructure",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "The ARH/ERHFA research infrastructure",
    "text": "The ARH/ERHFA research infrastructure\nThe knowledge about the history of rural films in Europe is collected in the European film database of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA). The ERHFA was founded in 2017. It is an association of film archives and research institutions interested in films from and about rural areas. The aim of the organisation is to promote the documentation, study and publication of (historical) films related to agricultural history and the history of rural areas. To achieve this goal, the ARH and the ERHFA operate a film database and an associated online portal, publish the Video Essays in Rural History series and organise workshops and panels at academic conferences.\nThe ARH/ERHFA film database currently contains metadata on around 4,300 films, including commissioned, amateur, author’s and feature films as well as television programmes. The status of the metadata collection differs from film to film. Of many films, a copy has been preserved, which, if digitised, is embedded directly in the database. For a number of other films, reference is made to institutions where the film can be viewed. Still other entries contain extensive metadata, without information about the film’s location, because it is not yet known whether a copy has survived or not. Finally, there are also fragmentary entries on films for which very little information is known to date, as well as on films that were planned but never produced. The database is a working tool that, like the online portal, is being continuously expanded as existing entries are complemented and new entries are added.\nThe database is structured according to works, i.e. versions or multiple copies of films are summarised in the entry for the corresponding work. Technical information on the individual copies can be obtained from the linked institutions that archive the films. However, the database not only contains links to digital copies or locations of film reels, but also details of written archival material or literature on the film. The database is thus a signpost pointing to institutions where more information is available.\nAround a quarter of the films listed in the database can be viewed in the online portal. The 27 institutions which contribute to the film database and the online portal come from Austria, Belgium, England, Finland, France, Germany, Ireland, the Netherlands, Portugal and Switzerland.\n\n\n\nFig. 2: The entries in the online portal can be searched using the quick and advanced search functions by search term, period of production, length, commissioner and production company.\n\n\nThe films are grouped according to the contributing institutions as well as thematic and chronological collections. Each collection consists of a short introductory text and a selection of the corresponding films. The chronological collections on the decades from the 1920s to the 1980s provide an overview of the development of film technology in the relevant period. The thematic collections illustrate the diversity of the films.\n\n\n\nFig. 3: Some of the chronological and thematic collections in the ARH/ERHFA online portal.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#films-as-sources",
    "href": "submissions/468/index.html#films-as-sources",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Films as Sources",
    "text": "Films as Sources\nThe accessibility of films via the ARH/ERHFA online portal facilitates the use of film sources in historical studies. As sources, films can be interpreted in at least two ways: firstly, as images of a bygone era that reveal much about the history of agriculture and, secondly, as media that intervened in this history and shaped it. As images, films visualise aspects of agricultural history that are hardly ever recorded in written and statistical sources. This may be because they were either not noticed or concealed, or because they cannot be recorded in writing. What sets the films apart from still images is that they also capture movements and sounds, which make additional contexts of agricultural work tangible, such as the verbal and non-verbal communication between humans and animals at work. Films thus bear witness, often unintentionally, to the fact that farming in practice often was not as it was portrayed or demanded in textbooks and magazines.\nHowever, films are more than mere images; they intervene in the context of their creation and use, create a reality of their own and exert an influence on the viewer.2 This was often used deliberately, for example if there was a need for media control when innovations of a technical, economic, political, social or medical nature had an impact on society or the environment. Changes of all kinds, including the controversies that accompanied them, were therefore an important reason to produce commissioned films. The films had the function of adapting their audiences to new requirements, creating acceptance for the innovation and laying the foundation for further changes. In this respect, commissioned films contributed to the creation of a willingness to cooperate and to consensus-building in modernisation processes.3 In the agricultural context, this function of films was used, for example, by the Eidgenössische Alkoholverwaltung EAV (Swiss Alcohol Board)4 and the plant protection company Dr Rudolf Maag AG, which commissioned and produced numerous films illustrating their activities and the use of their products.5\nThe dual function of audiovisual sources as images and as influencing media often cannot be adequately captured by written texts alone. This is why we conceptualise moving images also for analysing historical developments and communicating insights from historical research.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#films-as-means-of-communication",
    "href": "submissions/468/index.html#films-as-means-of-communication",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Films as Means of Communication",
    "text": "Films as Means of Communication\nAnyone attempting to transfer knowledge gained from audiovisual sources into the written formats will come up against limitations because much of what characterises moving images is lost when written down: the dynamics and (in the case of sound films) the interplay of image and sound in particular. It is, furthermore, often impossible to translate the content of the image into words, for example when it comes to the behaviour of (speechless) animals, human-animal interactions or disappeared (agricultural) practices, for which there is no vocabulary in industrialised societies.6\nTo counter these difficulties, the format of the historical video essay lends itself as a supplement to written texts. A video essay in our series is understood as a montage of historical film and image material that is supplemented by an analytical commentary. The audiovisual sources are both source material and visual carrier of the knowledge transfer and are contextualised and analysed by a commentary. In addition to the communication function, video essays can also be used as an analytical tool.\n\n\n\nFig. 4: The first video essay in the series Video Essays in Rural History focuses on the importance of working horses, cattle, dogs, mules and donkeys in agriculture and in the cities of the 19th and 20th centuries.7\n\n\nThe ARH and ERHFA have launched the Video Essays in Rural History series, in which five video essays from Switzerland, Belgium and Canada have been published to date. They address the importance of working animals, Swiss agronomists and farmers travelling to America in the early 20th century, neighbourly cooperation in rural Canada, the motorisation of Belgian agriculture and Mina Hofstetter, an ecofeminist pioneer of organic agriculture.\nThe video essay is to be understood as a supplement to, not a replacement for, written formats. The video essays published in the Video Essays in Rural History series are therefore published together with an accompanying text. The five to thirty-minute video essays fulfil academic criteria and at the same time appeal to a wider audience. So far, they meet with great interest both within and outside the academic community. They are presented at conferences, used in academic teaching, linked to in media reports and achieve a relatively high number of hits on YouTube (the video essay on working animals was clicked on 3,100 times in the first week after publication, for example).",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#footnotes",
    "href": "submissions/468/index.html#footnotes",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe film is available online in the ARH/ERHFA online portal: ruralfilms.eu (16.08.2024).↩︎\nBernhardt Markus, Visual History: Einführung in den Themenschwerpunkt, in: Zeitschrift für Geschichtsdidaktik, 12/1 (2013), p. 5–8, here: p. 5.↩︎\nZimmermann Yvonne, Dokumentarischer Film: Auftragsfilm und Gebrauchsfilm, in: Zimmermann Yvonne (Hg.), Schaufenster Schweiz: Dokumentarische Gebrauchsfilme 1896-1964, Zürich 2011, p. 34–83, here: p. 64 & 69f.↩︎\nAuderset Juri/Moser Peter, Rausch & Ordnung. Eine illustrierte Geschichte der Alkoholfrage, der schweizerischen Alkoholpolitik und der Eidgenössischen Alkoholverwaltung (1887-2015), Bern 2016; Wigger Andreas, Saft statt Schnaps. Das Filmschaffen der Eidgenössischen Alkoholverwaltung (EAV) von 1930 bis 1985, in: Geschichte im Puls, Dossier 3: Ekstase (2022), www.geschichteimpuls.ch (02.07.2024)↩︎\nPlaylist Eidgenössische Alkoholverwaltung (EAV), in: Archiv für Agrargeschichte, YouTube Playlist (02.07.2024); Playlist Dr. Rudolf Maag AG, in: Archiv für Agrargeschichte, YouTube Playlist (02.07.2024).↩︎\nWigger Andreas, Bewegende Tiere auf bewegten Bildern. Filme als Quellen und Vermittlungsformat zur Geschichte der arbeitenden Tiere in der Zeit der Massenmotorisierung (1950-1980), Videoessay zur Masterarbeit, Fribourg 2023, YouTube (25.06.2024).↩︎\nMoser Peter/Wigger Andreas, Working Animals. Hidden modernisers made visible, in: Video Essays in Rural History, 1 (2022), https://www.ruralfilms.eu/essays/videoessay_1_EN.html [16.08.2024].↩︎",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/459/index.html",
    "href": "submissions/459/index.html",
    "title": "Data Literacy and the Role of Libraries",
    "section": "",
    "text": "More and more, libraries are becoming important institutions when it comes to teaching data literacy and the basics of Digital Humanities (DH) tools and methods, especially to undergraduates or other people new to the subject matter. The Digital Humanities Work Group (AG DH), consisting of a selection of subject librarians from the University Library Basel (UB), have developed various formats to introduce students to these topics and continue to build and expand upon the available teaching elements in order to assemble customised lesson or workshop packages as needed. The aim of this talk is to share our experiences with the planning and teaching of three different course formats. These classes and workshops play, on one hand, an important part of making the library’s (historical) holdings and datasets visible and available for digital research and, on the other hand, they are means to engage with students and (early stage) researchers and imparting skills in the area of working with data at an easily accessible level. As of today, there have been three distinct formats in which the AG DH has introduced students to data literacy and working with digitised historical sources: a full semester course (research seminar) that the AG DH has come up with in collaboration with a professor for Jewish and General History; a 90-minute session on data literacy and working with subject specific datasets within the larger frame of an existing semester course on information, data, and media literacy; and, last but not least, another 90-minute session within a research seminar in literary studies to provide a brief introduction to DH and how it can be incorporated in further research on the seminar topic.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#introduction",
    "href": "submissions/459/index.html#introduction",
    "title": "Data Literacy and the Role of Libraries",
    "section": "",
    "text": "More and more, libraries are becoming important institutions when it comes to teaching data literacy and the basics of Digital Humanities (DH) tools and methods, especially to undergraduates or other people new to the subject matter. The Digital Humanities Work Group (AG DH), consisting of a selection of subject librarians from the University Library Basel (UB), have developed various formats to introduce students to these topics and continue to build and expand upon the available teaching elements in order to assemble customised lesson or workshop packages as needed. The aim of this talk is to share our experiences with the planning and teaching of three different course formats. These classes and workshops play, on one hand, an important part of making the library’s (historical) holdings and datasets visible and available for digital research and, on the other hand, they are means to engage with students and (early stage) researchers and imparting skills in the area of working with data at an easily accessible level. As of today, there have been three distinct formats in which the AG DH has introduced students to data literacy and working with digitised historical sources: a full semester course (research seminar) that the AG DH has come up with in collaboration with a professor for Jewish and General History; a 90-minute session on data literacy and working with subject specific datasets within the larger frame of an existing semester course on information, data, and media literacy; and, last but not least, another 90-minute session within a research seminar in literary studies to provide a brief introduction to DH and how it can be incorporated in further research on the seminar topic.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#research-seminarsemester-course",
    "href": "submissions/459/index.html#research-seminarsemester-course",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Research Seminar/Semester Course",
    "text": "Research Seminar/Semester Course\nTo this end, the AG DH organised a semester course in close collaboration with Prof. Dr. phil. Erik Petry with whom they have created and then co-taught a curriculum introducing various DH tools and methods to be tried out using the UB’s holdings on the topic of the first Zionist Congresses in Basel. The course was attended by MA students from the subjects of History, Jewish Studies and Digital Humanities. This research seminar was designed to provide an introduction to digital methods. We have divided our course into different phases. The first introduction to work organisation, data management and data literacy was followed by sessions that combined the basics of the topic and introductions to digital methods. We focussed on different forms of sources: images, maps and text, with one session being dedicated to each type. This meant we could offer introductions to a broad spectrum of DH tools and methods such as digital storytelling and IIIF, geomapping and working with GIS, and transcription, text analysis and topic modelling. As a transition to the third phase of the project, we organised a session in which we presented various sources either from the University Library or from other institutions – the Basel-Stadt State Archives and the Jewish Museum Switzerland. The overall aim of the course was to enable students to apply their knowledge directly. To this end, they developed small projects in which they researched source material using digital methods and were able to visualise the results of their work. In the third phase of the course, students were given time to work on their own projects. In a block event at the end of the semester, the groups presented their projects and the status of their work. We were able to see for ourselves the students’ exciting approaches and good realisations. The course was also a good experience for us subject librarians. Above all, we benefited from the broad knowledge in our team as well as the opportunity to gain new insights and experiences in select areas of DH. We particularly appreciated the good collaboration with Prof. Dr. Petry, who treated us as equal partners and experts. Despite the positive experience, this format is not sustainable: The effort involved in creating an entire semester course exceeds the resources available to regularly offer similar semester courses. Nevertheless, for this pilot project of the AG DH, the effort was justified because the course allowed us to make our holdings visible and they were researched.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#data-literacy-a-session-within-an-existing-idm-semester-course",
    "href": "submissions/459/index.html#data-literacy-a-session-within-an-existing-idm-semester-course",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Data Literacy – a Session Within an Existing IDM Semester Course",
    "text": "Data Literacy – a Session Within an Existing IDM Semester Course\nFor the second format, the AG DH was approached by the organisers of the regular IDM (“Informations-, Daten- & Medienkompetenz”) semester courses at the University Library Basel. These semester courses are offered for select subject areas to teach students basic information, data and media literacy skills tailored to their subject. The AG DH was asked to come up with two 90-minute sessions to introduce the students to the basics of data literacy. After talking through the requirements with the course lead, the AG DH decided to collaborate with the colleagues from the Open Science Team who would cover the first session dedicated to Research Data Management and a more general introduction to the subject matter. Building on that, the AG DH covers the second session, tailoring it to the requirements of the subject area in question (e.g. art history, sociology, cultural anthropology, economics etc.). Rather than by the whole group, these sessions are mainly prepared and taught by a member of the AG DH whose own subject specialty is closest to (or even the same as) the course’s audience. This means that not all AG DH members are involved in it all the time, therefore being more time and work efficient. Slides are, of course, liberally copied, pasted and reused. This ensures that not everyone has to do all the work while at the same time also guarantees that everyone in the group has access to all the information (which can then be adapted to the subject area). Of course these slides are always edited and brought up to date as to reflect the changes in the field.\nThe goals for the session on subject specific data literacy are intended for the students to…:\n\n…know the relevant sources where to get (research) data and/or corpora for their projects\n…understand the specifics of working with data as pertains to the subject in question\n…assemble subject specific (reused or collected) data sets and how to work with them (i.e. analyse and visualise).\n…introduce them to the people and contacts at the University Library who can help them with their further studies/research.\n\nA big challenge for these sessions is, of course, the sheer extent of working with data. It is impossible to teach every method/tool the students might need for their projects. Particularly in subjects like social anthropology, where almost everything and anything can be seen and collected as data, this session works mainly as a very broad overview of what is possible. The students are given an entry point, links, examples and an understanding of the different kinds of data they might encounter – e.g. texts and linguistic data, statistical data, geodata, image and audi(visual) data – but are required to then work their own way into what they’ll need for their own projects. Because this 90-minute session is only just enough to give a brief introduction and overview of what data is and how you could work with subject specific data, it is important to provide the students with enough links and contact addresses where they can find further assistance, like the subject librarian or the AG DH. However, because the target audience are always students of one specific subject area, it is also easier to tailor the session to that particular subject. (All subject areas may request a semester course from the IDM-team/the organisers.) This format has been a very positive experience in terms of collaboration – not only with the department of the subject but also with the colleagues organising the IDM semester course and the Open Science team.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#introduction-to-dh-for-a-research-seminar-in-english-literary-studies",
    "href": "submissions/459/index.html#introduction-to-dh-for-a-research-seminar-in-english-literary-studies",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Introduction to DH for a Research Seminar in English Literary Studies",
    "text": "Introduction to DH for a Research Seminar in English Literary Studies\nLastly we are also able to prepare bespoke inputs within the framework of a regular class. In this example, the idea for collaboration came about through an informal talk with Prof. Dr. Ina Habermann and her assistant MA Stefanie Heeg from the University of Basel’s English Department, while they were planning a research seminar on early modern travel writing. Since the UB has some of the texts discussed in their collections, I suggested teaching a session at the library where the students may look at the original print books and then talk about and discuss introductory aspects of DH when juxtaposing them with the digitised texts of the same. By using these examples the aim of this 90-minute session was to give the students an introduction to DH, metadata, authority files (in particular the GND) and – drawing on material used for the IDM session on data literacy – showing them possibilities of what they can do with and how to work with these digitised texts. Even though this was within the frame of a class in literary studies, the subject matter is closely related to historical research. While this session was also very dense, content wise, by hosting it at the UB and having the books from the historical holdings ready to be examined in the classroom, it added a nice touch of interactivity to the class. At the same time, preparing and teaching this session fulfils two intentions of the AG DH: first, to strengthen ties with the departments and let the researchers and teaching staff know, that the UB has the competence and people to help with and support with basic DH needs; second, to highlight and showcase our (digitised) collections and holdings, and to familiarise students and researchers with the possibilities of working with them. In addition to that, the UB could present itself as a location combining both the historical dimension with the original texts, as well as a centre for competence in digital methods.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#conclusion",
    "href": "submissions/459/index.html#conclusion",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Conclusion",
    "text": "Conclusion\nThese three different formats highlight some of the chances but also challenges the AG DH faces with regards to their work on with and for students and researchers, and the experiences and feedback from these different formats throw an important light on the role of the UB in the task of teaching skills in this field. Generally it can be said that it needs an active involvement from and by the AG DH to get into the teaching spaces. Either through directly talking with professors/teaching staff and offering to collaborate with them in contributing to their planned classes or by getting involved in existing course formats like the IDM semester courses. It can thus be shown that libraries play a key role in imparting knowledge and skills as well as guardians of cultural property in their function as reliable and long-lasting institutions. We also want to highlight aspects that can still be improved. Above all, this concerns the awareness and attractiveness of such services as well as cooperation with researchers and teachers from all subject areas that work digitally, and history in particular. The questions that drive the AG DH are many and varied: What are the needs of researchers and students? What do you need from your university library? Where do you see the possibility for the library to support and raise awareness with working with historical documents?",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/poster/476/index.html",
    "href": "submissions/poster/476/index.html",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "",
    "text": "This research explores the potential of Large Language Models (LLMs) to automatically generate diagrammatic models that synthesize complex historical narratives. Specifically, we focus on Thomas Piketty’s analysis of economic inequalities in the first half of the 20th century, as presented in his seminal work, Capital in the Twenty-First Century . Our project aims to bridge the gap between theoretical digital history and economic history by employing LLMs to translate Piketty’s causal explanations into visual representations, thus enhancing understanding and facilitating further analysis."
  },
  {
    "objectID": "submissions/poster/476/index.html#introduction",
    "href": "submissions/poster/476/index.html#introduction",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "",
    "text": "This research explores the potential of Large Language Models (LLMs) to automatically generate diagrammatic models that synthesize complex historical narratives. Specifically, we focus on Thomas Piketty’s analysis of economic inequalities in the first half of the 20th century, as presented in his seminal work, Capital in the Twenty-First Century . Our project aims to bridge the gap between theoretical digital history and economic history by employing LLMs to translate Piketty’s causal explanations into visual representations, thus enhancing understanding and facilitating further analysis."
  },
  {
    "objectID": "submissions/poster/476/index.html#models-in-history-from-prediction-to-exploration",
    "href": "submissions/poster/476/index.html#models-in-history-from-prediction-to-exploration",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "Models in history: from prediction to exploration",
    "text": "Models in history: from prediction to exploration\nTraditionally, models in the natural sciences served primarily as predictive tools, used to validate explanations through observed data. However, their role has expanded to encompass exploration, research, and the clarification of thought processes. This shift, acknowledged by the philosophy of science, emphasizes the ability of models to provide new perspectives and challenge existing assumptions. The models’ main function becomes to clarify our thinking processes.\nWith regard to that evolution of modeling, in the digital humanities, it should be noted that there is also a more “introspective” branch of research on models in the humanities, which can be described as the “meta-discipline” of the digital humanities, which attempts to evaluate the epistemological effects of models on research in the humanities, and which “calls for a shift from models as static objects (e.g., what functionalities they enable) to the dynamic process of modeling” . This distinction—between the simple use of models (model-based quantitative operationalization) and epistemological research into the implications of formal modeling—can be mapped onto the division between applied and theoretical digital humanities proposed by Michael Piotrowski ."
  },
  {
    "objectID": "submissions/poster/476/index.html#manual-causal-modeling-and-pikettys-historical-narrative",
    "href": "submissions/poster/476/index.html#manual-causal-modeling-and-pikettys-historical-narrative",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "“Manual” causal modeling and Piketty’s historical narrative",
    "text": "“Manual” causal modeling and Piketty’s historical narrative\nOur research delves into the realm of causal modeling, aiming to elucidate the cause-and-effect relationships within historical narratives. In this context, the “explanandum” (the phenomenon to be explained) is treated as the dependent variable, and the model seeks to identify its potential causes. We began by manually creating a semi-formal qualitative model based on Piketty’s explanation of economic inequalities from 1914 to 1950. Utilizing causal diagrams, as described by Judea Pearl (Pearl 2018), we formalized Piketty’s narrative, making explicit the implicit causal relationships within his analysis. The resulting model serves as a symbolic synthesis of our understanding of Piketty’s core argument: that outside periods of significant economic interventionism, wealth tends to grow faster than economic output, leading to increased inequality (r &gt; g, where r is the rate of return on capital and g is the rate of economic growth). While this trend was mitigated in the first half of the 20th century due to major sociopolitical shocks like the World Wars and the Great Depression, Piketty argues that it has resurfaced since the 1970s and 1980s, a phenomenon he terms the “return of capital.”"
  },
  {
    "objectID": "submissions/poster/476/index.html#llms-and-the-automatic-generation-of-historiographical-diagrams-starting-with-a-small-article",
    "href": "submissions/poster/476/index.html#llms-and-the-automatic-generation-of-historiographical-diagrams-starting-with-a-small-article",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "LLMs and the automatic generation of historiographical diagrams: starting with a small article",
    "text": "LLMs and the automatic generation of historiographical diagrams: starting with a small article\nOur initial exploration will involve using Google’s LLM (Gemini 1.5 Pro) to convert a concise historical article by Piketty into a simplified causal diagram. This article will be A Historical Approach to Property, Inequality and Debt: Reflections on Capital in the 21st Century . Our previous experience with manually constructing a causal model based on Piketty’s work highlighted the potential for automation using LLMs. LLMs have demonstrated remarkable capabilities in various domains, including understanding and generating code, translating languages, and even creating different creative text formats. We believe that LLMs can be trained to analyze historical texts, identify causal relationships, and automatically generate corresponding diagrammatic models. This could significantly enhance our ability to visualize and comprehend complex historical narratives, making implicit connections explicit and facilitating further exploration and analysis. Historiographical theories explore the nature of historical inquiry, focusing on how historians represent and interpret the past. The use of diagrams has been considered as a means to enhance the communication and understanding of these complex theories. Diagrams have been utilized to represent causal narratives in historiography, providing a visual means to support historical understanding and communicate research findings effectively Diagrams have indeed been employed to represent historiographical theories, particularly to illustrate causal narratives and enhance the clarity of historical explanations. On the other hand, Large Language Models (LLMs) have been increasingly integrated into various aspects of coding, from understanding and generating code to assisting in software development and customization. These models leverage vast amounts of data to provide support for a range of programming-related tasks. LLMs are proving to be versatile tools in the realm of coding, capable of understanding, generating, and customizing code across various programming languages and applications. They offer improvements in code-related tasks, user-friendly interactions, and support for low-resource languages. However, challenges such as bias in code generation and the need for human oversight in code review remain. Overall, LLMs are becoming an integral part of the software development process, offering both opportunities and areas for further research and development."
  },
  {
    "objectID": "submissions/poster/476/index.html#benefits-and-implications-of-that-research",
    "href": "submissions/poster/476/index.html#benefits-and-implications-of-that-research",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "Benefits and implications of that research",
    "text": "Benefits and implications of that research\nThe ability to automatically generate historiographical diagrams using LLMs offers several potential benefits:\n\nEnhanced understanding of complex historical narratives: Visual representations can clarify intricate causal relationships and make historical analysis more accessible to a wider audience.\nIdentification of uncertainties and biases: LLMs can be trained to recognize subtle markers of uncertainty and bias within historical texts, encouraging critical engagement with historical interpretations.\nEfficiency and scalability: Automating the process of diagram generation would save time and resources, allowing researchers and teachers to explore a wider range of historical topics and narratives."
  },
  {
    "objectID": "submissions/poster/476/index.html#references",
    "href": "submissions/poster/476/index.html#references",
    "title": "Modeling in history: using LLMs to automatically produce diagrammatic models synthesizing Piketty’s historiographical thesis on economic inequalities",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "submissions/poster/463/index.html",
    "href": "submissions/poster/463/index.html",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "",
    "text": "The significance of Open Research Data (ORD) is rapidly increasing in the research landscape, promoting transparency, reproducibility, and reuse (For more information about ORD in the Swiss higher education system, see swissuniversities 2021a; and swissuniversities 2021b). In historical research, transcriptions are crucial research data, serving as indispensable resources for the interpretation of the past. Despite their immense value, transcriptions have often remained unpublished, difficult to find, and lacked a central platform for access. Therefore, historians frequently had to re-transcribe the same sources. transcriptiones addresses this problem by providing the infrastructure for sharing, editing and reusing transcriptions (Fuchs et al. n.d.c)."
  },
  {
    "objectID": "submissions/poster/463/index.html#background",
    "href": "submissions/poster/463/index.html#background",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "",
    "text": "The significance of Open Research Data (ORD) is rapidly increasing in the research landscape, promoting transparency, reproducibility, and reuse (For more information about ORD in the Swiss higher education system, see swissuniversities 2021a; and swissuniversities 2021b). In historical research, transcriptions are crucial research data, serving as indispensable resources for the interpretation of the past. Despite their immense value, transcriptions have often remained unpublished, difficult to find, and lacked a central platform for access. Therefore, historians frequently had to re-transcribe the same sources. transcriptiones addresses this problem by providing the infrastructure for sharing, editing and reusing transcriptions (Fuchs et al. n.d.c)."
  },
  {
    "objectID": "submissions/poster/463/index.html#project-overview",
    "href": "submissions/poster/463/index.html#project-overview",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "Project Overview",
    "text": "Project Overview\ntranscriptiones is for everyone – researchers, students, and citizen scientists. By contributing their transcriptions, they enhance the visibility and impact of their work. Institutional barriers diminish and collaborations are established. The shared transcriptions are not restricted to a certain period or space. And importantly, contributors are not bound to any digitisation programmes by GLAM institutions but can provide transcriptions of whatever sources they are working on. This leads to the inclusion of diverse sources not typically found on platforms focused on digital copies. In addition, transcriptiones gathers metadata of the transcribed sources, harnessing a rich pool of crowdsourced knowledge. Some of them would otherwise remain uncollected. Overall, transcriptiones enables the reuse of transcriptions and provides valuable insights into sources.\nIn order to build and uphold a diverse community, transcriptiones needs to cater for the needs and skill sets of many different groups. This includes for example balancing a low-threshold and lightweight upload process for those wishing to quickly publish their transcriptions with the provision of comprehensive metadata required by researchers to properly contextualize the transcriptions they obtain from transcriptiones.\nAfter sharing, transcriptions are not intended to remain stagnant. Rather, the community is encouraged to adapt, enhance and therefore reuse the transcriptions. Additionally, users can also revise by adding metadata. The different versions of a transcribed document can be viewed in the document’s version history. Each revised version is assigned a unique, permanent URL that remains unchanged. This ensures that the exact version of a transcription is easily findable and can be accurately cited. By design, the contributed transcriptions vary in state. Sometimes only parts of a source are transcribed, or incomplete raw versions are provided. However, even such partial transcriptions are valuable for transcriptiones as they provide valuable insights into archival collections. Moreover, their quality improves through collaboration, like the principle used by Wikipedia. The open and collaborative nature of transcriptiones, however, requires the users to possess a certain degree of data literacy. Accessing the transcriptions and metadata demands an understanding of what to expect, along with preparedness for potential preprocessing before further use. Contributors on the other hand should not be afraid to publish transcriptions which contain unclear readings or incomplete sections of a source. They can anticipate that other users are cognizant of the potential appearance of transcriptions and might edit or expand them later. This is also in line with the Swiss Data Literacy Charter, according to which, data literacy enables people to act as data producers and data consumers alike (Swiss Academies of Arts and Sciences 2024, 4).\nAnother goal of transcriptiones is building a community of transcribers who interact with one another and enhance the transcriptions together. To facilitate this, several features have been implemented. Users can subscribe to other users, specific institutions, and reference numbers in order to stay up to date with recent developments related to their interests. Additionally, users can contact other contributors directly to exchange information about sources, manuscripts, or scientific findings."
  },
  {
    "objectID": "submissions/poster/463/index.html#towards-fair-transcriptions",
    "href": "submissions/poster/463/index.html#towards-fair-transcriptions",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "Towards FAIR transcriptions",
    "text": "Towards FAIR transcriptions\nFrom the research community’s perspective, findability, and therefore effective search strategies, are essential. For that reason, two distinct ways of navigating the transcriptiones collection have been implemented, each serving specific purposes. The field search allows users to initiate queries at varying levels of detail (Fuchs et al. n.d.b). This interface allows users to locate transcriptions of specific sources. By combining multiple fields, users can refine their searches and discover similar sources from a particular time period, for example. The second strategy is an inventory search, offering access to transcriptions based on archives, signatures, scribes, and different types of sources (Fuchs et al. n.d.a). This approach is similar to an archive plan search, designed to align with a search pattern historians are used to and to transpose this pattern to a platform which spans multiple GLAM institutions. Regarding the FAIR principles, these search strategies are crucial in making transcriptions of handwritten sources findable.\nGiven the increasing importance of digital research methods in history, it is important that data from transcriptiones is not only accessible to humans but also to machines. Therefore, access to transcriptions and metadata is provided through both the web application and a REST-API (Fuchs et al. n.d.d). Via the REST-API, lists and metadata of institutions, reference numbers, source types, scribes and documents can be accessed automatically in the JSON-format. The transcriptions themselves can also be automatically scraped either as plain text or as TEI-XML. Thanks to the API, digital historians can comfortably access the transcriptiones collection the way they need it to conduct quantitative research, to train language models or for any other task that requires automatic access to data and metadata. Furthermore, the API enables interoperability with other stakeholders and ensures that the impact of data reuse extends beyond the platform itself. One example of such a use of the transcriptiones API is the interface between transcriptiones and the Digitaler Lesesaal of the Staatsarchiv Basel-Stadt. Currently in development, this connection will enable direct links to existing transcriptions within the archive catalog.\nThe central aspects of transcriptiones are accessibility, transparency, collaboration, and reuse. While the aforementioned features and strategies of transcriptiones tackle those aspects with regard to the transcriptions and their metadata, the platform also promotes them in the context of code and its development. For this reason, the source code is openly available on Zenodo and GitHub under the very open BSD-3-Clause license (Fuchs et al. 2023a, 2023b)."
  },
  {
    "objectID": "submissions/poster/463/index.html#conclusion",
    "href": "submissions/poster/463/index.html#conclusion",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "Conclusion",
    "text": "Conclusion\ntranscriptiones provides the infrastructure for sharing and editing transcriptions, which it understands as research data. By doing so, it takes this type of data to the age of FAIR and open research data. As an open and collaborative platform that requires metadata during uploads to ensure proper attribution to the source and offers various search strategies, it ensures that transcriptions are findable. Accessibility is guaranteed through the free web application, which allows viewing transcriptions without registration as well as through the various export formats and the API. The latter is also an important cornerstone in providing transcriptions and metadata interoperably. Reusability is achieved through the plethora of metadata and the versioning of edited transcriptions and metadata (For further information about what the FAIR data principles are, see Wilkinson et al. 2016). At the same time, transcriptiones prompts a reconsideration of the perception of transcriptions, encouraging contributors to open up their work to collaboration. All these parts play together towards understanding transcriptions as invaluable research data which is worth gathering, sharing, enhancing and documenting so that many historians can use them for downstream research."
  },
  {
    "objectID": "submissions/poster/463/index.html#references",
    "href": "submissions/poster/463/index.html#references",
    "title": "transcriptiones – Create, Share and Access Transcriptions of Historical Manuscripts",
    "section": "References",
    "text": "References\n\n\nFuchs, Yvonne, Dominic Weber, Sorin Marti, and Nicolas Diener. 2023a. “Transcriptiones.” Zenodo. https://doi.org/10.5281/zenodo.8124784.\n\n\n———. 2023b. “Transcriptiones.” https://github.com/transcriptiones/transcriptiones.\n\n\n———. n.d.a. “Browse Collection.” Transcriptiones. Accessed July 21, 2024. https://transcriptiones.ch/display/.\n\n\n———. n.d.b. “Search.” Transcriptiones. Accessed July 21, 2024. https://transcriptiones.ch/search/.\n\n\n———. n.d.c. “Transcriptiones.” Transcriptiones. Accessed July 22, 2024. https://transcriptiones.ch/.\n\n\n———. n.d.d. “Transcriptiones API.” Transcriptiones. Accessed July 22, 2024. https://transcriptiones.ch/api/documentation/.\n\n\nSwiss Academies of Arts and Sciences. 2024. Swiss Data Literacy Charter. Swiss Academies of Arts and Sciences.\n\n\nswissuniversities. 2021a. Swiss National Open Research Data Strategy.\n\n\n———. 2021b. Swiss National Strategy Open Research Data. Version 1.0. Action Plan.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "submissions/poster/466/index.html",
    "href": "submissions/poster/466/index.html",
    "title": "Economies of Space: Opening up Historical Finding Aids",
    "section": "",
    "text": "In the realm of historical data processing, machine learning has emerged as a game-changer, enabling the analysis of vast archives and complex finding aids on an unprecedented scale. One intriguing case study exemplifying the potential of these techniques is the digitization of the Historical Land Registry of the City of Basel (=Historisches Grundbuch Basel, HGB). The HGB, compiled around the turn of the 20th century, contains a wealth of historical data meticulously collected on index cards. Each card represents a transaction or entry from source documents, and the structured data reflects the conventions and interests of its creators. This inherent complexity has set the stage for a multifaceted exploration, encompassing text recognition, specifically for handwritten materials, and information extraction, particularly event extraction.\nOne of the key accomplishments of this endeavor is the successful application of machine learning algorithms to decipher handwritten content, resulting in a remarkably low character error rate of just 4%. This breakthrough paves the way for extracting valuable information, such as named entities (persons, places, organizations), their relationships, and mentioned events, through specialized language models.\nWhen combined with property information, the extracted data offers a unique opportunity to visualize historical events and transactions on Geographical Information Systems. This process allows for analyzing normative and semantic shifts in the real estate market over time, shedding light on historical changes in language and practice.\nUltimately, this project signifies a milestone in historical data analysis. Machine learning techniques have matured so that even extensive datasets and intricate finding aids can be effectively processed. As a result, innovative approaches to large-scale historical data analysis are now within reach, offering new perspectives on dynamic urban economies during pre-modern times. This venture showcases how technological approaches and humanities deliberations go hand in hand to understand complex patterns in economic history.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@misc{burkart2024,\n  author = {Burkart, Lucas and Hodel, Tobias and Hitz, Benjamin and\n    Vonwiller, Aline and Prada Ziegler, Ismail and Aeby, Jonas and\n    Fuchs, Katrin},\n  editor = {Baudry, Jérôme and Burkart, Lucas and Joyeux-Prunel,\n    Béatrice and Kurmann, Eliane and Mähr, Moritz and Natale, Enrico and\n    Sibille, Christiane and Twente, Moritz},\n  title = {Economies of {Space:} {Opening} up {Historical} {Finding}\n    {Aids}},\n  date = {2024-08-28},\n  url = {https://digihistch24.github.io/book-of-abstracts/submissions/poster/466/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBurkart, Lucas, Tobias Hodel, Benjamin Hitz, Aline Vonwiller, Ismail\nPrada Ziegler, Jonas Aeby, and Katrin Fuchs. 2024. “Economies of\nSpace: Opening up Historical Finding Aids.” Edited by Jérôme\nBaudry, Lucas Burkart, Béatrice Joyeux-Prunel, Eliane Kurmann, Moritz\nMähr, Enrico Natale, Christiane Sibille, and Moritz Twente. Digital\nHistory Switzerland 2024: Book of Abstracts. https://digihistch24.github.io/book-of-abstracts/submissions/poster/466/."
  },
  {
    "objectID": "LICENSE-AGPL.html",
    "href": "LICENSE-AGPL.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users’ Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others’ Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nRemote Network Interaction; Use with the GNU General Public License.\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0",
    "crumbs": [
      "Abstracts",
      "About",
      "License (Code)"
    ]
  }
]