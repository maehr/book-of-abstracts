[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Book of Abstracts",
    "section": "",
    "text": "The DigiHistCH24 conference on “Historical Research, Digital Literacy, and Algorithmic Criticism” brings together scholars and professionals to address the evolving role of digital technologies in historical research. Hosted by the University of Basel on September 12-13, 2024, the conference will focus on the integration of digital tools, the importance of digital literacy, and the critical examination of algorithms within the discipline.\nThis book of abstracts contains all the papers presented at the conference, providing a comprehensive overview of current research at the intersection of history and digital technology. These papers cover a range of topics, from innovative methodologies and software applications to the challenges of digital data management and algorithmic analysis in historical research.\nWe are pleased to present this collection, which reflects the state of the art in digital history, and anticipate that the discussions it stimulates will make a significant contribution to the field.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Session\n        \n         \n          Title\n        \n         \n          Author(s)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nSession\n\n\nTitle\n\n\nAuthor(s)\n\n\n\n\n\n\nSession 1A\n\n\nUsing GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)\n\n\nEliane Schmid\n\n\n\n\nSession 1B\n\n\nTables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.\n\n\nGabi Wuethrich\n\n\n\n\nSession 1B\n\n\nTeaching the use of Automated Text Recognition online. Ad fontes goes ATR\n\n\nLaura Bitterli, Lorenz Dändliker\n\n\n\n\nSession 2A\n\n\nFrom manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)\n\n\nKaspar Gubler\n\n\n\n\nSession 2A\n\n\nData Literacy and the Role of Libraries\n\n\nCatrina Langenegger, Johanna Schüpbach\n\n\n\n\nSession 3A\n\n\nData-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries\n\n\nNadezhda Povroznik\n\n\n\n\nSession 3A\n\n\nGeovistory, a LOD Research Infrastructure for Historical Sciences\n\n\nStephen Hart, Vincent Alamercery, Francesco Beretta, Djamel Ferhod, Sebastian Flick, Tobias Hodel, David Knecht, Gaétan Muck, Alexandre Perraud, Morgane Pica, Pierre Vernus\n\n\n\n\nSession 3B\n\n\nConnecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity\n\n\nChristian Forney, Martin Stuber\n\n\n\n\nSession 4A\n\n\nA handful of pixels of blood\n\n\nAdrian Demleitner\n\n\n\n\nSession 4A\n\n\nFilms as sources and as means of communication for knowledge gained from historical research\n\n\nPeter Moser, Andreas Wigger\n\n\n\n\nSession 4B\n\n\nTowards Computational Historiographical Modeling\n\n\nMichael Piotrowski\n\n\n\n\nSession 4B\n\n\nOn the Historiographic Authority of Machine Learning Systems\n\n\nDominic Weber\n\n\n\n\nSession 5A\n\n\nTraining engineering students through a digital humanities project: Techn’hom Time Machine\n\n\nCyril Lacheze, Marina Gasnier\n\n\n\n\nSession 5A\n\n\nContributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students\n\n\nFrancesco Beretta\n\n\n\n\nSession 6A\n\n\nOn a solid ground. Building software for a 120-year-old research project applying modern engineering practices\n\n\nChristian Sonder, Bastian Politycki\n\n\n\n\nSession 6A\n\n\nWhen the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections\n\n\nVictoria Gioia Désirée Landau\n\n\n\n\nSession 6B\n\n\nFrom Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All\n\n\nMoritz Feichtinger\n\n\n\n\nSession 7B\n\n\nFrom record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700\n\n\nBenjamin Hitz, Ismail Prada Ziegler, Aline Vonwiller\n\n\n\n\n\nNo matching items\n\n Back to topReuseCC BY-SA 4.0",
    "crumbs": [
      "Abstracts"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "",
    "section": "",
    "text": "AboutContributing Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "",
    "section": "Pull Request Process",
    "text": "Pull Request Process\n\nEnsure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Abstracts",
      "About",
      "Contributing"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "",
    "section": "",
    "text": "AboutCode of Conduct Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "",
    "section": "Our Pledge",
    "text": "Our Pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "",
    "section": "Our Standards",
    "text": "Our Standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "",
    "section": "Enforcement Responsibilities",
    "text": "Enforcement Responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at digital-history-2024@unibas.ch. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "",
    "section": "Enforcement Guidelines",
    "text": "Enforcement Guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Abstracts",
      "About",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "fonts/LICENSE-OFL.html",
    "href": "fonts/LICENSE-OFL.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nCopyright 2020 The Jost Project Authors (https://github.com/indestructible-type/Jost)\nThis Font Software is licensed under the SIL Open Font License, Version 1.1. This license is available with a FAQ at: https://scripts.sil.org/OFL\n\nSIL OPEN FONT LICENSE\nVersion 1.1 - 26 February 2007\nPREAMBLE The goals of the Open Font License (OFL) are to stimulate worldwide development of collaborative font projects, to support the font creation efforts of academic and linguistic communities, and to provide a free and open framework in which fonts may be shared and improved in partnership with others.\nThe OFL allows the licensed fonts to be used, studied, modified and redistributed freely as long as they are not sold by themselves. The fonts, including any derivative works, can be bundled, embedded, redistributed and/or sold with any software provided that any reserved names are not used by derivative works. The fonts and derivatives, however, cannot be released under any other type of license. The requirement for fonts to remain under this license does not apply to any document created using the fonts or their derivatives.\nDEFINITIONS “Font Software” refers to the set of files released by the Copyright Holder(s) under this license and clearly marked as such. This may include source files, build scripts and documentation.\n“Original Version” refers to the collection of Font Software components as distributed by the Copyright Holder(s).\n“Modified Version” refers to any derivative made by adding to, deleting, or substituting — in part or in whole — any of the components of the Original Version, by changing formats or by porting the Font Software to a new environment.\n“Author” refers to any designer, engineer, programmer, technical writer or other person who contributed to the Font Software.\nPERMISSION & CONDITIONS Permission is hereby granted, free of charge, to any person obtaining a copy of the Font Software, to use, study, copy, merge, embed, modify, redistribute, and sell modified and unmodified copies of the Font Software, subject to the following conditions:\n\nNeither the Font Software nor any of its individual components, in Original or Modified Versions, may be sold by itself.\nOriginal or Modified Versions of the Font Software may be bundled, redistributed and/or sold with any software, provided that each copy contains the above copyright notice and this license. These can be included either as stand-alone text files, human-readable headers or in the appropriate machine-readable metadata fields within text or binary files as long as those fields can be easily viewed by the user.\nThe name(s) of the Copyright Holder(s) or the Author(s) of the Font Software shall not be used to promote, endorse or advertise any Modified Version, except to acknowledge the contribution(s) of the Copyright Holder(s) and the Author(s) or with their explicit written permission.\nThe Font Software, modified or unmodified, in part or in whole, must be distributed entirely under this license, and must not be distributed under any other license. The requirement for fonts to remain under this license does not apply to any document created using the Font Software.\n\nTERMINATION This license becomes null and void if any of the above conditions are not met.\nDISCLAIMER THE FONT SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, INCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM OTHER DEALINGS IN THE FONT SOFTWARE.\n\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "submissions/428/index.html",
    "href": "submissions/428/index.html",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "",
    "text": "In 2121, nothing is as it once was: a nasty virus is keeping the world on tenterhooks – and people trapped in their own four walls. In the depths of the metaverse, contemporaries are searching for data to compare the frightening death toll of the current killer virus with its predecessors during the Covid-19 pandemic and the «Spanish flu». There is an incredible amount of statistical material on the Covid-19 pandemic in particular, but annoyingly, this is only available in obscure data formats such as .xslx in the internet archives. They can still be opened with the usual text editors, but their structure is terribly confusing and unreadable with the latest statistical tools. If only those digital hillbillies in the 2020s had used a structured format that not only long-outdated machines but also people in the year 2121 could read… Admittedly, very few epidemiologists, statisticians and federal officials are likely to have considered such future scenarios during the pandemic years. Quantitative social sciences and the humanities, including medical and economic history, but also memory institutions such as archives and libraries, should consciously consider how they can sustainably preserve the flood of digital data for future generations. Thus, the sustainable processing and storage of statistical printed data from the time of the First World War makes it possible to gain new insights into the so-called “Spanish flu” e. g. in the city of Zurich even today. The publications by the Statistical Office of the City of Zurich, which were previously only available in “analog” paper format, have been digitized by the Zentralbibliothek (Central Library, ZB) Zurich as part of Joël Floris’ Willy Bretscher Fellowship 2022/2023 (Floris (2023)). This project paper has been written in the context of this digitisation project, as issues regarding digital recording, processing, and storage of historical statistics have always occupied quantitative economic historians “for professional reasons”. The basic idea of this paper is to prepare tables with historical health statistics in a sustainable way so that they can be easily analysed using digital means. The aim was to capture the statistical publications retro-digitized by the ZB semi-automatically with OCR in Excel tables and to prepare them as XML documents according to the guidelines of the Text Encoding Initiative (TEI), a standardized vocabulary for text structures. To do this, it was first necessary to familiarise with TEI and its appropriate modules, and to apply them to a sample table in Excel. To be able to validate the Excel table manually transferred to XML, I then developed a schema based on the vocabularies of XML and TEI. This could then serve as the basis for an automated conversion of the Excel tables into TEI-compliant XML documents. Such clearly structured XML documents should ultimately be relatively easy to convert into formats that can be read into a wide variety of visualisation and statistical tools.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#introduction",
    "href": "submissions/428/index.html#introduction",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "",
    "text": "In 2121, nothing is as it once was: a nasty virus is keeping the world on tenterhooks – and people trapped in their own four walls. In the depths of the metaverse, contemporaries are searching for data to compare the frightening death toll of the current killer virus with its predecessors during the Covid-19 pandemic and the «Spanish flu». There is an incredible amount of statistical material on the Covid-19 pandemic in particular, but annoyingly, this is only available in obscure data formats such as .xslx in the internet archives. They can still be opened with the usual text editors, but their structure is terribly confusing and unreadable with the latest statistical tools. If only those digital hillbillies in the 2020s had used a structured format that not only long-outdated machines but also people in the year 2121 could read… Admittedly, very few epidemiologists, statisticians and federal officials are likely to have considered such future scenarios during the pandemic years. Quantitative social sciences and the humanities, including medical and economic history, but also memory institutions such as archives and libraries, should consciously consider how they can sustainably preserve the flood of digital data for future generations. Thus, the sustainable processing and storage of statistical printed data from the time of the First World War makes it possible to gain new insights into the so-called “Spanish flu” e. g. in the city of Zurich even today. The publications by the Statistical Office of the City of Zurich, which were previously only available in “analog” paper format, have been digitized by the Zentralbibliothek (Central Library, ZB) Zurich as part of Joël Floris’ Willy Bretscher Fellowship 2022/2023 (Floris (2023)). This project paper has been written in the context of this digitisation project, as issues regarding digital recording, processing, and storage of historical statistics have always occupied quantitative economic historians “for professional reasons”. The basic idea of this paper is to prepare tables with historical health statistics in a sustainable way so that they can be easily analysed using digital means. The aim was to capture the statistical publications retro-digitized by the ZB semi-automatically with OCR in Excel tables and to prepare them as XML documents according to the guidelines of the Text Encoding Initiative (TEI), a standardized vocabulary for text structures. To do this, it was first necessary to familiarise with TEI and its appropriate modules, and to apply them to a sample table in Excel. To be able to validate the Excel table manually transferred to XML, I then developed a schema based on the vocabularies of XML and TEI. This could then serve as the basis for an automated conversion of the Excel tables into TEI-compliant XML documents. Such clearly structured XML documents should ultimately be relatively easy to convert into formats that can be read into a wide variety of visualisation and statistical tools.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#data-description",
    "href": "submissions/428/index.html#data-description",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Data description",
    "text": "Data description\nA table from the monthly reports of the Zurich Statistical Office serves as an example data set. The monthly reports were digitised as high-resolution pdfs with underlying Optical Character Recognition (OCR) based on Tesseract by the Central Library’s Digitisation Centre (DigiZ) as part of the Willi Bretscher Fellowship project. They are available on the ZB’s Zurich Open Platform (ZOP, Statistisches Amt der Stadt Zürich (1919)), including detailed metadata information. They were published by the Statistical Office of the City of Zurich as a journal volume under this title between 1908 and 1919, and then as «Quarterly Reports» until 1923. The monthly reports each consist of a 27-page table section with individual footnotes, and conclude with a two-page explanatory section in continuous text. For this study, the data selection is limited to a table for the year 1914 and the month of January (Statistisches Amt der Stadt Zürich (1919)). In connection with Joël Floris’ project, which aims at obtaining quantitative information on Zurich’s demographic development during the «Spanish flu» from the retro-digitisation project, it was obvious to focus on tables with causes of death. The corresponding table number 12 entitled «Die Gestorbenen (in der Wohnbev.) nach Todesursachen und Alter» («The Deceased (in the Resident Pop.) by Cause of Death and Age») can be found on page seven of the monthly report. It contains monthly data on causes of death, broken down by age group and gender, as well as comparative figures for the same month of the previous year. The content of this table is to be prepared below in the form of a standardized XML document with an associated schema that complies with the TEI guidelines.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#methods-for-capturing-historical-tables-in-xml",
    "href": "submissions/428/index.html#methods-for-capturing-historical-tables-in-xml",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Methods for capturing historical tables in XML",
    "text": "Methods for capturing historical tables in XML\nThe source of inspiration for this project paper was a pioneering research project originally based at the University of Basle. In the research project, the annual accounts of the city of Basle from 1535 to 1610 were digitally edited (Calvi (2015)). Technical implementation was carried out by the Center for Information Modeling at the University of Graz. Based on a digital text edition prepared in accordance with the TEI standard, the project manages to combine facsimile, web editing in HTML, and table editing via an RDF (Resource Description Framework ) and XSLT (eXtensible Stylesheet Language Transformations ) in an exemplary manner. The edition thus allows users to compile their own selection of booking data in a “data basket” for subsequent machine-readable analysis. In an accompanying article, project team member Georg Vogeler describes the first-time implementation of a numerical evaluation and how “even extensive holdings can be efficiently edited digitally” (Vogeler 2015). However, as mentioned, the central basis for this is XML processing of the corresponding tabular information based on the TEI standard. This project is based on the April 2022 version (4.4.0) of the TEI guidelines (Burnard (2022)). They include a short chapter on the preparation of tables, formulas, graphics, and music. And even the introduction to Chapter 14 is being rather cautious with regard to TEI application for table formats, warning that layout and presentation details are more important in table formats than in running text, and that they are already covered more comprehensively by other standards and should be prepared accordingly in these notations. On asking the TEI-L mailing list whether it made sense to prepare historical tables with the TEI table module, the answers were rather reserved (https://listserv.brown.edu/cgi-bin/wa?A1=ind2206&L=TEI-L#24). Only the Graz team remained optimistic that TEI could be used to process historical tables, albeit in combination with an RDF including a corresponding ontology. Christopher Pollin also provided github links via TEI to the DEPCHA project, in which they are developing an ontology for annotating transactions in historical account books.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#table-structure-in-tei-xml",
    "href": "submissions/428/index.html#table-structure-in-tei-xml",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Table structure in TEI-XML",
    "text": "Table structure in TEI-XML\nBasically, the TEI schema treats a table as a special text element consisting of line elements, which in turn contain cell elements. This basic structure was used to code Table 12 from 1914, which I transcribed manually as an Excel file. Because exact formatting including precise reproduction of the frame lines is very time-consuming, the frame lines in the project work only served as structural information and are not included as topographical line elements as TEI demands. Long dashes, which correspond to zero values in the source, are interpreted as empty values in the TEI-XML. I used the resulting worksheet as the basis for the TEI-XML annotation, in which I also added some metadata. I then had to create an adapted local schema as well as a TEI header, before structuring the table’s text body. Suitable heading (“head”) elements are the title of the table, the table number as a note and the «date» of the table. The first table row contains the column headings and is assigned the role attribute “label” accordingly. The third-last cell of each row contains the row total, which I have given the attribute “ana” for analysis and the value “#sum” for total, following the example of the Basle Edition. The first cell of each row again names the cause of death and must therefore also be labelled with the role attribute “label”. The second last row shows the sum of the current monthly table, which is why it is given the “#sum” attribute for all respective cells. Finally, the last line shows the total for the previous year’s month. It is therefore not only marked with the sum attribute, but also with a date in the label cell. A potential confounding factor for later calculations is the row “including diarrhea”, which further specifies diseases of the digestive organs but must not be included in the column total. Accordingly, it is provided with another analytical attribute called “#exsum”. As each cell in the code represents a separate element, the »digitally upcycled table 12 in XML format ultimately extends over a good 550 lines of code, which I’m happy to share on request.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#challenges-and-problems",
    "href": "submissions/428/index.html#challenges-and-problems",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Challenges and problems",
    "text": "Challenges and problems\nAn initial problem already arose during the OCR-based digitisation. The Central Library (ZB)’s Tesseract-based OCR software, which specializes in continuous text, simply failed to capture the text in the tables. I therefore first had to transcribe the table by hand, which is error-prone. In principle, however, it is irrelevant in TEI in which format the original text was created. The potential for errors when transferring Excel data into the “original” XML is also high, especially if the table is complex and/or detailed. Ideally, i. e. with a clean OCR table, it ought to be possible to export OCR content in pdfs to XML. When speaking with the ZB’s DigiZ, they confirmed not being happy with OCR quality anymore, and are considering improvement with regard to precision. Due to the extremely short instructions for table preparation in TEI, I underestimated the variety of different text components that TEI offers. The complexity of TEI is not clear from the rough overview of the individual chapters and their introductory descriptions. This only became clear while adjusting table 12 to TEI standards. By becoming accustomed to TEI, its limitations regarding table preparation also became more evident: It is fundamentally geared towards structuring continuous text rather than text forms, where the structure or layout also indicates meaning, as is the case with tables. The conversion of the sample table into XML and the preparation of an associated TEI schema, which is reduced to the elements present in the sample document, yet remains valid with the TEI standard, proved to be time-consuming code work. Thus, both the sample XML and the local schema each comprise over 500 lines of code – and this basically for only a single – though complex – table with a few metadata. In addition, the extremely comprehensive and complex TEI schema on which my XML document is based is not suitable for implementation in Excel. As a result, I had to prepare an XML table schema that was as general as possible, which may be used to convert the Excel tables into XML in the future, thus reducing error potential of the XML conversion.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#ideas-for-project-expansion",
    "href": "submissions/428/index.html#ideas-for-project-expansion",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Ideas for Project Expansion",
    "text": "Ideas for Project Expansion\nBecause, as mentioned, the OCR output of the tables in this case is not usable, it should now be crucial for any digitisation project to achieve high-quality OCR of the retro-digitised tables. Table recognition is definitely an issue in economic history research, and there are several open source development tools around on Git-Repositories, which yet have to set a standard, however. Ideally, the tables recognized in this way would then provide better text structures in the facsimile. With the module for the transcription of original sources, TEI offers extensive possibilities for linking text passages in the transcription with the corresponding passages in the facsimiles. Such links could ideally be used as training data for text recognition programs to improve their performance in the area of table recognition. Other TEI elements that lend structure to the table, such as the dividing lines and the long dashes for the empty values, could also serve as such structural recognition features. Additional important TEI elements such as locations and gender would further increase the content of the TEI XML format. Detailed metadata, as e.g. provided by the retro-digitized version of the ZOP, can be easily integrated into the TEI header area “xenodata”. Finally, in view of the complex structure of the tables, it is essential to understand and implement XSLT (eXtensible Stylesheet Language Transformation) for automated structuring, and as a basis for RDF used e.g. by the Graz team.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/428/index.html#conclusion",
    "href": "submissions/428/index.html#conclusion",
    "title": "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics.",
    "section": "Conclusion",
    "text": "Conclusion\nSo far, tables seem to have had a shadowy existence within the Text Encoding Initiative (TEI) – or, as TEI pioneer Lou Burnard remarked in the TEI mailing list on behalf of my question whether TEI processing of tables made sense: “Tables are tricky”. The main reason for this probably lies in the continuous text orientation of existing tools and users, who are also less interested in numerical formats. In principle, however, preparation according to the TEI standard offers the opportunity to think conceptually about the function of tabularly structured data and to make changes, e.g. in serial sources such as statistical tables, comprehensible. The clearly structured text processing of TEI could provide a basis for improving the still rather poor quality of text recognition programs when recording tables. And a platform-independent, non-proprietary data structure such as XML would be almost indispensable for the sustainable long-term archiving of “digitally born” statistics, which have experienced a boom in recent years, and especially during the pandemic. After all, our descendants should also be able to access historical statistics during the next one.",
    "crumbs": [
      "Abstracts",
      "Tables are tricky. Testing Text Encoding Initiative (TEI) Guidelines for FAIR upcycling of digitised historical statistics."
    ]
  },
  {
    "objectID": "submissions/453/index.html",
    "href": "submissions/453/index.html",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "",
    "text": "Over the past few decades, we have witnessed a major transformation in the digital resources and methodologies available, particularly in the field of Artificial Intelligence (AI), with significant implications for society and the economy. As it is stated in the White Paper The Digital Turn in the Sciences and Humanities by the German Research Foundation’s (DFG), the digital turn is bringing about three major changes in research: former analogue research practices are being realised with digital tools (transformative change); data-intensive technologies allow new research questions to be addressed (enabling change); digital technologies, especially AI methods, can even replace humans in parts of the research project (substitutive change).\nThis phenomenon can also be observed in the human and social sciences (HSS), and even in history, and is particularly striking in the area of open data publication. On the one hand, data can be deposited in well-known, dedicated repositories, such as Zenodo, Nakala, DaSCH or DANS, and a growing number of data journals (e.g. the Journal of Open Humanities Data) publish papers dedicated to contextualising data production in order to facilitate its reuse. On the other hand, directly accessible data are available in the form of relational databases that can be queried (e.g. the PRELIB project) or, using the RDF framework, in the form of Linked Open Data (e.g. the Sphaera project or the Geovistory collaborative platform). We can thus observe that the digital transformation of research practices in HSS (transformative change) is leading to the production and publication of an exponentially growing wealth of information, making it possible to address new research questions (enabling change), in particular by applying AI methodologies in the context of new disciplines known under the label of computational humanities (substitutive change).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#introduction",
    "href": "submissions/453/index.html#introduction",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "",
    "text": "Over the past few decades, we have witnessed a major transformation in the digital resources and methodologies available, particularly in the field of Artificial Intelligence (AI), with significant implications for society and the economy. As it is stated in the White Paper The Digital Turn in the Sciences and Humanities by the German Research Foundation’s (DFG), the digital turn is bringing about three major changes in research: former analogue research practices are being realised with digital tools (transformative change); data-intensive technologies allow new research questions to be addressed (enabling change); digital technologies, especially AI methods, can even replace humans in parts of the research project (substitutive change).\nThis phenomenon can also be observed in the human and social sciences (HSS), and even in history, and is particularly striking in the area of open data publication. On the one hand, data can be deposited in well-known, dedicated repositories, such as Zenodo, Nakala, DaSCH or DANS, and a growing number of data journals (e.g. the Journal of Open Humanities Data) publish papers dedicated to contextualising data production in order to facilitate its reuse. On the other hand, directly accessible data are available in the form of relational databases that can be queried (e.g. the PRELIB project) or, using the RDF framework, in the form of Linked Open Data (e.g. the Sphaera project or the Geovistory collaborative platform). We can thus observe that the digital transformation of research practices in HSS (transformative change) is leading to the production and publication of an exponentially growing wealth of information, making it possible to address new research questions (enabling change), in particular by applying AI methodologies in the context of new disciplines known under the label of computational humanities (substitutive change).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#a-paradigm-shift",
    "href": "submissions/453/index.html#a-paradigm-shift",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "A paradigm shift",
    "text": "A paradigm shift\nThis important transformation of historical research raises the question of a paradigm shift. This concept was used by Thomas Kuhn in 1962 in his book The Structure of Scientific Revolutions (Kuhn 1962) to describe the intellectual structure of disciplines and to analyse the ruptures that lead to scientific revolutions. There are two essential elements to be considered: on the one hand, the paradigm consists of all the shared methods, practices and achievements that form the basis and structure of a disciplinary community; on the other hand, it includes, in its ancient, original sense, the teaching practices applied during education with the aim of enabling the acquisition of the skills essential to the practice of a discipline. Since the purpose of scientific activity is the production of knowledge, the paradigm enables students to learn the methods and rules that are legitimate within a disciplinary community. The digital turn thus raises the question of the transformation of methods and forms of knowledge production in the historical sciences, as can be seen from the publications of a growing number of scholars (e.g. the Journal of Digital History).\nOn the basis of this analysis, it seems essential to introduce training in digital methodologies and tools into the standard disciplinary curriculum of history, and not just in optional Digital Humanities Minors. Since learning disciplinary tools is at the heart of the paradigm of a discipline, digital methodologies should be taught from the beginning of university studies, so that future generations of teachers, doctoral students, professors and researchers can make the transition to the new paradigm from within. This will enable to create a disciplinary community trained in the new methodologies, familiar with the issues from direct experience, and capable of defending the place of the historical sciences in the field of contemporary science and the digital society (Beretta 2023).",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#masters-course-in-digital-methodology-for-historical-research",
    "href": "submissions/453/index.html#masters-course-in-digital-methodology-for-historical-research",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "Master’s course in digital methodology for historical research",
    "text": "Master’s course in digital methodology for historical research\nThese considerations stem not only from my work as a CNRS researcher who has spent the last fifteen years building collaborative information systems for research (symogih.org, ontome.net, geovistory.org)(Beretta 2024), in line with the vision that, as the DFG White Paper points out, “digital infrastructure is essential for research and must be built for long-term service”, but also from ten years of experience in teaching digital methodology at bachelor and master level in history, first at the University of Lyon 3 and for the last four years at the University of Neuchâtel, which currently offers courses in digital methodology in the master’s programmes in Historical Sciences and in Regional Heritage and Digital Humanities.\nBut at this point an essential question arises: what should be taught to history students to help them make the most of the digital transition and build a new paradigm? Looking at recent handbooks, e.g. (Antenhofer, Kühberger, and Strohmeyer 2023; Döring et al. 2022; Schuster and Dunn 2021), or at educational resources like the programminghistorian.org project, we can see a huge variety of approaches and areas of application of digital methods, and often the answer to the question depends on the own field of research and experience. In this sense, I will not provide a somewhat abstract review of the literature, and existing courses, but rather share some aspects of my own approach in the hope that they may be of some use or inspiration to others.\nMy teaching at Master’s level consists of a three-part programme: the first semester deals with understanding the research cycle in history, setting up an information system and discovering the semantic web; the second focuses on learning data analysis and visualisation methods using Python notebooks; the third is about applying the methods to the students’ own research agenda. This teaching programme has two objectives, which correspond to the first two components of the digital transformation mentioned in the DFG White Paper: to learn a methodology suitable for the manual collection of information from sources, according to the best practices of computer science (transformative change); to learn a pool of data analysis and visualisation methodologies, allowing the exploitation of the growing number of existing resources (enabling change). These courses therefore provide students with basic skills, particularly in data analysis, which they can apply directly to their Master’s thesis and, if they wish, continue on to computational research courses such as Machine Learning or Natural Language Processing (substitutive change).\nSince the aim of research is to produce knowledge, an analysis of the research process, conceptualised in terms of a research cycle, forms the basis of my courses. This choice underlines the iterative dimension that is specific to the scientific approach in general and also applies to the formulation and verification (or falsification) of hypotheses that is specific to the social sciences.\n\n\n\n\n\n\nCycle of knowledge production in historical disciplines\n\n\n\n\nFigure 1\n\n\n\nIn this context, knowledge is understood as the result of the analysis and interpretation of information. With regard to information it is at the heart of the scientific process and can be defined as a representation of reality (which is the only datum is the world we observe), and more precisely as an identification and representation of the objects in the world (people, organisations, artefacts, etc.), their characteristics (physical properties of objects, education and income levels of people, opinions, etc.) and their relationships in time and space (membership in organisations, exchange of messages or goods, journeys, etc.). Knowledge can thus be defined as an interpretation of the world represented in the information collected, and if the former is the result of the scientific activity and is generally published in the form of books or articles, the latter should be understood as a most accurate approximation of the facts in words, making the information reusable for new research when shared in the form of digital open data according to the FAIR principles.\nAs the diagram of the knowledge production cycle shows, all research must begin with the definition of a research agenda that fits within the horizon of existing knowledge, expressed in literature, and that defines the methodology that will be adopted and the research questions to be answered. Zotero seems to be the best tool for this task, not only for storing bibliographical references, but also for enriching them with your own notes and categories, and for connecting them to resources on the web, thus realising the first step of a digitally transformed research. On the basis of their line of inquiry, student must then select from the available mass of sources the relevant ones in order to gather the information that will be analysed and serve as a basis for knowledge. They will have to decide what information will be systematically retained and how it will be conceptualised and produced. This raises the issue of the conceptual model and the choice of digital storage technology, because while spreadsheets may be adequate if one is limited to systematically collecting a certain number of characteristics of a population of individuals of the same type, as soon as one wishes to inform about complex relationships between different objects (persons, organisations, artefacts, opinions, economic values, etc.) in space and time, it is essential to use a relational or graph-oriented database in order to capture the full wealth of the required information.\nThis is precisely the content of the teaching of the first semester and I propose to the students to follow the example of the teacher’s own GitHub repository in order to document, in a dedicated GitHub repository and wiki the progress of their research cycle. In other words, I’m adopting a kind of teaching by example, where the whole approach is documented in a sample project available on GitHub that can be imitated and applied to one’s own subject, while endeavouring to go through all the proposed steps by creating one’s own SQLite database, one’s own analyses in Python, etc.\nTo propose the simplest and most concrete use case, I adopt a proposopographical approach and invite students to search Wikipedia for the biographical records of a population that corresponds to their interests, for example political activists or fashion designers, while asking themselves some questions to which they would like to find answers. We then consider the Wikipedia biographical records for this population as sources and define a catalogue of information to be extracted that will lead to the creation of a conceptual model and an initial SQLite database. Students will thus acquire the basic elements for creating a simple, easy-to-manage information system, which will greatly facilitate the manual input of relatively complex information from the sources analysed (transformative change).\nSince it does not make sense to produce a lot of information manually in the context of this course, at this stage I take advantage of the DBPedia and Wikidata projects, which provide a wealth of information on the previously selected populations in the form of structured data published in RDF. Students will therefore learn how to retrieve this information using the SPARQL language and import it into their SQLite database for refinement, thus discovering the process of re-using existing data, which can be considerable in volume with thousands of individuals described and dozens of pieces of information about them (enabling change).\nThis step marks the transition to the second semester, which begins by learning basic skills in Python and using Jupyter notebooks. To be able to analyse the information collected, it must be simplified and coded. It is at this stage that the research questions are introduced and a range of tools are applied to the information collected in the form of digital data: univariate and multivariate statistical analysis, network analysis, spatial representation, etc. Students will discover a new notion of model, now in the statistical sense, that emerges from these analyses and has an eminently heuristic function, since the representations produced by analysis software always require critical discussion, contextualisation and interpretation. At the same time, these methods and digital tools make visible significant phenomena that would otherwise be impossible to see “with the naked eye”, given the considerable volume and complexity of the information collected on the Semantic Web.\nAt the end of the process, students formulate some possible answers to their research questions and document the results obtained in their repository wiki, accompanied by graphics resulting from the analysis. They thus complete the research cycle by producing new knowledge in response to their initial research agenda, publishing online not only the results of their investigations, but also the database, the Python notebooks and the discussion of the analyses that led to their conclusions, thus learning in practice to undertake a reproducible scientific approach. The third semester is devoted to accompanying students who wish to realise their Master’s thesis using the methods learned in the previous semesters. This is still an ongoing process in Neuchâtel, so in my paper I’ll present some results from the master’s theses written by students at Lyon 3 university.",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/453/index.html#results-and-discussion",
    "href": "submissions/453/index.html#results-and-discussion",
    "title": "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master’s Students",
    "section": "Results and discussion",
    "text": "Results and discussion\nI observed in all these years that if the students invest some time in practising the exercises and follow the learning cycle in this kind of apprenticeship by example during the two semesters, they can achieve amazing results (e.g. Militant.e.s pour le droits des femmes and Fashion Designers). But at the same time I have to admit that the learning curve is steep, because in just one year students learn the basics of conceptual modelling, SQL, SPARQL, Python and the essential concepts of various data analysis methods. As well as versioning with GIT and putting data and notebooks online. On the one hand, a certain pedagogical investment is necessary, especially to support students who have less of a natural inclination towards digital technology. On the other hand, the more technical part of this method should be introduced at bachelor level, like GitHub versioning and Python. At the University of Neuchâtel, a brand new minor in Digital Humanities has been introduced in the bachelor’s programme, which will enable students who have taken it to benefit more from the master’s courses.\nAs far as the Master’s thesis is concerned, it seems that the conceptual modelling and the setting up of a database for the input of information extracted from sources are the most useful, while the venture into collecting data available on the web as a basis for the Master’s thesis does not yet seem attractive. However, there are exceptions, as shown by a work using the Refuge Huguenot database, which I will present in my paper. In conclusion, it seems that at the moment students that take this course can only reach the level of transformative change. But experience shows that it is only with the development of appropriate research infrastructure and the emergence of a wider community of digital disciplinary practices that we will be able to provide students with a context that will allow them to achieve the enabling and substitutive changes, and thus bring about an effective paradigm shift. It is up to the new generations to make this happen.",
    "crumbs": [
      "Abstracts",
      "Contributing to a Paradigm Shift in Historical Research by Teaching Digital Methods to Master's Students"
    ]
  },
  {
    "objectID": "submissions/438/index.html",
    "href": "submissions/438/index.html",
    "title": "A handful of pixels of blood",
    "section": "",
    "text": "The 1980s marked the arrival of the home computer. Computing systems became affordable and were marketed to private consumers through state-supported programs and new economic opportunities (Haddon 1988; Williams 1976). Early models, such as the ZX Spectrum1, Texas Instrument TI-99/4A2, or the Atari3, quickly became popular in Europe and opened the door for digital technology to enter the home. This period also marks the advent of homebrew video game culture and newly emerging creative programming practices (Swalwell 2021; Alberts and Oldenziel 2014). As part of this process, these early programmers not only had to figure out how to develop video games but also were among the first to incorporate graphics into video games. This created fertile grounds for a new array of video game genres and helped popularize video games as a mainstream media.\nI’m researching graphics programming for video games from the 1980s and 1990s. The difference to other visual media lies in the amalgamation of computing and the expression of productive or creative intent by video game designers and developers. The specifics of video game graphics are deeply rooted in how human ideas must be translated into instructions that a computer understands. This necessitates a mediation between the computer’s pure logic and a playing person’s phenomenological experience. In other words, the video game image is a specific type of interface that needs to take care of a semiotic layer and offer functional affordances. I am interested in how early video game programmers worked with these interfaces, incorporating their own visual inspirations and attempting to work with the limited resources at hand. Besides critical source code analysis, I also extensively analyze formal aspects of video game images. For the latter, I depend on FAVR to properly describe and annotate images in datasets relevant to my inquiries. The framework explicitly deals with problems of analyzing video game graphics. It guides the annotation of images by their functional, material, and formal aspects and aids in analyzing narrativity and the rhetoric of aesthetic aspects (Arsenault, Côté, and Larochelle 2015).\nThe video game image also differs substantially from the image in animation or other software interfaces, to which they are often compared. Next to its interactivity, it also holds a double function of telling the game as well as offering the affordances to let the player participate. It is between animation and the user interface of the software, with added techno-visual dimensionalities such as resolution or frame rate. The concepts of the “ergodic animage” (Arsenault, Côté, and Larochelle 2015) and “algorithmic images” (Fizek 2022) aptly describe these video games-related aspects. The two terms imply that video game images are of a calculated nature and don’t represent reality but construct one and display the operations of software. Further, these images only work when players participate in them.\nClassical visual analysis is limited in its ability to deal with video game images due to their visual and material diversity, as well as a disciplinary vocabulary that is not a good fit for video game graphics (Arsenault, Côté, and Larochelle 2015). The same is true for the formal analysis of video game images through computer vision models, for example, towards object or image classifications. Neither general-purpose models nor such specialized user interfaces can deal with the visual diversity of video game images and interfaces. FAVR fills this gap by explicitly concentrating on what is displayed on the screen rather than what these images convey. For FAVR, the image on the screen becomes a specific type of interface at the intersection of the game’s rules and mechanics and their visual mediation.\nWhile the framework can identify different game modes and the different functionalities those screens can encompass, more intricate details can escape an analysis. FAVR distinguishes between tangible, intangible, and negative space, as well as agents, in-game and off-game elements, and interfaces. Whereas the aspect of space concerns the overall composition of the screen, the second set of attributes circumscribes the construction of the image. Intangible space, for example, is concerned with information relevant to gameplay but without the direct agency of the player. Examples are life bars or a display of the current score. As another example, off-game denotes decorative background elements. Being of a time-based and interactive nature, some of the relevant information only unfolds as animation or through player interaction. Further, not all visual mediations of the games’ operation are represented as expected in software interfaces or classic visual compositions.\nA simple example could be blood spurting from an agent, which can be any gameplay-relevant character on screen. The blood holds information relevant to the player, indicating that the character on screen got hurt and may prompt a change in play behavior. Whereas a life bar can represent the player’s character health, such indications are usually absent for enemies. Some video games also play with the distinction between in- and off-game planes. In Final Fight (Capcom, 1989, Arcade), our character walks from left to right in a raging city and, on the way, fights numerous enemies entering the screen from left and right. The off-game plane, the background, is composed of run-down houses and alleyways. At one point during the game, those houses’ doors start to open and spawn enemies as well. This mixes up the formerly established convention of what visual information is relevant for gameplay in terms of interactive and decorative elements.\nAnother relevant point regarding FAVR is its limitation to qualitative analysis and manual application. Since I am interested in a larger historical trajectory of video game images in the 1980s and 1990s, I need to leverage digital tools and computational methods to aid my research. I work with two image corpora in my research. A smaller corpus contains 1525 screenshots from 35 video games from Switzerland from 1987-1998. Acquiring a sufficient number of screenshots from old video games from Switzerland is difficult due to their low popularity. The corpus consists of video stills from Let’s Plays4 and screenshots from various video game databases. The second and larger corpus consists of 115’848 screenshots from 4316 video games and is solely sourced through the Mobygames database. Mobygames is one of the largest community-driven platforms for the collection of knowledge on video games. Being maintained mainly by amateurs and video game enthusiasts is not without problems (Pfister et al. 2023). There are open questions on accessing data, searchability, and, most importantly, completeness. Working with Mobygames makes it difficult to assess what will be missing from the dataset. Despite these shortcomings, the work of the community behind such database platforms is of immense value to video game research.\nTo leverage the potential of these corpora, I need to be able to apply FAVR in a formalized and digital method. To that end, I created a linked open ontology that derives and expands on FAVR (Demleitner 2023). It is based on CIDOC and can be applied in Tropy or similar image-annotation tools by providing templates. Other video game-related ontologies were not suitable for the tasks at hand. Most of the better-developed ontologies, such as Video Game Ontology (VGO), Digital Game Ontology (DGO), and Game Metadata and Citation Project (GAMECIP) are concentrating on describing the contents of a game and are mostly abandoned (Martino et al. 2023). Interestingly, both the VGO and VideOWL try to be of benefit to the industry and game developers. In turn, I’m mainly interested in the historic contextualization of video games and the practices of video game development.\nSo far, I was able to formalize the framework’s aspect on game modes and space, as well as create annotation templates that are building on those aspects. These were made for Tropy, a software that enables researchers to organize their visual material, properly describe the images with metadata, and make create annotations. The templates so far allow for the annotation of video game images regarding their overall composition of the screen, spaces as well as game modes. The screenshots provided above demonstrate the annotation of an in-game screenshot of Traps ‘n’ Treasures (Starbyte, 1993, Amiga). Such an annotation allows the comparison with other games, for example regarding the ratio of dynamic versus static image space. Such a ratio was an important factor in video game development, as dynamic image space needed more resources. The annotation can then be exported exportable as JSON5 and used in further analysis and digital methods.\nTo be able to analyze large quantities of video game images towards their functionality as interfaces, digital methods need to be leveraged. Computer vision (CV) models are of limited help regarding this inquiry. CV models are generally trained to extract semantic value, focusing primarily on object classification (Kurfess 2003) or segmentation tasks (Xu 2024). However, what is considered of semantic value typically does not include user interface elements, particularly in the specific context of video game images with their dual functionality. Image similarity cluster visualizations based on embeddings calculated using both classic convolutional neural network6 models like ResNet101 (He et al. 2015) and newer transformer7 models such as DINOv2 (Oquab et al. 2024) have shown [demleitnerThgieComingofageofthevideogameimageInitial2024] that these models are quite capable of recognizing what equals to modes in FAVR, although they lack the ability to properly annotate the images on that level. This limitation is likely due to the visual diversity present in video game images, where narrative elements and the visual mediation of game mechanics coexist on a wide spectrum. Visual Material annotated in Tropy with the FAVR ontology could potentially be used to train or fine-tune new models that are more adept at recognition in this domain.\nThe Framework for the Analysis of Visual Representation in Video Games is a welcomed vantage point for my research inquiry. After translating the framework into a linked open ontology, further work is needed to refine and expand it to encompass more subtle aspects of video game interfaces. Whereas the ontology developed so far works on a formal level, I have yet to research to what extent FAVR can be leveraged to applications of distant viewing of larger video game image corpora. Despite being implemented only in a limited form so far, the FAVR has proven to be a valuable tool in analyzing video game images towards their formal, discursive, and historical aspects.",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/438/index.html#media-list",
    "href": "submissions/438/index.html#media-list",
    "title": "A handful of pixels of blood",
    "section": "Media List",
    "text": "Media List\n\nFig. 1-2: Screenshots from Barbarian (1987) - MobyGames, accessed July 09, 2024\nFig. 3-4: Screenshots of Final Fight (Arcade) Playthrough - NintendoComplete - YouTube, accessed July 09, 2024\nFig. 5-6: Screenshots provided by the author\nBarbarian (Palace Software Inc, 1987, Amiga, DOS)\nFinal Fight (Capcom, 1987, Arcade)\nTraps ‘n’ Treasures (Starbyte, 1993, Amiga)",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/438/index.html#footnotes",
    "href": "submissions/438/index.html#footnotes",
    "title": "A handful of pixels of blood",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZX Spectrum, accessed May 13, 2024↩︎\nTI-99/4A, accessed May 13, 2024↩︎\nAtari 8-bit computers - Wikipedia, accessed July 12, 2024↩︎\nLet’s Play - Wikipedia, accessed July 09, 2024↩︎\nfavr-ontology/examples/ball-raider-1987-main-gameplay.json, accessed July 09, 2024↩︎\nConvolutional neural network - Wikipedia, accessed July 19, 2024↩︎\nTransformer (deep learning architecture) - Wikipedia, accessed July 19, 2024↩︎",
    "crumbs": [
      "Abstracts",
      "A handful of pixels of blood"
    ]
  },
  {
    "objectID": "submissions/445/index.html",
    "href": "submissions/445/index.html",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "",
    "text": "Scholars and interested laypeople who want to adequately deal with historical topics or generally extract information from differently structured historical documents need both knowledge of old scripts and methods for analysing complex layouts. Studies of written artefacts are only possible if they can be read at all – written in unfamiliar scripts such as Gothic Cursive, Humanist Minuscule or German Kurrent and sometimes with rather unconventional layouts. Until now, the relevant skills have been developed, for example, by the highly specialised field of palaeography. In the last few years, a shift in practice has taken place. With digital transcription tools based on deep learning models trained to read these old scripts and accompanying layouts on the rise, working with old documents or unusual layouts is becoming easier and quicker. However, using the corresponding software and platforms can still be intimidating. Users need to have a particular understanding of how to approach working with Automated Text Recognition (ATR) depending on their projects aims. This is why the Ad fontes platform (Ad Fontes 2018) is currently developing an e-learning module that introduces students, researchers, and other interested users (e.g. citizen scientists) to ATR, its use cases, and best practices in general and more specifically into how exactly they can use ATR for their papers and projects.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#introduction",
    "href": "submissions/445/index.html#introduction",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "",
    "text": "Scholars and interested laypeople who want to adequately deal with historical topics or generally extract information from differently structured historical documents need both knowledge of old scripts and methods for analysing complex layouts. Studies of written artefacts are only possible if they can be read at all – written in unfamiliar scripts such as Gothic Cursive, Humanist Minuscule or German Kurrent and sometimes with rather unconventional layouts. Until now, the relevant skills have been developed, for example, by the highly specialised field of palaeography. In the last few years, a shift in practice has taken place. With digital transcription tools based on deep learning models trained to read these old scripts and accompanying layouts on the rise, working with old documents or unusual layouts is becoming easier and quicker. However, using the corresponding software and platforms can still be intimidating. Users need to have a particular understanding of how to approach working with Automated Text Recognition (ATR) depending on their projects aims. This is why the Ad fontes platform (Ad Fontes 2018) is currently developing an e-learning module that introduces students, researchers, and other interested users (e.g. citizen scientists) to ATR, its use cases, and best practices in general and more specifically into how exactly they can use ATR for their papers and projects.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#teaching-atr-online-the-setting",
    "href": "submissions/445/index.html#teaching-atr-online-the-setting",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "Teaching ATR online – the setting",
    "text": "Teaching ATR online – the setting\nDigital methods allow a more comprehensive range of users to assign, analyse and interpret sources digitally. This increased availability of data (mainly in the form of digitized images) also protects the original documents. Recognising handwriting with the help of machine learning, known as ATR, has been greatly improved and is becoming increasingly important in various disciplines. Machine learning methods, especially deep learning, have been used for complex evaluation decisions for a number of years.(Muehlberger et al. 2019) For text recognition, especially for recognising handwriting, ATR can achieve far better results than conventional Optical Character Recognition (OCR). However, many non-standardised fonts and layouts will lead to high error rates in the recognition processes, which is why it is essential to clean up data manually. The reading order of individual lines or blocks of text, in particular, poses major challenges for machine transcription tools.\nEven ATR itself presents several hurdles; the existing tools are often only intuitive to use to a limited extent. Due to the error rates described above, cleaning up the automatically recognized texts by hand is essential. New users must familiarise themselves with these processes, whether this is on their own at home or in a mentored university or non-academic course. In addition, text recognition itself is only part of the learning curve: to work independently with ATR, it is also necessary to recognise when which form of text and layout recognition makes sense, where it is worth investing time to save more time later on and how to proceed with the output. For these reasons, the ongoing DIZH project PATT (Potentials of Advanced Text Technologies: Machine Learning-based Text Recognition)(UZH 2024) at the University of Zurich and the Zurich University of Applied Sciences is currently developing an open-source e-learning module teaching students, young researchers, and the interested public (in the sense of citizen science) how to use ATR.\nDeveloping an exhaustive learning module is a desideratum, as many researchers working in history or linguistics today want to work with automated text and layout recognition. This complex digital skill set involves the critical categorisation of the machine’s feedback. Although the steps involved in manuscript reading are becoming more efficient, they also require new skills: the work no longer centres on direct engagement with handwriting or print, but on the efficient and task-appropriate correction of the results of automated text and layout recognition, often bringing in the original sources later in for a combined distant and close reading.\nThe learning module will be published on Ad fontes. This e-learning tool has been helping researchers to prepare for their work in the archive for around 20 years.(Kränzle and Ritter 2004) It consists of exercises and tutorials that introduce researchers to different types of sources and techniques used to transcribe and analyse them. The platform was completely revised in autumn 2018, meets current standards, and is designed to be interactive to provide a good learning experience.(Hodel and Nadig 2019) The platform has attracted a great deal of attention, both within Switzerland and internationally. Ad fontes’ open access policy (modules are published under a Creative Commons license) ensures that the new module will not be hidden behind a paywall or disappear from the WWW after a short time. The Ad fontes e-learning project is based at the Chair of Medieval History of Prof. Dr. Simon Teuscher.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#when-and-how-to-use-atr-for-a-specific-project",
    "href": "submissions/445/index.html#when-and-how-to-use-atr-for-a-specific-project",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "When and how to use ATR for a specific project",
    "text": "When and how to use ATR for a specific project\nClear target groups and precise learning objectives have been defined for the learning module. As a first step in this direction, we involved experts in the PATT project, defining the following two target groups: First, people with no experience with ATR and the corresponding software and platforms; second, scholars who are already informed about the principles but would like to expand their knowledge systematically and targeted. Our general learning objective is to convey how ATR can be used in a time-efficient, project-specific manner without the recognition process becoming a time waster.\nThe new e-learning module will consist of three parts: (1) What and why ATR?, (2) When is ATR useful? (3) How do I use ATR precisely for my work? The first chapter leads new users into the topic of ATR, OCR and HTR (Handwritten Text Recognition), deep learning, and the most prominent platforms and software. The second chapter draws up various application scenarios a student could find themselves in by showing best practices. This chapter speaks on the research aims as well as the ‘quality’ and quantity of sources. The third chapter is based on these application scenarios and shows signposts for each of the identified applications.\nChapters 2 and 3 will contain the project’s greatest contributions: The aim is to show, from the perspective of historical scholarship, how the prerequisites of the corpora used and the project’s own objective influence how we can use ATR sensibly. From a research perspective, the usefulness of using ATR in this way can be determined primarily by whether it saves time when cataloguing and reading documents. For example, if there are many different manuscripts in a corpus, it is more or less time-efficient, depending on the number of documents, to find a suitable HTR model with a low error rate or to, alternatively, train a model independently. Correction and production of ground truths is a factor to be considered. The amount of text to be transcribed plays a decisive role at this stage: If only a few pages need to be transcribed (depending on the size of the project, “a few” could be 5 or 100), a manual transcription often makes more sense. What constitutes a “good” transcription also varies depending on the research requirements of the corpus: If only individual sections (e.g., certain persons or concepts) in a larger corpus are of interest for the research question, a transcription with a character error rate (CER) of up to 10% may be sufficient to identify those text passages (e.g., with keyword spotting or full text search). Being able to quickly filter out relevant text passages from imperfectly transcribed text volumes promises a relevant expansion of the source base that can potentially be considered, even for smaller research projects. If your own research question is interested in the entire text – a close-reading – a significantly lower CER is necessary to be able to read the text. Both, when it comes to identifying relevant text passages and when analysing smaller text parts for close reading, the reading skills of ATR users are still indispensable, as specific parts of a digitized image need to be consulted. However, also here ATR can be very useful as a first step in a larger process.\nWe use a spider diagram model to communicate the various influences on the benefits of ATR for your own project. This provides a visual representation of various factors that influence work with ATR. The four factors we identified are: (1) the heterogeneity of hands, i.e. the variety of handwriting in the texts; (2) the amount of text; (3) the method, this refers to the types of analysis distant reading and close reading on a sliding scale; (4) the research question; respectively its narrowness or breadth.\n\n\n\nOur model showing identified factors for the use of ATR in historical projects\n\n\nA high heterogeneity of handwriting could affect the accuracy of ATR as the recognition software might have difficulties to recognize the text consistently. A higher degree of heterogeneity requires a broader model based on larger amount of training data.(Hodel et al. 2021) Large amounts of text often mean that ATR needs to be able to work efficiently and scalable to process large amounts of data. For small amounts of text, the focus could be on the level of detail of the recognition or more manual correction. A close reading would require a more precise and detailed analysis of the texts, which means that the ATR models must be accurate and able to recognize fine details, while distant reading focuses more on the recognition of patterns and trends. A broad research question might require a general analysis of many texts, which means that the ATR models should be versatile and robust (e.g. based on TrOCR models)(Li et al. 2022). A specific research question might mean the ATR must focus on specific details and accurate detections. In summary, working with ATR requires a careful balance between the quantity and type of texts, the desired accuracy and detail of the analysis, and the heterogeneity of the manuscripts to be analysed. The diagram helps to organise these factors visually and to understand their interactions.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/445/index.html#our-difficulties",
    "href": "submissions/445/index.html#our-difficulties",
    "title": "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR",
    "section": "Our difficulties",
    "text": "Our difficulties\nWe have already recognised some difficulties for our module and would like to address them briefly: The fast-moving nature of tools and products prevents us from providing precise instructions and means that we can only provide a general introduction to the technology rather than the software(-suites) themselves. Some of this software requires licence or operates on a pay-per-use basis and is therefore not a viable option for everyone. When referring to these products, our open-source teaching module also provides free advertising for paid tools. On the other hand, free tools have disadvantages, which means they are not helpful in all cases. We, therefore, must find a good balance between these two poles. In the technical realisation of our teaching module, we are limited by the options developed during the relaunch. We, therefore, must develop our teaching module within the structures of the existing options. We would furthermore like to set up a FAQ page on ATR, but this requires us to be able to collect and identify these problems and questions systematically.",
    "crumbs": [
      "Abstracts",
      "Teaching the use of Automated Text Recognition online. Ad fontes goes ATR"
    ]
  },
  {
    "objectID": "submissions/464/index.html",
    "href": "submissions/464/index.html",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "",
    "text": "Historical objects from ancient civilizations have made their way from their point of origin to institutions worldwide, the majority over the past two centuries, some long before. Today, they are often housed in knowledge, art and cultural institutions – short: GLAMs (galleries, libraries, archives, museums). When an institution makes its holdings available online, it attracts a varied audience with different interests and backgrounds when approaching objects. Among them are researchers investigating items for their archeological and (art) historical properties, or for the textual documents contained on their surface.\nOne such object type are documents on papyrus, predominantly from modern-day Egypt, but also discovered at sites in countries such as Afghanistan, Jordan, Greece and Italy (famously the Pompeii and Herculaneum papyri). While papyrus collections can display common characteristics and patterns of acquisition (e.g., targeted excavation or purchase in the late 19th and early 20th century, private donations and bequests to Classics departments and national museums in the century since), they can differ vastly in their treatment and prioritization by their holding institutions. From fully interoperable, comprehensive metadata generation and object digitization endeavors to mere mentions of the existence of papyri (edited and unedited) at a given institution in print publications, the prerequisites for accessing and interacting with a set of papyri largely depend on institutional investment in their collection.\nSo even before texts deciphered from ancient heritage objects like papyri can or should be used for innovative computational methods – such as machine learning (Sommerschield et al. 2023), digital paleography and character recognition (Marthot-Santaniello 2021), as well as fragment reassembly and reconstruction (Pirrone, Beurton-Aimar, and Journet 2021) –, we stand to benefit from enhancing existing metadata, and taking a look at the institutions and platforms that provide us with the information needed to engage with historical source material digitally.",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#introduction",
    "href": "submissions/464/index.html#introduction",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "",
    "text": "Historical objects from ancient civilizations have made their way from their point of origin to institutions worldwide, the majority over the past two centuries, some long before. Today, they are often housed in knowledge, art and cultural institutions – short: GLAMs (galleries, libraries, archives, museums). When an institution makes its holdings available online, it attracts a varied audience with different interests and backgrounds when approaching objects. Among them are researchers investigating items for their archeological and (art) historical properties, or for the textual documents contained on their surface.\nOne such object type are documents on papyrus, predominantly from modern-day Egypt, but also discovered at sites in countries such as Afghanistan, Jordan, Greece and Italy (famously the Pompeii and Herculaneum papyri). While papyrus collections can display common characteristics and patterns of acquisition (e.g., targeted excavation or purchase in the late 19th and early 20th century, private donations and bequests to Classics departments and national museums in the century since), they can differ vastly in their treatment and prioritization by their holding institutions. From fully interoperable, comprehensive metadata generation and object digitization endeavors to mere mentions of the existence of papyri (edited and unedited) at a given institution in print publications, the prerequisites for accessing and interacting with a set of papyri largely depend on institutional investment in their collection.\nSo even before texts deciphered from ancient heritage objects like papyri can or should be used for innovative computational methods – such as machine learning (Sommerschield et al. 2023), digital paleography and character recognition (Marthot-Santaniello 2021), as well as fragment reassembly and reconstruction (Pirrone, Beurton-Aimar, and Journet 2021) –, we stand to benefit from enhancing existing metadata, and taking a look at the institutions and platforms that provide us with the information needed to engage with historical source material digitally.",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#state-of-collection-metadata",
    "href": "submissions/464/index.html#state-of-collection-metadata",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "State of Collection Metadata",
    "text": "State of Collection Metadata\nMost publications in papyrology up to the 21st century, and especially editio princeps of texts on papyrus, have been print editions, making it the community’s work and responsibility to keep track of digitized papyrological editions and the texts contained in them. This is where digital tools and resources have been developed early on, from creating born-digital editions to digitizing collection holdings (Ast 2022). A starting point for an overview of these collection locations is a dataset expanded from Trismegistos’ TM Collections database, identifying around 800 papyrus-holding institutions in close to 40 countries, offering a sense of the scale of distribution of this ancient heritage object type.\nFor collections that have been digitized, there are questions and problem areas each institution must address throughout the digitization process and after, many of which can start in the analog and are transferred to the digital. Often, these are challenges connected to historical designations, institutional decision-making and accessibility of the ancient heritage objects.\nIn terms of historical designations, this can include terminology (description, typology and classification of texts, e.g., “private letter”, “amulet”, “writing exercise”) and periodization, both for objects dated to specific years (365 BCE) and to broader timespans (4th c. BCE); even assigning objects to categories like “Ptolemaic” and “Late Roman”, or “Greek world” (a cultural or geographic descriptor) is connected to both terminology and periodization. Institutional decision-making over many years further decides the storage of an object; how it is housed and inventoried is a categorization in itself, and designates which specialist or curator is in charge (classifying an object among, e.g., manuscripts, or in the Antiquities, Classics or Archeology category/department). The papyrus-holding institution (as well as the respective cataloger, curator, data manager) also sets a focus for its collections, anticipating or encouraging audience types, selecting presentation forms and grouping items. Lastly, accessibility encompasses many aspects: how findable an object is (its retrievability and being uniquely identifiable), whether understanding can be generated by proper contextualization being in place (object provenance, acquisition), and if connections and links within and beyond the collection are being offered, such as to aggregators, projects or related institutions (seeing as objects are often fragmented, the pieces having been distributed across collections decades ago). It also includes the information being provided, such as how current metadata is (date last edited), who generated it (metadata editor), and whether it can be displayed in more than one language (through tailor-made translations, or the availability of an automatic translation of the webpage as a whole). Here, with multilingualism, one can again run into the terminology problem, and creating equivalence between languages.\nWith all these elements, uniformity can be difficult to implement, particularly if oftentimes there are analog predecessors (e.g., card catalogs) being transposed to digital metadata when a collection is digitized. Standards are not often agreed upon and correctly implemented, and even it they were, accessibility pertains not only to the objects themselves and whether they can be viewed, but also to the historical – even specifically papyrological – education necessary to engage with them. When terms are not even agreed upon by scholars engaging with them every day, how can a wider audience be expected to understand and interact with either the objects or the scholarship derived from them?",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#approaches-to-onlinedigital-collections",
    "href": "submissions/464/index.html#approaches-to-onlinedigital-collections",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "Approaches to Online/Digital Collections",
    "text": "Approaches to Online/Digital Collections\nWhen the digitization of collections is analyzed, it is usually done by approaching a single institution, by conducting a national survey (of e.g., an institution type, such as all museums, or all libraries), or by using a similar institution- or collection-focused scope; thus, there are not many studies of this kind considering a specific object type across institutions worldwide. However, since this is how researchers tend to approach collections (gathering source material on particular topics, geographic regions or time periods by papyri, ostraca, inscriptions, and other object types), and since with papyrology there is a sub-discipline across several fields (Egyptology, Ancient History, Greek Philology, Latin Philology, Coptology, Arabic Studies, Iranian Studies, and more) devoted to their study, using papyri as a case study object for such research is promising.\nPapyri have enjoyed increased accessibility in the past years, thanks in great part to the dedication and work of individuals generating connections to aggregators, such as Trismegistos (Depauw 2018) and Papyri.info (Berkes 2018). Even Europeana, with its aim of being a digital ecosystem for the European cultural heritage sector, has a growing “Papyrus” category (incl. e.g., Rijksmuseum van Oudheden in Leiden), wherein institutions have made their collections available as entries on Europeana, linking back to the museum’s online catalog via an object permalink. Europeana also allows users to customize the website language to one of 24 options. This is rarely the case for institutional websites: the Musées Royaux d’Art et d’Histoire in Brussels have an entirely trilingual web presence (English, Dutch and French), while there have also been interesting compromises, such as in the case of the Medelhavsmuseet in Stockholm, where metadata categories are partially available in both English and Swedish.\nThe digitization of papyri has not always been linked to available digital metadata, meaning photographs and pertinent, connected information were not uploaded at the same time, or made available in one place. This is unfortunate, since having only the image or only the metadata at hand is rarely enough – one often helps correct the other (in the case of, e.g., inventory number mix-ups) and both offer contextualization to one another. While this has not always been the case, nowadays it is usually done this way: today, the British Library and other larger UK institutions will combine individual digitization requests – made by researchers or projects – for items not yet available on their online catalog with a metadata upload, using the requesting party’s expertise on the item. Technical solutions providing images and metadata together, like making a collection accessible online using IIIF (International Image Interoperability Framework), are gaining traction, typically implemented by the holding institution, since an image server is required to store the digitized assets. Biblioteca Universitaria di Bologna is among the institutions that have recently migrated their digital collections system to one that supports IIIF.\nWhen single institutions have not been willing or able to provide collections with a digital presence, there have been successful attempts by collections at pooling their resources: initiatives like the Papyrus Portal as a platform for primarily German papyrus-holding collections, or collaborations like UC Berkeley’s “Regional Partners” (Badè Museum of Biblical Archaeology, California State University, Stanford University, and Washington State University) for uploading to APIS (Advanced Papyrological Information System), now part of Papyri.info.\nWhile DOIs or similar persistent identifiers for single object entries are rare on institutional websites (e.g., Library of Trinity College Dublin), Trismegistos provides persistent identifiers for a number of categories related to its work, such as TM Texts IDs, TM Archive IDs and even TM Collections IDs. Currently, not all institutions with digitized papyri in their own online collections catalog refer to the connected TM identifier(s), much less provide a hyperlink to the related TM Texts entry. This would have to be remedied with institution-specific outreach initiatives. When it comes to the aforementioned fragmentation of once-whole papyri, identified matches should also be pointed out in online catalogs. This is as of yet still a service rendered by Trismegistos Texts, which must also make decisions regarding how to handle linking or merging related, physically separated texts in their IDs.\nMetadata standards from the field of cultural heritage offer themselves for implementation by GLAMs. There are institutional online collection catalogs utilizing Dublin Core (e.g., Biblioteca Universitaria di Bologna) and permitting DCMI metadata downloads (e.g., Library of Trinity College Dublin), but as with any standard, while some objects are equipped with expressive metadata classes, some rely on the «description» class for full text instead of encoding more specific metadata. This is usually a sign that a papyrus has not yet been optimized for online view for any number of reasons, or that papyri represent only a small part of the institution’s overall collection. ICOM’s CIDOC CRM is also becoming increasingly relevant (Liu, Hindmarch, and Hess 2023), including the extension CRMtex intended for cultural heritage objects with textual content, such as ancient documents, akin to CRMsci and CRMarchaeo for scientific and archeological data respectively (Felicetti and Murano 2017).\nPapyrology has benefitted from many of the thought processes and implementations originally intended for its sister-field of epigraphy. Standards have been developed specifically for ancient texts when it comes to transcription (according to the Leiden system) and encoding, namely EpiDoc for the digital edition of ancient texts in TEI XML (Bodard 2010). The EpiDoc Guidelines also offer recommendations for metadata fields aimed at the description of text-bearing objects, which apart from the description of physical characteristics includes provenance information. It also allows for linking to external controlled vocabularies.\nExisting and developing controlled vocabularies and gazetteers for locations (e.g., Getty, Pleiades, Elliott 2021), chronology (e.g., PeriodO, Golden and Shaw 2016) and terminology (e.g., EAGLE, continued by a Working Group at Epigraphy.info, Mannocci et al. 2014; Liuzzo 2018; and FAIR Epigraphy, Heřmánková, Horster, and Prag 2022) have been massive accomplishments, continuously maintained and ready to be utilized. In the direction of accessibility, multilingualism and openness of collections to a wider audience, inspiration could also be drawn from less complex schemata, but highly informative resources, such as the recent IFLA «Open Access Vocabulary», composed in English, with a translation of the terminology into Spanish, Chinese and Arabic (Bradley and Reilly 2024).",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/464/index.html#conclusion",
    "href": "submissions/464/index.html#conclusion",
    "title": "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper is a work-in-progress of a section of an ongoing dissertation project, presenting and discussing preliminary findings and continued research angles. There are further considerations to be made regarding the funding, available infrastructure and size of institutions, the priority-setting of collections, and institutional guidelines that determine if, why and how a collection is digitized. Marked changes can already be seen in the approach of institutions towards their online collections, in the direction of the tenets of the FAIR principles, making their objects more findable (DOIs, consistently maintained platforms), accessible (digitization, machine-readable metadata), interoperable (standards, nomenclature, vocabularies, IIIF) and even reusable (open licensing of images and metadata). This is also seen in how institutions are becoming increasingly sensitized to a broader audience engaging with their collections, with explicit use of the FAIR, and in some cases the CARE principles, in their online collection presentation and the steps taken to get there (Carroll et al. 2021).",
    "crumbs": [
      "Abstracts",
      "When the Data Becomes Meta: Quality Control for Digitized Ancient Heritage Collections"
    ]
  },
  {
    "objectID": "submissions/459/index.html",
    "href": "submissions/459/index.html",
    "title": "Data Literacy and the Role of Libraries",
    "section": "",
    "text": "More and more, libraries are becoming important institutions when it comes to teaching data literacy and the basics of Digital Humanities (DH) tools and methods, especially to undergraduates or other people new to the subject matter. The Digital Humanities Work Group (AG DH), consisting of a selection of subject librarians from the University Library Basel (UB), have developed various formats to introduce students to these topics and continue to build and expand upon the available teaching elements in order to assemble customised lesson or workshop packages as needed. The aim of this talk is to share our experiences with the planning and teaching of three different course formats. These classes and workshops play, on one hand, an important part of making the library’s (historical) holdings and datasets visible and available for digital research and, on the other hand, they are means to engage with students and (early stage) researchers and imparting skills in the area of working with data at an easily accessible level. As of today, there have been three distinct formats in which the AG DH has introduced students to data literacy and working with digitised historical sources: a full semester course (research seminar) that the AG DH has come up with in collaboration with a professor for Jewish and General History; a 90-minute session on data literacy and working with subject specific datasets within the larger frame of an existing semester course on information, data, and media literacy; and, last but not least, another 90-minute session within a research seminar in literary studies to provide a brief introduction to DH and how it can be incorporated in further research on the seminar topic.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#introduction",
    "href": "submissions/459/index.html#introduction",
    "title": "Data Literacy and the Role of Libraries",
    "section": "",
    "text": "More and more, libraries are becoming important institutions when it comes to teaching data literacy and the basics of Digital Humanities (DH) tools and methods, especially to undergraduates or other people new to the subject matter. The Digital Humanities Work Group (AG DH), consisting of a selection of subject librarians from the University Library Basel (UB), have developed various formats to introduce students to these topics and continue to build and expand upon the available teaching elements in order to assemble customised lesson or workshop packages as needed. The aim of this talk is to share our experiences with the planning and teaching of three different course formats. These classes and workshops play, on one hand, an important part of making the library’s (historical) holdings and datasets visible and available for digital research and, on the other hand, they are means to engage with students and (early stage) researchers and imparting skills in the area of working with data at an easily accessible level. As of today, there have been three distinct formats in which the AG DH has introduced students to data literacy and working with digitised historical sources: a full semester course (research seminar) that the AG DH has come up with in collaboration with a professor for Jewish and General History; a 90-minute session on data literacy and working with subject specific datasets within the larger frame of an existing semester course on information, data, and media literacy; and, last but not least, another 90-minute session within a research seminar in literary studies to provide a brief introduction to DH and how it can be incorporated in further research on the seminar topic.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#research-seminarsemester-course",
    "href": "submissions/459/index.html#research-seminarsemester-course",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Research Seminar/Semester Course",
    "text": "Research Seminar/Semester Course\nTo this end, the AG DH organised a semester course in close collaboration with Prof. Dr. phil. Erik Petry with whom they have created and then co-taught a curriculum introducing various DH tools and methods to be tried out using the UB’s holdings on the topic of the first Zionist Congresses in Basel. The course was attended by MA students from the subjects of History, Jewish Studies and Digital Humanities. This research seminar was designed to provide an introduction to digital methods. We have divided our course into different phases. The first introduction to work organisation, data management and data literacy was followed by sessions that combined the basics of the topic and introductions to digital methods. We focussed on different forms of sources: images, maps and text, with one session being dedicated to each type. This meant we could offer introductions to a broad spectrum of DH tools and methods such as digital storytelling and IIIF, geomapping and working with GIS, and transcription, text analysis and topic modelling. As a transition to the third phase of the project, we organised a session in which we presented various sources either from the University Library or from other institutions – the Basel-Stadt State Archives and the Jewish Museum Switzerland. The overall aim of the course was to enable students to apply their knowledge directly. To this end, they developed small projects in which they researched source material using digital methods and were able to visualise the results of their work. In the third phase of the course, students were given time to work on their own projects. In a block event at the end of the semester, the groups presented their projects and the status of their work. We were able to see for ourselves the students’ exciting approaches and good realisations. The course was also a good experience for us subject librarians. Above all, we benefited from the broad knowledge in our team as well as the opportunity to gain new insights and experiences in select areas of DH. We particularly appreciated the good collaboration with Prof. Dr. Petry, who treated us as equal partners and experts. Despite the positive experience, this format is not sustainable: The effort involved in creating an entire semester course exceeds the resources available to regularly offer similar semester courses. Nevertheless, for this pilot project of the AG DH, the effort was justified because the course allowed us to make our holdings visible and they were researched.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#data-literacy-a-session-within-an-existing-idm-semester-course",
    "href": "submissions/459/index.html#data-literacy-a-session-within-an-existing-idm-semester-course",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Data Literacy – a Session Within an Existing IDM Semester Course",
    "text": "Data Literacy – a Session Within an Existing IDM Semester Course\nFor the second format, the AG DH was approached by the organisers of the regular IDM (“Informations-, Daten- & Medienkompetenz”) semester courses at the University Library Basel. These semester courses are offered for select subject areas to teach students basic information, data and media literacy skills tailored to their subject. The AG DH was asked to come up with two 90-minute sessions to introduce the students to the basics of data literacy. After talking through the requirements with the course lead, the AG DH decided to collaborate with the colleagues from the Open Science Team who would cover the first session dedicated to Research Data Management and a more general introduction to the subject matter. Building on that, the AG DH covers the second session, tailoring it to the requirements of the subject area in question (e.g. art history, sociology, cultural anthropology, economics etc.). Rather than by the whole group, these sessions are mainly prepared and taught by a member of the AG DH whose own subject specialty is closest to (or even the same as) the course’s audience. This means that not all AG DH members are involved in it all the time, therefore being more time and work efficient. Slides are, of course, liberally copied, pasted and reused. This ensures that not everyone has to do all the work while at the same time also guarantees that everyone in the group has access to all the information (which can then be adapted to the subject area). Of course these slides are always edited and brought up to date as to reflect the changes in the field.\nThe goals for the session on subject specific data literacy are intended for the students to…:\n\n…know the relevant sources where to get (research) data and/or corpora for their projects\n…understand the specifics of working with data as pertains to the subject in question\n…assemble subject specific (reused or collected) data sets and how to work with them (i.e. analyse and visualise).\n…introduce them to the people and contacts at the University Library who can help them with their further studies/research.\n\nA big challenge for these sessions is, of course, the sheer extent of working with data. It is impossible to teach every method/tool the students might need for their projects. Particularly in subjects like social anthropology, where almost everything and anything can be seen and collected as data, this session works mainly as a very broad overview of what is possible. The students are given an entry point, links, examples and an understanding of the different kinds of data they might encounter – e.g. texts and linguistic data, statistical data, geodata, image and audi(visual) data – but are required to then work their own way into what they’ll need for their own projects. Because this 90-minute session is only just enough to give a brief introduction and overview of what data is and how you could work with subject specific data, it is important to provide the students with enough links and contact addresses where they can find further assistance, like the subject librarian or the AG DH. However, because the target audience are always students of one specific subject area, it is also easier to tailor the session to that particular subject. (All subject areas may request a semester course from the IDM-team/the organisers.) This format has been a very positive experience in terms of collaboration – not only with the department of the subject but also with the colleagues organising the IDM semester course and the Open Science team.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#introduction-to-dh-for-a-research-seminar-in-english-literary-studies",
    "href": "submissions/459/index.html#introduction-to-dh-for-a-research-seminar-in-english-literary-studies",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Introduction to DH for a Research Seminar in English Literary Studies",
    "text": "Introduction to DH for a Research Seminar in English Literary Studies\nLastly we are also able to prepare bespoke inputs within the framework of a regular class. In this example, the idea for collaboration came about through an informal talk with Prof. Dr. Ina Habermann and her assistant MA Stefanie Heeg from the University of Basel’s English Department, while they were planning a research seminar on early modern travel writing. Since the UB has some of the texts discussed in their collections, I suggested teaching a session at the library where the students may look at the original print books and then talk about and discuss introductory aspects of DH when juxtaposing them with the digitised texts of the same. By using these examples the aim of this 90-minute session was to give the students an introduction to DH, metadata, authority files (in particular the GND) and – drawing on material used for the IDM session on data literacy – showing them possibilities of what they can do with and how to work with these digitised texts. Even though this was within the frame of a class in literary studies, the subject matter is closely related to historical research. While this session was also very dense, content wise, by hosting it at the UB and having the books from the historical holdings ready to be examined in the classroom, it added a nice touch of interactivity to the class. At the same time, preparing and teaching this session fulfils two intentions of the AG DH: first, to strengthen ties with the departments and let the researchers and teaching staff know, that the UB has the competence and people to help with and support with basic DH needs; second, to highlight and showcase our (digitised) collections and holdings, and to familiarise students and researchers with the possibilities of working with them. In addition to that, the UB could present itself as a location combining both the historical dimension with the original texts, as well as a centre for competence in digital methods.",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/459/index.html#conclusion",
    "href": "submissions/459/index.html#conclusion",
    "title": "Data Literacy and the Role of Libraries",
    "section": "Conclusion",
    "text": "Conclusion\nThese three different formats highlight some of the chances but also challenges the AG DH faces with regards to their work on with and for students and researchers, and the experiences and feedback from these different formats throw an important light on the role of the UB in the task of teaching skills in this field. Generally it can be said that it needs an active involvement from and by the AG DH to get into the teaching spaces. Either through directly talking with professors/teaching staff and offering to collaborate with them in contributing to their planned classes or by getting involved in existing course formats like the IDM semester courses. It can thus be shown that libraries play a key role in imparting knowledge and skills as well as guardians of cultural property in their function as reliable and long-lasting institutions. We also want to highlight aspects that can still be improved. Above all, this concerns the awareness and attractiveness of such services as well as cooperation with researchers and teachers from all subject areas that work digitally, and history in particular. The questions that drive the AG DH are many and varied: What are the needs of researchers and students? What do you need from your university library? Where do you see the possibility for the library to support and raise awareness with working with historical documents?",
    "crumbs": [
      "Abstracts",
      "Data Literacy and the Role of Libraries"
    ]
  },
  {
    "objectID": "submissions/474/index.html",
    "href": "submissions/474/index.html",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "",
    "text": "We live in the information age, in the information society, respectively. Scholars might disagree on when exactly it began and what its defining characteristics are, but they all agree that the flow and power of information are of unprecedented importance in our world (Castells and Castells 2000; Cortada 2002; Beniger 1997). The advent of digital information and communication technology has accelerated the pace and increased the amount of information humanity is producing, processing, and transmitting. Thus, to navigate through our world made up of information, it has become imperative to develop a sort of “digital literacy,” commonly defined as the ability to acquire information, assess its quality, and apply it to a given problem (Lankshear and Knobel 2008; Reedy and Parker 2018; Carmi and Yates 2020). This essay argues that historians can contribute significantly to the formulation of a canon in digital literacy, because their training and epistemic traditions are based on evaluating the authenticity, credibility, perspective, and context of sources. However, this paper will emphasize that a foundational understanding of the functional principles of digital information processing and basic approaches of digital forensics must be incorporated into the historian’s toolbox. It will demonstrate that the history of computing offers a path to acquire this knowledge and to disseminate it. To conclude, the paper will point out that the technological, social, political, cultural, and economic context and embeddedness of information, its production, and circulation, are fundamental for interpretation and understanding, highlighting again the favorable position for historians to play a significant part in providing orientation and critique in the information age and contributing to general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#introduction",
    "href": "submissions/474/index.html#introduction",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "",
    "text": "We live in the information age, in the information society, respectively. Scholars might disagree on when exactly it began and what its defining characteristics are, but they all agree that the flow and power of information are of unprecedented importance in our world (Castells and Castells 2000; Cortada 2002; Beniger 1997). The advent of digital information and communication technology has accelerated the pace and increased the amount of information humanity is producing, processing, and transmitting. Thus, to navigate through our world made up of information, it has become imperative to develop a sort of “digital literacy,” commonly defined as the ability to acquire information, assess its quality, and apply it to a given problem (Lankshear and Knobel 2008; Reedy and Parker 2018; Carmi and Yates 2020). This essay argues that historians can contribute significantly to the formulation of a canon in digital literacy, because their training and epistemic traditions are based on evaluating the authenticity, credibility, perspective, and context of sources. However, this paper will emphasize that a foundational understanding of the functional principles of digital information processing and basic approaches of digital forensics must be incorporated into the historian’s toolbox. It will demonstrate that the history of computing offers a path to acquire this knowledge and to disseminate it. To conclude, the paper will point out that the technological, social, political, cultural, and economic context and embeddedness of information, its production, and circulation, are fundamental for interpretation and understanding, highlighting again the favorable position for historians to play a significant part in providing orientation and critique in the information age and contributing to general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#forensic-source-criticism",
    "href": "submissions/474/index.html#forensic-source-criticism",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "Forensic Source Criticism",
    "text": "Forensic Source Criticism\nThe need for historians and archivists to engage with the methods of computer forensics is well established among those who work with digitally born objects (Ries 2022; Duranti and Endicott-Popovsky 2010; Fickers 2020). Increasingly, digitized sources and re-born digitals are retrieved via the internet and incorporated into scholarship. Therefore, “digital basics” become indispensable for evaluating the credibility and meaning of a given source (Milligan 2019, 241). Learning to deal with digital objects and their specific qualities, elements, and characteristics is as indispensable for historians of the information age as learning the scriptures and languages of the past for historians of the pre-modern period. As Trevor Owens and Thomas Padilla stated: “In much the same way that a historian who studies eighteenth-century documents needs to learn to read various kinds of handwriting scripts to develop an ability to read and decipher those texts, historians are going to need to develop sophisticated understandings of how digital media systems functioned at particular points in time and how different kinds of users used them” (Owens and Padilla 2021, 12). Arguing for the importance of forensic methods for historical inquiry and source criticism, Thorsten Ries has stated: “If historians are to critically appraise primary sources and establish the circumstances of their creation, provenance, processing history, so as to facilitate the identification of forgeries, fakes and disinformation, it is essential to explore the forensic history of the material creation of these records” (Ries 2022, 184).\nThe most basic and essential skills of forensic source criticism for historians working on born-digital objects include overcoming “screen essentialism,” the ability to retrieve and interpret metadata, knowledge of encoding formats and their meaning, and the ability to read and understand code. Overcoming “screen essentialism” (Owens 2018, 46) means acknowledging that the interfaces through which we interact with computing devices should be understood as performances. Interfaces themselves are complex interactions between various programs, routines, hardware, and software, not only the displays, screens, input-output devices, and peripherals allowing us to control the operations of the computer. They are designed to free the user’s mind from thinking about all of these underlying functions, enabling her to focus on her specific task. The downside of this comfortable arrangement is that the user loses sight of many preconfigured decisions on how to process, render, and display data. “Screen essentialism” refers to the tendency to take “what you see for what there is.” Overcoming it means understanding that the visual impression we get on a given system is just one of many possible others. Think of resolution and colors on a basic level, think of the difference between a text-editor on the one hand and an integrated development environment on the other. Being able to distinguish between the “performative” elements of the display and the core properties of a digital object is an essential part of digital literacy. The properties characterizing a digital object are often stored in its metadata.\n“Metadata is our friend,” James Baker wrote (Baker 2019). Indeed, metadata, i.e., descriptive data about digital objects attached to them by the system that produced the object or data, can contain valuable information for answering some of the most important questions of source criticism: date of production, authorship, size, format, and some more or less useful properties. Knowing where to find metadata of files and objects and checking whether or not they are consistent with the content of the data is thus a basic and pivotal first step in evaluating born-digital sources. An example would be to check if the metadata in a text file about the time of creation and last modification is in line with what it claims to report and from what perspective. However, to substantiate such evaluations based on metadata, historians also need to know how metadata can be misleading or manipulated. The date-time-stamp attached to each file, for example, is automatically generated by the operating system. This again is dependent on the configuration of the system time and the time zone in the system’s settings and can be changed (Baker 2019).\nFurther important insights into the characteristics of any given digital object can be derived from some understanding of the principles of encoding and formats. Basically, all digital objects are comprised of two distinct bits (1 and 0), but there is an infinite number of ways to encode information based on the binary representation of signals. Text or letters, respectively, can be encoded in different ways: there is the Morse/alphabet, for example, which uses only short and long signals; there are modern and widely used encoding schemes for letters like ASCII or UTF/8, which supports also non-western characters (Pargman and Palme 2009). The same goes for numbers, which can be represented in binary, hexadecimal, or any nested encoding. Images can be represented in various ways, depending on the way the distribution of black, white, or colored pixels in a grid are encoded (Dourish 2017). To understand and critically read the encoding of any digital object is to acknowledge and scrutinize the choices of the significant properties and their representation determined by the respective encoding format.\nFinally, an integral part of digital literacy is a basic understanding of algorithms and code. Here again, the history of computing serves as an introduction and explanation at the same time. In his book “Computer Power and Human Reason” from 1976, computing pioneer Joseph Weizenbaum provides a simple explanation of the principle of a Turing machine, demonstrating that there is no functional difference between data and processing instruction, because they are equally codified in binaries and stored in the same memory. Almost in passing, he introduces the concept of giving human-readable and easy-to-memorize names for the instruction, like “STORE,” “GET,” etc., thereby conveying Assembler language to his readers (Weizenbaum 1976). With these concepts in mind, it is simple to understand that even different higher programming languages employ a similar set of basic concepts and instructions, such as functions, values, arguments, loops, conditional statements, and so on. These basics, which are available in countless introductory chapters and tutorials all over the internet, are sufficient to follow the arguments made by proponents of critical code studies on single lines of code or longer programs (Marino 2020; Krajewski 2020; Jaton and Bowker 2020; Montfort 2014).\nDigital literacy and source criticism of born-digital objects employing basic concepts of forensics, therefore, aims to understand the logical and functional location within and relations to its environment and operating system, because such objects can neither exist nor can they be understood outside of these relationships. How can these insights be made productive for source criticism, i.e., for evaluating the integrity and authenticity of a born-digital object? One such application is scrutinizing a given file’s integrity by comparing different versions of it. Most systems automatically produce backup copies of each file and also store temporary versions while the file is in use. These previous and alternative versions are often either invisible in the contents of a given directory as displayed by the common file managers of personal computers. In addition, such files are often stored within an application’s directory instead of the directory the user is working on (Kirschenbaum 2012). If a copy or a previous version of a file can be located, assumptions about its coherence and originality are possible. Even without the ability to open or read a file, comparing its size and the one of the previous version can be revealing. If a copy and the original of a given file are truly identical, it can be verified by comparing the hash-sum of both files. There are numerous open-source tools and instructions to do that available also to novices (Altheide and Carvey 2011; Hosmer and Kessler 2014).\nAnother approach to source criticism inspired by digital forensics is to read between the lines, or “between the bits,” more precisely. Thorsten Ries, for example, has demonstrated that text files can contain much more than they might reveal at first sight, i.e., overcoming the perspective of screen essentialism (Ries 2022, 176–78). In his examples, he reads the Revision Identifier for Style Definition (RSID) automatically attached to each MS Word file to make statements about a file’s creation and revision history. Similarly, Trevor Owens has demonstrated that important information about a file’s history can be retrieved simply by changing its file-type extension and thus opening it with different software (Owens 2018, 43). In Owens’ example, he opens an .mp3 music file with a simple text editor by changing the extension to .txt, which enables him literally to “read” all of the file’s metadata. Depending on the scheme employed by the file managing system, this reading “against the grain” might reveal information that is not accessible with a simple right-click. Similarly, it might be worth a try to open a file of unknown format with the vim editor for a first inspection.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#system-and-environment-contextualizing-digital-objects",
    "href": "submissions/474/index.html#system-and-environment-contextualizing-digital-objects",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "System and Environment: Contextualizing digital objects",
    "text": "System and Environment: Contextualizing digital objects\nAll elements of the expanded and updated version of source criticism outlined above point to an increased attention towards the systems and environments into which the production and processing of digital objects are embedded. On a basic level, computation always relies on specific logical, material, and technical systems and environments, i.e., the operating system, hardware, storage media, exchange formats, transmission protocols, etc. Inspired by platform studies and the “new materialism,” recent research on digital objects has emphasized the platform character of all digital media and objects and argued for their understanding as “assemblages” (Owens 2018; Zuanni 2021). This line of research emphasizes the multiple relations and dependencies of all digital objects to systems and environments. All data has to be organized according to certain file formats and standards to be transferable and processable; file formats require specific applications to be read and manipulated; applications and programs, in turn, rely on operating systems, which again are bound to specific hardware configurations and must be maintained and updated, and so on. With networked computing, web-based applications, and cloud storage, complex and nested platforms and assemblages have become the norm. Consequently, any concept of digital literacy or data literacy must incorporate critical reflection on the relations, dependencies, and determinations of systems and infrastructure (Gray, Gerlitz, and Bounegru 2018). This is in line with recent research in science and technology studies and the materialistic turn in the history of computing, which center on connectivity and reliance on large and complex infrastructure networks in their studies (Parks and Starosielski 2015; Galloway and Thacker 2014; Edwards et al. 2009). Here again, following the historical unfolding and development of these infrastructures helps to understand both their general functionality and their specifics, which are sometimes more the result of traditions and path dependencies than of technical necessity.\nIn the same way that historians trace back the provenance, perspective, and implicit presuppositions of a “classic” paper-based source, they must reflect on the system-environment of a digital object, its relations to it, its location within it, and the epistemic consequences of that positionality and relations.\nTheorizing characteristics of born-digital sources, Chiara Zuanni illustrates this positionality within technological assemblages with the example of a social media post: “Provenance might refer in the first instance to the author of a post, but it can also be traced to the data center hosting the specific content (thinking about where the content is written on a server, its forensic origin), reflecting the ways the post has traveled through the infrastructure, e.g., from a personal device to a server, and has subsequently been queried by its viewer. The agency of assemblage is therefore critical in delivering content through its material infrastructure. This agency leads to the circulation of information, a global participation in events and cultural trends, and the environmental and economic impacts of the infrastructure” (Zuanni 2021, 189).\nThis quote touches upon a different meaning of the term “environment,” referring to the ecological consequences of data processing and transmission and the necessary infrastructures. The impact and environmental costs of data transfer and routing via the internet are difficult to evaluate, but the energy needs and the direct and indirect results of environmental destruction are enormous (Pasek, Vaughan, and Starosielski 2023). Similarly, the term “system” can be employed to describe the socio-economic, political, and cultural configurations and power structures in which the production of computing software and hardware as well as data-driven knowledge are organized and enforced. While proponents of “new materialism” have convincingly demonstrated that the “cloud” is actually a very material and manifest assemblage consisting of data centers and routing and transmission infrastructures, the term “smart technologies” suggests that data processing is a predominantly automatized and de-humanized affair, omitting the work of humans at each and every level and corner of the information society: from underpaid female workers assembling processors to click-workers around the world training algorithms and normalizing data, to the often unremarked and uncredited specialists maintaining and repairing the systems we take for granted. Even Artificial Intelligence, the very symbol of working and thinking without human involvement, actually consists of a lot of human labor (Mullaney et al. 2021).\nIncluding context, systems, and environment into the analysis and reflection of computing and born-digital objects is therefore at the same time a productive research agenda for the history of computing and an approach to an updated variant of source criticism and general digital literacy. Historians, trained to contextualize and situate information provided by sources within specific historic, social, cultural, and spatial contexts, can apply their instruments of critique and evaluation easily to digital objects and additionally provide guidance to the formulation of a general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/474/index.html#contextualization-and-critique",
    "href": "submissions/474/index.html#contextualization-and-critique",
    "title": "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All",
    "section": "Contextualization and Critique",
    "text": "Contextualization and Critique\nThis essay has so far argued that historians already have some valuable methods and approaches at their disposal to adapt their inquiries to novel, digital-born artifacts and media, and that they need to incorporate knowledge about the basic principles of computing and its history into their toolbox to be able to make sense of new media and archives. However, it is pivotal to keep in mind that providing interpretation and critique remains their core task. Decoding a digital artifact, tracing the history of its emergence, and understanding its relation to its technical environment serve but one objective: to make claims and arguments about its meaning. This is and remains a fundamentally critical approach that does not exclude a reflection on the methods themselves. Zack Lischer-Katz, for example, reminds us that digital forensics were not developed by and for historians, but serve a specific task in police investigations and the courtroom: “However, caution must be exercised when considering forensics as a guiding approach to archives. The epistemological basis of forensic science embeds particular assumptions about knowledge and particular systems of verification and evidence that are based on hierarchical relations of power, positivist constructions of knowledge, and the role of evidence […] A critical approach to the tools of digital forensics by archivists and media scholars requires thinking through how the forensic imagination may impose forms of knowing that reproduce particular power relations” (Lischer-Katz 2016, 5–6).\nAt a very basic level, historians and humanists, in general, are particularly strong exactly when their findings are more than just an addition and re-arrangement of available information. Using the distinctions between symbols and signals, Berry and colleagues have formulated an eloquent reminder of this task: “Digital humanists must address the limits of signal processing head-on, which becomes even more pressing if we also consider another question brought about by the analogy to Shannon and Weaver’s model of communication. The sender-receiver model describes the transmission of information. The charge of the digital humanities is, instead, the production of knowledge. An uncritical trust in signal processing becomes, from this perspective, quite problematic, insofar as it can confuse information for knowledge, and vice versa. […] Neither encoding or coding (textual analysis) is in fact a substitute for humanistic critique (understood in the broad sense)” (Berry et al. 2019).\nCritique is and must remain the central concern of historians. This critique must be directed to the authenticity and credibility of born-digital objects and the systems that produce them. To do so, they must learn from the tools and approaches of computer forensics. But what distinguishes historians from the forensic experts is that they don’t stop at the limits of the technical systems but extend their contextualization to the broader cultural, economic, and social structures that enable the development of specific technologies. This is why the historian’s perspective and approach are indispensable for general digital literacy.",
    "crumbs": [
      "Abstracts",
      "From Source-Criticism to System-Criticism, Born Digital Objects, Forensic Methods, and Digital Literacy for All"
    ]
  },
  {
    "objectID": "submissions/465/index.html",
    "href": "submissions/465/index.html",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "",
    "text": "Over the last few years, Machine Learning applications became more and more popular in the humanities and social sciences in general, and therefore also in history. Handwritten Text Recognition (HTR) and various tasks of Natural Language Processing (NLP) are now commonly employed in a plethora of research projects of various sizes. Even for PhD projects it is now feasible to research large corpora like serial legal source, which would not be possible entirely by hand. This acceleration of research processes implies fundamental changes to how we think about sources, data, research and workflows.\nIn history, Machine Learning systems are typically used to speed up the production of research data. As the output of these applications is never entirely accurate or correct, this raises the question how historians can use machine generated data together with manually created data without propagating errors and uncertainties to downstream tasks and investigations.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#introduction",
    "href": "submissions/465/index.html#introduction",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "",
    "text": "Over the last few years, Machine Learning applications became more and more popular in the humanities and social sciences in general, and therefore also in history. Handwritten Text Recognition (HTR) and various tasks of Natural Language Processing (NLP) are now commonly employed in a plethora of research projects of various sizes. Even for PhD projects it is now feasible to research large corpora like serial legal source, which would not be possible entirely by hand. This acceleration of research processes implies fundamental changes to how we think about sources, data, research and workflows.\nIn history, Machine Learning systems are typically used to speed up the production of research data. As the output of these applications is never entirely accurate or correct, this raises the question how historians can use machine generated data together with manually created data without propagating errors and uncertainties to downstream tasks and investigations.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#facticity",
    "href": "submissions/465/index.html#facticity",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Facticity",
    "text": "Facticity\nThe question of the combined usability of machine-generated and manually generated data is also a question of the reliability or facticity of data. Data generated by humans are not necessarily complete and correct either, as they are a product of human perception. For example, creating transcriptions depends on the respective transcription guidelines and individual text understanding, which can lead to errors. However, we consider transcriptions by experts as correct and use them for historical research. This issue is even more evident in the field of editions. Even very old editions with methodological challenges are valued for their core content. Errors may exist, but they are largely accepted due to the expertise of the editors, treating the output as authorised. This pragmatic approach enables efficient historical research. Historians trust their ability to detect and correct errors during research.\nFrancesco Beretta represents data, information, and knowledge as a pyramid: data form the base, historical information (created from data through conceptual models and critical methods) forms the middle, and historical knowledge (produced from historical information through theories, statistical models and heuristics) forms the top (Beretta 2023, fig. 3). Interestingly, however, he makes an important distinction regarding digital data: “Digital data does not belong to the epistemic layer of data, but to the layer of information, of which they are the information technical carrier” (Translation: DW. Original Text: “[L]les données numériques n’appartiennent pas à la strate épistémique des données, mais bien à celle de l’information dont elles constituent le support informatique.” Beretta 2023, 18)\nAndreas Fickers adds that digitization transforms the nature of sources, affecting the concept of the original (Fickers 2020, 162). Sources are preprocessed using HTR/OCR and various NLP strategies. The resulting digital data are already processed historical information. This shift from analog to digital means that what we extract from sources is not just given but constructed (Beretta 2023, 26). Analog historical research, which relies on handwritten archival documents, also depends on transcriptions or editions to conduct research pragmatically; and here, too, data becomes information. The main difference is that with the generation of digital data, the (often linear) structure of sources is typically dissolved in favour of a highly fragmented and hyperconnected structure (For hyperconnectivity see Fickers 2022, 51–54; For the underlying concept of hypertextual systems see Landow 2006, 53–58; for a a more extensive discussion of digital representations of fragmented texts see Weber 2021). This is partly due to the way sources are processed into historical information using digital tools and methods, but it is inherently connected with issues of storing, retrieving, and presenting digital data – in a very technical sense.\nThe concept of factoids introduced by Michele Pasin and John Bradley, is central to this argument. They define factoids as pieces of information about one or more persons in a primary source. Those factoids are then represented in a semantic network of subject-predicate-object triples (Pasin and Bradley 2015, 89–90). This involves extracting statements from their original context, placing them in a new context, and outsourcing verification to later steps. Therefore, factoids can be contradictory. Francesco Beretta applies this idea to historical science, viewing the aggregation of factoids as a process aiming for the best possible approximation of facticity (Beretta 2023, 20). The challenge is to verify machine output sufficiently for historical research and to assess the usefulness of the factoid concept. Evaluating machine learning models and their outputs is crucial for this.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#qualifying-error-rates",
    "href": "submissions/465/index.html#qualifying-error-rates",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Qualifying Error Rates",
    "text": "Qualifying Error Rates\nEvaluating the output of a machine learning system is not trivial. Models can be evaluated using various calculated scores, which is done continuously during the training process. However, these performance metrics are statistical measures that generally refer to the model and are based on a set of test data. Even the probabilities output by machine learning systems when applied to new data are purely computational figures, only partially suitable for quality assurance. This verification is further complicated by the potentially vast scale of the output. Therefore, historical science must find a pragmatic way to translate statistical evaluation metrics into qualitative statements and identify systematic sources of error.\nIn automatic handwriting recognition, models are typically evaluated using character error rate (CER). These metrics only tell us the percentage of characters or words incorrectly recognised compared to a ground truth. They do not reveal the distribution of these errors, which is important when comparing automatic and manual transcriptions. For detailed HTR model evaluation, CERberus is being developed (Haverals 2023). This tool compares ground truth with HTR output from the same source. Instead of calculating just the character error rate, it breaks down the differences further. Errors are categorised into missing, excess, and incorrectly recognised characters. Additionally, a separate CER is calculated for all characters and Unicode blocks in the text, aggregated into confusion statistics that identify the most frequently confused characters. Confusion plots are generated to show the most common errors for each character. These metrics do not pinpoint specific errors but provide a more precise analysis of the model’s behaviour. CERberus cannot evaluate entirely new HTR output without comparison text but is a valuable tool for Digital History, revealing which character forms are often confused and guiding model improvement or post-processing strategies.\nIn other machine learning applications, such as named entity recognition (NER), different metrics are important, requiring detailed error source analysis. Evaluating NER is more complex than HTR because it involves categorizing longer text sections based on context. Precision (how many recognised positives are true positives) and recall (how many actual positives are recognised) are combined into the F1-score to indicate model performance. Fu et al. proposed evaluating NER with a set of eight annotation attributes influencing model performance. These attributes are divided into local properties (entity length, sentence length, unknown word density, entity density) and aggregated attributes (annotation consistency and frequency at the token and entity levels) (Fu, Liu, and Neubig 2020, 3). Buckets of source points where a model performs particularly well or poorly are created and separately evaluated (Fu, Liu, and Neubig 2020, 1). This analysis identifies conditions affecting model performance, guiding further training steps and dataset expansion.\nThe qualitative error analysis presented here does not solve the question of authorizing machine learning output for historical research. Instead, it provides tools to assess models more precisely and analyse training and test datasets. Such investigations extend the crucial source criticism in historical science to digital datasets and the algorithms and models involved in their creation. This requires historians to expand their traditional methods to include new, less familiar areas.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/465/index.html#three-strategic-directions",
    "href": "submissions/465/index.html#three-strategic-directions",
    "title": "On the Historiographic Authority of Machine Learning Systems",
    "section": "Three Strategic Directions",
    "text": "Three Strategic Directions\nIn the following last part of this article, the previously raised questions and problem areas will be consolidated, from which three strategic directions for digital history will be derived. These will be suggestions for how the theory, methodology, and practice of Digital History could evolve to address and mitigate the identified problem areas. The three perspectives should not be viewed in isolation or as mutually exclusive. Instead, they are interdependent and should work together to meet the additional challenges.\n\nDirection 1: Formulating Clear Needs\nWhen data is collected or processed into information in the historical research process a certain pragmatism is involved. Ideally, such a project would fully and consistently transcribe the entire collection with the same thoroughness, but in practice, a compromise is often found between completeness, correctness, and pragmatism. Often, for one’s own research purposes, it is sufficient to transcribe a source only to the extent that its meaning can be understood. This compromise has not fully transitioned into Digital History. Even if a good CER is achieved, there is pressure to justify how these potential errors are managed in the subsequent research process. This skepticism is not fundamentally bad, and the epistemological consequences of erroneous machine learning output are worthy of discussion. Nonetheless, the resulting text is usually quite readable and usable.\nThus, I argue that digital history must more clearly define and communicate its needs. However, it must be remembered that Digital History also faces broader demands. Especially in machine learning-supported research, the demand for data interoperability is rightly emphasised. Incomplete or erroneous datasets are, of course, less reusable by other research projects.\n\n\nDirection 2: Creating Transparency\nThe second direction for digital history is to move towards greater transparency. The issue of reusability and interoperability of datasets from the first strategic direction can be at least partially mitigated by transparency.\nAs Hodel et al. convincingly argued, it is extremely sensible and desirable for projects using HTR to publish their training data. This allows for gradual development towards models that can generalise as broadly as possible (Hodel et al. 2021, 7–8). If a CERberus error analysis is conducted for HTR that goes beyond the mere CER, it makes sense to publish this alongside the data and the model. With this information, it is easier to assess whether it might be worthwhile to include this dataset in one’s own training material. Similarly, when NER models are published, an extended evaluation according to Fu et al. helps to better assess the performance of a model for one’s own dataset.\nPasin and Bradley, in their prosopographic graph database, indicate the provenance of each data point and who captured it (Pasin and Bradley 2015, 91–92). This principle could also be interesting for Digital History, by indicating in the metadata whether published research data was generated manually or by a machine, ideally with information about the model used and the annotating person for manually generated data. Models provide a confidence estimate with their prediction, indicating how likely the prediction is correct. The most probable prediction would be treated as the first factoid. The second or even third most probable prediction from the systems cloud provide additional factoids that can be incorporate into the source representation. These additional pieces of information can support the further research process by allowing inconsistencies and errors to be better assessed and balanced.\n\n\nDirection 3: Data Criticism and Data Hermeneutics\nThe shift to digital history requires an evaluation and adjustment of our hermeneutic methods. This ongoing discourse is not new, and Torsten Hiltmann has identified three broad directions: first, the debate about extending source criticism to data, algorithms, and interfaces; second, the call for computer-assisted methods to support text understanding; and third, the theorization of data hermeneutics, or the “understanding of and with data” (Hiltmann 2024, 208).\nEven though these discourse strands cannot be sharply separated, the focus here is primarily on data criticism and hermeneutics. The former can fundamentally orient itself towards classical source criticism. Since digital data is not given but constructed, it is crucial to discuss by whom, for what purpose, and how data was generated. This is no easy task, especially when datasets are poorly documented. Therefore, the call for data and model criticism is closely linked to the plea for more transparency in data and model publication.\nIn the move towards data hermeneutics, a thorough rethinking of the factoid principle can be fruitful. If, as suggested above, the second or even third most likely predictions of a model are included as factoids in the publication of research data, this opens up additional perspectives on the sources underlying the data. From these new standpoints, the data – and thus the sources – can be analyzed and understood more thoroughly. Additionally, this allows for a more informed critique of the data, and extensive transparency also mitigates the “black box” problem of interpretation described by Silke Schwandt (Schwandt 2022). If we more precisely describe and reflect on how we generate digital data from sources as historians, we will find that our methods are algorithmic (Schwandt 2022, 81–82). This insight can also support the understanding of how machine learning applications work. Data hermeneutics thus requires both a critical reflection of our methods and a more transparent approach to data and metadata.",
    "crumbs": [
      "Abstracts",
      "On the Historiographic Authority of Machine Learning Systems"
    ]
  },
  {
    "objectID": "submissions/480/index.html",
    "href": "submissions/480/index.html",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "",
    "text": "To start with some definitions, the term “flora” – in the sense of a document – denotes a directory in which the plant species of a specific area are systematically listed, often together with a description and additional information. “Herbarium” refers to a collection of preserved (usually dried and pressed) plants or fungi for scientific purposes; an individual botanical object in a herbarium collected at a specific place and time is called a “specimen” (Wagenitz 2003).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#introduction",
    "href": "submissions/480/index.html#introduction",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "",
    "text": "To start with some definitions, the term “flora” – in the sense of a document – denotes a directory in which the plant species of a specific area are systematically listed, often together with a description and additional information. “Herbarium” refers to a collection of preserved (usually dried and pressed) plants or fungi for scientific purposes; an individual botanical object in a herbarium collected at a specific place and time is called a “specimen” (Wagenitz 2003).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#floras-and-herbaria-in-the-history-of-knowledge",
    "href": "submissions/480/index.html#floras-and-herbaria-in-the-history-of-knowledge",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Floras and herbaria in the history of knowledge",
    "text": "Floras and herbaria in the history of knowledge\nBoth the flora and the herbarium are regarded as decisive innovations in the development of botany into an independent scientific discipline from the mid-16th century onwards. Together with the botanical garden, which is functionally related and established in the same period (Rieppel 2016; Findlen 2006a), only the invention of the herbarium made it possible to work out regional floras based on systematic empirical fieldwork (Flannery 2023; Müller-Wille 2019). Paula Findlen summarised the motivation behind this crucial invention: “The more naturalists observed nature in situ, the more they realized that limited contact with specimens did not yet yield enough knowledge to describe and compare medicinal herbs. They needed to take nature home” (Findlen 2006b, 447; see Sunderland 2016). Findlen stands for the history of knowledge or the renewed history of science that firstly emphasises the shift from finalised knowledge to the act of its production, secondly shows an increased interest in the everyday intellectual life of small groups, circles or networks and thirdly focuses on practices and material cultures of knowledge (Müller-Wille, Carsten, and Sommer 2017; Förschler and Mariss 2017; Holenstein, Steinke, and Stuber 2013). In this perspective and with the catchy phrase “collecting as knowledge”, the creation of a natural history collection, such as a herbarium, is seen as knowledge production (Heesen and Spary 2001). The activity of collecting expresses not only the fact that dispersed natural objects are brought together in a single location, but also that the forms of representation associated with them, such as illustrations, descriptions, lists and publications, are included in the repositories, where they are available for comparison, retracing and synoptic synthesis (Klemun 2017, 235).\nThe precise structures and functions of floras and herbaria with their spatial relations between local and global can only be understood in the context of the “collaborative knowledge culture of botany”. Therefore the interplay of the three central resources on which botany depended in early modern times has to be reconstructed: living and dried plants, relevant specialised literature (e.g. floras) and correspondence (Dietz 2017a, 2017b). The nexus of correspondence, plant transfer and collection policy was first reconstructed by Emma Spary using the example of the network of André Thouin (1747–1824), director of the Jardin du Roi in Paris (Spary 2000, 49–98). The analysis of such networks draws attention to the extensive transfer of dried plants and seeds as the basis of knowledge production (Dauser et al. 2008). A wide variety of methods has been used to correspond efficiently, to save time and to avoid loss of information. First and foremost is the use of reference catalogues. This means that lists of transmitted or desired plant species could simply be referred to the numbers that had been assigned to the species in an published flora (Dietz 2017a, 96–99). Secondly, network analyses show that natural history owes its existence not only to the outstanding figures, but also developed through the participation of thousands of amateurs working locally (Klemun 2017, 239). Correspondence networks not only serve to improve understanding of herbaria and floras, it is also possible to go in the opposite direction: herbaria themselves can serve as a source for social network analyses by systematically evaluating the collectors of the individual specimen (Siracusa et al. 2020; Groom, O’Reilly, and Humphrey 2014).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#digitisation-of-herbaria-from-a-botanical-perspective",
    "href": "submissions/480/index.html#digitisation-of-herbaria-from-a-botanical-perspective",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Digitisation of herbaria from a botanical perspective",
    "text": "Digitisation of herbaria from a botanical perspective\nThe value of herbaria has long been recognized in the fields of taxonomy, systematics and biogeography. Moreover, in recent decades they have proven to be fundamental for dealing with the biology of climate change, biodiversity, phenology, conservation and biological invasions. Given the high scientific and cultural value of herbarium collections, many efforts to make them more accessible have already been made in the last 20 years. Digitization is an essential first step in the process of transforming this vast amount of data associated with physical specimen into flexible digital data formats that allow information to be re-categorized according to variable criteria (Roma-Marzio et al. 2023, 108; see generally Andraschke and Wagner 2020). Building on centuries of research based on herbarium specimens collected over time and around the globe, which are freely accessible and aggregable, a “new era” of discovery, synthesis and prediction using digitized collection data is postulated (James et al. 2018; Nelson and Ellis 2018). Digitization and online availability of specimen facilitates the rapid exploration and dissemination of accurate biodiversity data on an unprecedented scale: “The emerging ‘herbarium of the future’ (or the ‘global metaherbarium’) will be the central element guiding the exploration, illumination, and prediction of plant biodiversity change in the Anthropocene” (Davis 2023, 412). It should be borne in mind that collections are usually associated with various distortions that need to be characterised and mitigated to make data usable. Most common are taxonomic and collector biases, which can be understood as the effects of particular recording preferences of key collectors on the overall taxonomic composition of the biological collections to which they contribute (Siracusa et al. 2020; Davis 2023, 421; Jaroszynska et al. 2023). In order to capture such phenomena so that they can be taken into account in the data analysis, precise knowledge of the entire context in which a herbarium was created is required. This is exactly the aim of the approach described above under history of knowledge. Obviously, there is a bridge here between the research interests of the natural sciences and the humanities. An overview published in 2024 shows that the topic of accessibility and digitization of herbaria as “archives of biodiversity” has also gained new relevance in Switzerland in recent years. Apart from two major exceptions, the Platter-Herbarium and Les Herbiers de Rousseau, there have been no attempts to do this in an interdisciplinary context (Stämpfli 2024). Additionally there is a lack of including the interaction with the functionally linked correspondence networks and contemporary floras. For this reason, the experience with historical plants gained on hallerNet, on which we pursued an interdisciplinary approach to the interaction between different types of entities (letters, species, specimens, reviews), may be of general interest.",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#historical-plants-on-the-data-and-edition-platform-hallernet",
    "href": "submissions/480/index.html#historical-plants-on-the-data-and-edition-platform-hallernet",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Historical plants on the data and edition platform hallerNet",
    "text": "Historical plants on the data and edition platform hallerNet\nThe data and edition platform hallerNet opens up historical networks of knowledge in Switzerland in their European interconnectedness. The basis of the currently around 128,000 data objects is formed by extensive prosopographical and bibliographical data that has been compiled in a relational database (FAUST) since the early 1990s as part of three SNSF projects at the University of Bern. A transformation project (2016–2019) transferred this extensively interlinked data into a XML data structure compliant to the Text Encoding Initiative (TEI) and thus turned it into “reusable research data” based on the FAIR data criteria (Dängeli and Stuber 2020). The platform nowadays contains, among others, around 46,000 publications, 31,000 persons and 1,200 institutions, which are systematically linked to 9,000 edited reviews and 5,000 edited letters. In our context, the 4,955 plant entities currently on hallerNet are at the centre. Starting point were the 1,737 species of flowering plants mentioned in Haller’s Swiss Flora Historia Stirpium (Haller 1768), which were systematically referenced in the aforementioned relational database to Haller’s first edition of the Swiss Flora Enumeratio (Haller 1742), to Linné’s Species plantarum (Linné 1753) and to the current nomenclature. This concordance between Haller’s and Linné’s nomenclature, compiled by Luc Lienhard with reference to Johan Rudolf Suter’s Flora Helvetica (Suter 1802), not only makes Haller’s Swiss flora accessible, but does also provide access to pre-Linnaean botany in general. On this basis, we transformed the botanical data, which was originally divided into four different data types, for the new XML structure into a generic data model based on today’s plant entities (InfoFlora, Global Biodiversity Information Facility GBIF), and treat their (historical) names as name variants. In this way, entities become flexibly adaptable for other historical floras that are partly or completely outside Haller’s and Linné’s nomenclature. At the same time, the data model structured according to today’s nomenclature facilitates reference to current issues in historical ecology. The following summary of realized, initiated or planned expansions illustrates this double advantage.\n\nEcological data: The diverse ecological information in Haller’s Historia on habitat, frequency, typical altitudinal range and specific localities is of far above-average quality for the 18th century (Lienhard 2005). hallerNet systematically records a total of 7,545 locality details, whereby the 1,920 different localities have been georeferenced for the most part as kilometre squares with their corner points and additionally linked to the neighbouring municipalities (‘populated places’) in order to appear in the hallerNet place register. Haller’s extraordinary data is thus available in a flexible structure whose exploration is only just beginning (Lienhard 2008, 2000). Historical biodiversity research, which is high on the agenda of environmental history (Goethem and Zanden 2019), has a wealth of source material at its disposal. Using appropriate methods of analysis, this will massively extend its temporal scope (Jaroszynska et al. 2023; Wang et al. 2023; Stöckli et al. 2012; Lachat 2010).\nCollectors and correspondents: Haller’s Historia also often includes the collector to whom Haller owes the information. These 109 people are all curated on hallerNet and systematically referenced for each species (1,342 times in total). The network is to be further expanded by systematically labelling the plants mentioned in Haller’s botanical correspondence, some of which has already been edited on hallerNet and most of which are made accessible via the International Image Interoperability Framework (IIIF). How great the potential of the correspondence is, both for the reconstruction of the ‘knowledge culture’ and for the supplementation and specification of the location data, is demonstrated by some analyses already available (Favre 2021; Hächler 2008; Lienhard 2005).\nBook references and reviews: For his extensive information on the plant species of his Swiss flora, Haller also uses a vast amount of historical data from his predecessors. For example synonyms and place references, which have already been added to hallerNet. Together with the links to Hallers other botanical publications (Steinke and Profos 2004, 186–95), to the numerous botanical publications in Hallers personal library (Monti 1983–1994, integrated in hallerNet) and to Hallers countless botanical reviews in the Göttingische Gelehrten Anzeigen, all of which are available in edited form on hallerNet, the integral process of botanical knowledge production could be precisely reconstructed (see Dietz submittet; Lienhard 2005).\nUseful plants: 656 species or varieties are listed in a total of eleven systematic catalogues in the context of the Bernese Economic Society, which was presided over by Haller. In this catalogues, the Latin-universal plant names were consistently linked to the dialectal-regional plant names. On hallerNet, 755 actions are linked to them, most of which obtained from the meetings of the Economic Society (Stuber and Lienhard 2007; Stuber 2008). This reveals a wide range of interferences between botany and agricultural botany (Dauser and Stuber submittet; Gerber-Visser and Stuber 2019; Stuber 2018; Boscani Leoni and Stuber 2017).\nHerbaria: The digitisation of Haller’s herbaria is one of the most intruding unfulfilled postulates in the study of Haller’s botany and beyond. Haller’s main herbarium, which after being sold by his heirs to Emperor Joseph II was first sent to Pavia and later to Paris by Napoleon, is now in the Muséum National d’Histoire Naturelle and comprises more than 10,000 specimens in a total of 60 volumes (including 8 volumes of cryptogams) (Margéz, Aupic, and Lamy 2006; Zoller 1958a); further there is a smaller herbarium by Haller in Göttingen (Zoller 1958b). Due to the fact that the current location of Haller’s herbaria is not in Switzerland, its digitization is not in scope of being supported by the ongoing SwissCollNet project, the national initiative for the digitization of natural history collections (Frick, Stieger, and Scheidegger 2019). As part of SwissCollNet, however, the lichen herbarium of Jean-Frédéric Chaillet (1747–1839), which is kept in the Neuchâtel herbarium (NEU), has now been edited on hallerNet as sub-project Lichens of the Enlightenment led by Jason Grant. This is consequent in terms of content, as Chaillet operated as a direct Swiss successor to Haller and referred to him wherever possible. At the same time, it represents a milestone for hallerNet, as the platform data structures for Herbaria could be developed. The centrepiece are the 943 lichen specimens, which firstly contain the transcribed original information on the label. Secondly, they are linked to the original scan via IIIF and additionally with positional accuracy, as there are several specimens sticked on one herbarium page. Thirdly, they are assigned to the species entities, which point to authority data (GBIF, Index Fungorum, SwissLichens). This species entities also contain the data from historical floras, in this case all from the manuscript flora by Chaillet and, where already listed, from Haller’s Historia. The information from historical floras is often the decisive key to relate the objects in a herbarium to present-day taxonomic databases. The assignment of source terms to standardised data is thus presented transparently on the platform, which is particularly essential for a period in which botanical nomenclature is still very unstable. Additionally, the structure of the data follows Darwin Core standard which facilitates the connection to other systems such as the Neuchâtel Herbarium, the emerging SwissCollNet database or the global Index Herbariorum (Vust et al. in prep.).\nPlant lists: The connectivity of hallerNet is also demonstrated by Meike Knittel’s ongoing guest edition of plant lists in the circle of the Zurich botanist Johannes Gessner (1709–1790), the Naturforschende Gesellschaft and the botanical garden, which document the actual exchange of seeds and list a total of 1,829 individual actions (see Knittel in print).",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "submissions/480/index.html#conclusion",
    "href": "submissions/480/index.html#conclusion",
    "title": "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity",
    "section": "Conclusion",
    "text": "Conclusion\nReconstructing the whole interaction in which floras and herbaria interplayed, difficulties arise in integrating digital approaches to historical correspondence networks (e.g. Edmondson and Edelstein 2019) with digitization methods for floras and herbaria, which are located in different scientific disciplines. The challenge for the data and edition platform hallerNet is therefore to find interdisciplinary solutions. With tools, methods and workflows of the digital humanities, traceable relations between text, scans and structural data are determined in an innovative way. That allows to rely today’s botanical authority data systematically to the historical information such as changing plant names, specimens, locality information and plant collectors. For the interoperability of the data, the orientation towards the Darwin Core standard is mandatory, for the sustainable editorial quality the TEI guidelines. Originally developed in the natural sciences, the FAIR data principles became a standard in the humanities (especially for GLAM institutions), and thus serve as an overarching guideline; in particular, FAIR guarantees the sustainable handling of data, which therefore remains ‘reusable’ for future generations of users because the traces of the normalization and flexibilization processes can be traced in detail. With this integration of different disciplinary standards and different types of sources, hallerNet could become a dynamic and cross-collection instrument for the interdisciplinary research of historical plants and biodiversity in Switzerland in the period before 1850. The current transformation of hallerNet into the national collaborative platform République des Lettres will further strengthen this potential.",
    "crumbs": [
      "Abstracts",
      "Connecting floras and herbaria before 1850 – challenges and lessons learned in digital history of biodiversity"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#repository-structure",
    "href": "about.html#repository-structure",
    "title": "",
    "section": "Repository Structure",
    "text": "Repository Structure\nThe structure of this repository follows the Advanced Structure for Data Analysis of The Turing Way and is organized as follows:\n\nsubmissions/ Contains the submissions for the conference.\nbook-of-abstracts.md The introduction to the book of abstracts.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#use",
    "href": "about.html#use",
    "title": "",
    "section": "Use",
    "text": "Use\nThis data is openly available to everyone and can be used for any research or educational purpose. If you use this data in your research, please cite as specified in CITATION.cff. The following citation formats are also available through Zenodo:\n\nBibTeX\nCSL\nDataCite\nDublin Core\nDCAT\nJSON\nJSON-LD\nGeoJSON\nMARCXML\n\nZenodo provides an API (REST & OAI-PMH) to access the data. For example, the following command will return the metadata for the most recent version of the data\ncurl -i https://zenodo.org/api/records/ZENODO_RECORD",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#support",
    "href": "about.html#support",
    "title": "",
    "section": "Support",
    "text": "Support\nThis project is maintained by @digihistch24. Please understand that we can’t provide individual support via email. We also believe that help is much more valuable when it’s shared publicly, so more people can benefit from it.\n\n\n\nType\nPlatforms\n\n\n\n\n🚨 Bug Reports\nGitHub Issue Tracker\n\n\n📊 Report bad data\nGitHub Issue Tracker\n\n\n📚 Docs Issue\nGitHub Issue Tracker\n\n\n🎁 Feature Requests\nGitHub Issue Tracker\n\n\n🛡 Report a security vulnerability\nSee SECURITY.md\n\n\n💬 General Questions\nGitHub Discussions",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#roadmap",
    "href": "about.html#roadmap",
    "title": "",
    "section": "Roadmap",
    "text": "Roadmap\n\nAdd all submissions to the Book of Abstracts\nAdd a Table of Contents to the Book of Abstracts\nAdd an introduction to the Book of Abstracts\nAdd styling to the Book of Abstracts",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "",
    "section": "Contributing",
    "text": "Contributing\nAll contributions to this repository are welcome! If you find errors or problems with the data, or if you want to add new data or features, please open an issue or pull request. Please read CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#versioning",
    "href": "about.html#versioning",
    "title": "",
    "section": "Versioning",
    "text": "Versioning\nWe use SemVer for versioning. The available versions are listed in the tags on this repository.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#authors-and-acknowledgment",
    "href": "about.html#authors-and-acknowledgment",
    "title": "",
    "section": "Authors and acknowledgment",
    "text": "Authors and acknowledgment\n\nMoritz Mähr - Initial work - maehr\nMoritz Twente - Submissions - mtwente\nKapitolina Kostina - Submissions - consincopa\n\nSee also the list of contributors who contributed to this project.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "",
    "section": "License",
    "text": "License\nThe data in this repository is released under the Creative Commons Attribution 4.0 International (CC BY 4.0) License - see the LICENSE-CCBYSA file for details. By using this data, you agree to give appropriate credit to the original author(s) and to indicate if any modifications have been made.\nThe code in this repository is released under the GNU Affero General Public License v3.0 - see the LICENSE-AGPL file for details. By using this code, you agree to make any modifications available under the same license.",
    "crumbs": [
      "Abstracts",
      "About"
    ]
  },
  {
    "objectID": "LICENSE-CCBYSA.html",
    "href": "LICENSE-CCBYSA.html",
    "title": "",
    "section": "",
    "text": "AboutLicense (Data) Code\n\n\n\n\nAttribution-ShareAlike 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nAdditional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0",
    "crumbs": [
      "Abstracts",
      "About",
      "License (Data)"
    ]
  },
  {
    "objectID": "submissions/468/index.html",
    "href": "submissions/468/index.html",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "",
    "text": "Digital tools like the online portal and the Video Essays in Rural History series of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA) have greatly facilitated the use of films as sources and the publication of audiovisual media as means of communication. This significantly enhances the source base of historical studies of the 20th century and therefore enables scholars to include new perspectives in their research. It furthermore enables researchers to reach new audiences by communicating the results of their studies in audiovisual formats.\nThis presentation will first introduce the relevance of films in rural history and the role that the agricultural sector played in film history. It will then present the research infrastructure of the Archives of Rural History and the European Rural History Film Association. The presentation then concludes with reflections on the use of films as sources and means of communication in historical studies.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#introduction",
    "href": "submissions/468/index.html#introduction",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "",
    "text": "Digital tools like the online portal and the Video Essays in Rural History series of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA) have greatly facilitated the use of films as sources and the publication of audiovisual media as means of communication. This significantly enhances the source base of historical studies of the 20th century and therefore enables scholars to include new perspectives in their research. It furthermore enables researchers to reach new audiences by communicating the results of their studies in audiovisual formats.\nThis presentation will first introduce the relevance of films in rural history and the role that the agricultural sector played in film history. It will then present the research infrastructure of the Archives of Rural History and the European Rural History Film Association. The presentation then concludes with reflections on the use of films as sources and means of communication in historical studies.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#agriculture-in-films-films-in-agriculture",
    "href": "submissions/468/index.html#agriculture-in-films-films-in-agriculture",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Agriculture in Films – Films in Agriculture",
    "text": "Agriculture in Films – Films in Agriculture\nThe agricultural sector was one of the pioneers when it came to producing moving pictures. Film production outside the United States really started after World War I. The films made about rural Europe were used by organisations for educational purposes as well as for advertising products and for teaching the rural population new values and techniques. While in France the government funded a rural cinema campaign in the interwar period, in Switzerland it were mainly the agricultural organisations (often in cooperation with state institutions) which promoted the film as a medium of communication. And women farmers used the new medium to present their work on the farms from their own perspective. A crucial period in the development of the rural film production are the 1960s, when significant changes took place both in the structures and in the actors involved. Up to the 1960’s, agricultural films were almost exclusively so-called commercial or, more precisely, commissioned films. These films were commissioned by state departments, agricultural organisations or scientific institutions for specific purposes – but the films were often used for a variety of purposes. The producers normally were film production companies producing feature or cinema films as well. Indeed, most of them could not have survived from the risky feature-film business alone if they had not had a halfway steady income from their commercial activities, that is: producing commissioned films. Quite often these commissioned films – whether agricultural or otherwise – were shown as supporting films (Vorfilme) immediately before a feature film was shown in the cinema. The practice of broadcasting a commissioned film with an industrial, tourist or agricultural content as a supporting film for a feature film furthermore contributed to a better acceptance of the latter category as a form of art in the feuilleton of “respectable” papers where feature films for a long time in the 20th century were judged as “low-culture”.\nRural films up to the 1960’s can, broadly speaking, be divided into two categories: feature films under the cultural heading and commissioned films produced for industrial, tourist and agricultural clients. Exactly because agricultural films were regarded as part of the economic, not the cultural world, they were not judged as sophisticated enough and culturally valuable enough to be preserved for the future by the existing film archives. This attitude only changed significantly in the 1960/70s, when the so-called author-director films began their remarkable career. Intellectuals influenced by the student movement of the late 1960s began to look at agriculture, especially the peasantry in remote or mountain areas, from new perspectives. They literally produced new pictures, pictures their audience often did not associate with the rural world at all. The author-directors called themselves “documentary” film makers, convinced to “show nothing but the reality”.\nA second element that was crucial for the development and broadening of the independent film makers was the rise and breakthrough of television. TV provided a new outlet for the author- director film. It became, in addition to the state, an important financial support for the filmmakers. And it opened up for them a new, pre-dominantly urban audience that began to be interested in the peasant-mountain world for a variety of reasons.\n\n\n\nFig. 1: Milk transport with a handcart and a horse-drawn cart, shown in a remarkable split screen. Film still from the last of the three Swiss milk films (1923–1929), entitled Wir und die Milch (1929).1",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#the-arherhfa-research-infrastructure",
    "href": "submissions/468/index.html#the-arherhfa-research-infrastructure",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "The ARH/ERHFA research infrastructure",
    "text": "The ARH/ERHFA research infrastructure\nThe knowledge about the history of rural films in Europe is collected in the European film database of the Archives of Rural History (ARH) and the European Rural History Film Association (ERHFA). The ERHFA was founded in 2017. It is an association of film archives and research institutions interested in films from and about rural areas. The aim of the organisation is to promote the documentation, study and publication of (historical) films related to agricultural history and the history of rural areas. To achieve this goal, the ARH and the ERHFA operate a film database and an associated online portal, publish the Video Essays in Rural History series and organise workshops and panels at academic conferences.\nThe ARH/ERHFA film database currently contains metadata on around 4,300 films, including commissioned, amateur, author’s and feature films as well as television programmes. The status of the metadata collection differs from film to film. Of many films, a copy has been preserved, which, if digitised, is embedded directly in the database. For a number of other films, reference is made to institutions where the film can be viewed. Still other entries contain extensive metadata, without information about the film’s location, because it is not yet known whether a copy has survived or not. Finally, there are also fragmentary entries on films for which very little information is known to date, as well as on films that were planned but never produced. The database is a working tool that, like the online portal, is being continuously expanded as existing entries are complemented and new entries are added.\nThe database is structured according to works, i.e. versions or multiple copies of films are summarised in the entry for the corresponding work. Technical information on the individual copies can be obtained from the linked institutions that archive the films. However, the database not only contains links to digital copies or locations of film reels, but also details of written archival material or literature on the film. The database is thus a signpost pointing to institutions where more information is available.\nAround a quarter of the films listed in the database can be viewed in the online portal. The 27 institutions which contribute to the film database and the online portal come from Austria, Belgium, England, Finland, France, Germany, Ireland, the Netherlands, Portugal and Switzerland.\n\n\n\nFig. 2: The entries in the online portal can be searched using the quick and advanced search functions by search term, period of production, length, commissioner and production company.\n\n\nThe films are grouped according to the contributing institutions as well as thematic and chronological collections. Each collection consists of a short introductory text and a selection of the corresponding films. The chronological collections on the decades from the 1920s to the 1980s provide an overview of the development of film technology in the relevant period. The thematic collections illustrate the diversity of the films.\n\n\n\nFig. 3: Some of the chronological and thematic collections in the ARH/ERHFA online portal.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#films-as-sources",
    "href": "submissions/468/index.html#films-as-sources",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Films as Sources",
    "text": "Films as Sources\nThe accessibility of films via the ARH/ERHFA online portal facilitates the use of film sources in historical studies. As sources, films can be interpreted in at least two ways: firstly, as images of a bygone era that reveal much about the history of agriculture and, secondly, as media that intervened in this history and shaped it. As images, films visualise aspects of agricultural history that are hardly ever recorded in written and statistical sources. This may be because they were either not noticed or concealed, or because they cannot be recorded in writing. What sets the films apart from still images is that they also capture movements and sounds, which make additional contexts of agricultural work tangible, such as the verbal and non-verbal communication between humans and animals at work. Films thus bear witness, often unintentionally, to the fact that farming in practice often was not as it was portrayed or demanded in textbooks and magazines.\nHowever, films are more than mere images; they intervene in the context of their creation and use, create a reality of their own and exert an influence on the viewer.2 This was often used deliberately, for example if there was a need for media control when innovations of a technical, economic, political, social or medical nature had an impact on society or the environment. Changes of all kinds, including the controversies that accompanied them, were therefore an important reason to produce commissioned films. The films had the function of adapting their audiences to new requirements, creating acceptance for the innovation and laying the foundation for further changes. In this respect, commissioned films contributed to the creation of a willingness to cooperate and to consensus-building in modernisation processes.3 In the agricultural context, this function of films was used, for example, by the Eidgenössische Alkoholverwaltung EAV (Swiss Alcohol Board)4 and the plant protection company Dr Rudolf Maag AG, which commissioned and produced numerous films illustrating their activities and the use of their products.5\nThe dual function of audiovisual sources as images and as influencing media often cannot be adequately captured by written texts alone. This is why we conceptualise moving images also for analysing historical developments and communicating insights from historical research.",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#films-as-means-of-communication",
    "href": "submissions/468/index.html#films-as-means-of-communication",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Films as Means of Communication",
    "text": "Films as Means of Communication\nAnyone attempting to transfer knowledge gained from audiovisual sources into the written formats will come up against limitations because much of what characterises moving images is lost when written down: the dynamics and (in the case of sound films) the interplay of image and sound in particular. It is, furthermore, often impossible to translate the content of the image into words, for example when it comes to the behaviour of (speechless) animals, human-animal interactions or disappeared (agricultural) practices, for which there is no vocabulary in industrialised societies.6\nTo counter these difficulties, the format of the historical video essay lends itself as a supplement to written texts. A video essay in our series is understood as a montage of historical film and image material that is supplemented by an analytical commentary. The audiovisual sources are both source material and visual carrier of the knowledge transfer and are contextualised and analysed by a commentary. In addition to the communication function, video essays can also be used as an analytical tool.\n\n\n\nFig. 4: The first video essay in the series Video Essays in Rural History focuses on the importance of working horses, cattle, dogs, mules and donkeys in agriculture and in the cities of the 19th and 20th centuries.7\n\n\nThe ARH and ERHFA have launched the Video Essays in Rural History series, in which five video essays from Switzerland, Belgium and Canada have been published to date. They address the importance of working animals, Swiss agronomists and farmers travelling to America in the early 20th century, neighbourly cooperation in rural Canada, the motorisation of Belgian agriculture and Mina Hofstetter, an ecofeminist pioneer of organic agriculture.\nThe video essay is to be understood as a supplement to, not a replacement for, written formats. The video essays published in the Video Essays in Rural History series are therefore published together with an accompanying text. The five to thirty-minute video essays fulfil academic criteria and at the same time appeal to a wider audience. So far, they meet with great interest both within and outside the academic community. They are presented at conferences, used in academic teaching, linked to in media reports and achieve a relatively high number of hits on YouTube (the video essay on working animals was clicked on 3,100 times in the first week after publication, for example).",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/468/index.html#footnotes",
    "href": "submissions/468/index.html#footnotes",
    "title": "Films as sources and as means of communication for knowledge gained from historical research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe film is available online in the ARH/ERHFA online portal: ruralfilms.eu (16.08.2024).↩︎\nBernhardt Markus, Visual History: Einführung in den Themenschwerpunkt, in: Zeitschrift für Geschichtsdidaktik, 12/1 (2013), p. 5–8, here: p. 5.↩︎\nZimmermann Yvonne, Dokumentarischer Film: Auftragsfilm und Gebrauchsfilm, in: Zimmermann Yvonne (Hg.), Schaufenster Schweiz: Dokumentarische Gebrauchsfilme 1896-1964, Zürich 2011, p. 34–83, here: p. 64 & 69f.↩︎\nAuderset Juri/Moser Peter, Rausch & Ordnung. Eine illustrierte Geschichte der Alkoholfrage, der schweizerischen Alkoholpolitik und der Eidgenössischen Alkoholverwaltung (1887-2015), Bern 2016; Wigger Andreas, Saft statt Schnaps. Das Filmschaffen der Eidgenössischen Alkoholverwaltung (EAV) von 1930 bis 1985, in: Geschichte im Puls, Dossier 3: Ekstase (2022), www.geschichteimpuls.ch (02.07.2024)↩︎\nPlaylist Eidgenössische Alkoholverwaltung (EAV), in: Archiv für Agrargeschichte, YouTube Playlist (02.07.2024); Playlist Dr. Rudolf Maag AG, in: Archiv für Agrargeschichte, YouTube Playlist (02.07.2024).↩︎\nWigger Andreas, Bewegende Tiere auf bewegten Bildern. Filme als Quellen und Vermittlungsformat zur Geschichte der arbeitenden Tiere in der Zeit der Massenmotorisierung (1950-1980), Videoessay zur Masterarbeit, Fribourg 2023, YouTube (25.06.2024).↩︎\nMoser Peter/Wigger Andreas, Working Animals. Hidden modernisers made visible, in: Video Essays in Rural History, 1 (2022), https://www.ruralfilms.eu/essays/videoessay_1_EN.html [16.08.2024].↩︎",
    "crumbs": [
      "Abstracts",
      "Films as sources and as means of communication for knowledge gained from historical research"
    ]
  },
  {
    "objectID": "submissions/450/index.html",
    "href": "submissions/450/index.html",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "",
    "text": "GIS (Geographic Information Systems) have become increasingly valuable in spatial history research since the mid-1990s, and is particularly useful for analyzing socio-spatial dynamics in historical contexts (Kemp 2009, 16; Gregory and Ell 2007, 1). My PhD research applies GIS to examine and compare the development of public urban green spaces, namely public parks and playgrounds, in the port cities of Hamburg and Marseille, between post-WWII urban reconstruction and the First Oil Shock in 1973. The management and processing of data concerning green space evolution in GIS allow visualization of when and where parks were created, and how these reflect socio-spatial differentiations. This layering of information offers ways to evaluate historical data and construct arguments, while also helping communicate the project to a wider audience. To critically assess the application of GIS in historical research, I will use the SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis framework. This popular business consultancy approach (Minsky and Aron 2021) serves here as a structure for systematic reflection on how digital methods can enhance historical research and where caution is needed. The goal is to provoke critical thinking about when using GIS genuinely support research beyond producing impressive visuals, and to explore the balance between close and distant reading of historical data.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#introduction",
    "href": "submissions/450/index.html#introduction",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "",
    "text": "GIS (Geographic Information Systems) have become increasingly valuable in spatial history research since the mid-1990s, and is particularly useful for analyzing socio-spatial dynamics in historical contexts (Kemp 2009, 16; Gregory and Ell 2007, 1). My PhD research applies GIS to examine and compare the development of public urban green spaces, namely public parks and playgrounds, in the port cities of Hamburg and Marseille, between post-WWII urban reconstruction and the First Oil Shock in 1973. The management and processing of data concerning green space evolution in GIS allow visualization of when and where parks were created, and how these reflect socio-spatial differentiations. This layering of information offers ways to evaluate historical data and construct arguments, while also helping communicate the project to a wider audience. To critically assess the application of GIS in historical research, I will use the SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis framework. This popular business consultancy approach (Minsky and Aron 2021) serves here as a structure for systematic reflection on how digital methods can enhance historical research and where caution is needed. The goal is to provoke critical thinking about when using GIS genuinely support research beyond producing impressive visuals, and to explore the balance between close and distant reading of historical data.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#strengths",
    "href": "submissions/450/index.html#strengths",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Strengths",
    "text": "Strengths\nGIS are composed of layers of data sets and are mainly used for mapping, georeferencing, and data analysis (e.g. spatial analysis). The data can be varied in what it represents but must be linked to spatial parameters for it to be positioned in a visualization for analysis of spatial information (Wheatley and Gillings 2005, 1, 8). GIS layers for historical studies can be viewed as models of data sets. They represent specific topics in time and space simplistically, and spark reflection (Van Ruymbeke 2021, 8). The screen shot of my QGIS workspace (Figure 1) shows the city of Marseille with parks marked in various stages of planning in the early 1970s. This was the time when longstanding Mayor Gaston Defferre launched the large-scale greening initiative Mille Points Verts pour Marseille (Gassier 1971). Defferre and his team’s goal was to react to a growing ecological awareness and increase the number of green spaces for a more livable city (Chelini 1971). They also organized events to include and educate citizens and to garner their support for the upcoming elections (Anonymous 1971). The majority of parks created within Mille Points Verts remain until today, with only a handful of additional parks added after the mid-1970s. This is visible when the layer with parks and gardens from a 2018 dataset provided by the government of Marseille is selected (Figure 2). The strength of GIS layering is evident when we apply distant reading techniques: skimming over the model we see a display of spatial relations of park distribution and location as well as their connection to time.  In order for this visualization to take shape, I produced and assembled data. Specifically, I selected data and went through the process of closely reading my historical sources, learning to understand them and to think through their meaning. In this way maps are social documents. By themselves, they do not reveal anything yet. But by superimposing visualizations, GIS can reveal thinking processes of the data curators and map creators (cf. for example C. Jones 2021).\n\n\n\n\n\n\n\n\n\n(a) Planned parks (1970-71) within the ‘Mille Points Verts’ initiative.\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\n\n(a) This image can be deceiving as it seems that many parks do not overlap. Figure 1 shows approximate planning locations. The 2018 state of parks therefore shows the current locations of many of the planned ones from 1970-71.\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#weaknesses",
    "href": "submissions/450/index.html#weaknesses",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Weaknesses",
    "text": "Weaknesses\nThe curation of data, although an important and empowering step for the historian and GIS researcher, also reveals the weaknesses of GIS: the mismatch between GIS requirements (in terms of data structuring and quality) and the imperfection of historical data. GIS software is created for geographers, not historians. Everything in GIS is structured data and therefore cannot handle ambiguities natural to historical sources. Sources in whatever form they are collected by the historian first must be organized, selected, tabularized, geocoded and/or georeferenced (Kemp 2009, 16–17). The caveat here is that a historian’s data is hardly ever complete. Missing records shape both what we can and cannot analyze – especially when working with GIS. In historical narration on text the researcher can explain gaps and postulate why this may be the case. GIS do not allow for gaps and thus we can only produce models and visualizations with the numerical evidence available.\nThe visualization presented here helps to model different states of an object: the park. As my data is not complete, discrepancies between the mapped data and the on-the-ground reality occur, especially since the planned parks had vague names sometimes only matching the name of an entire neighborhood. This raises the question of how to capture temporality. How can the aspect of time appear on a two-dimensional visualization? Rendering the time layer onto the spatial one demands creativity and an awareness that time is something constructed (Massey speaks of “implicit imaginations of time and space” Massey 2005, 22).\nFrom the map making perspective, time significantly impacts the creation process. GIS work is time-consuming and labor-intensive. It involves meticulous manual searching, assembling, and layering of data. However, linking to the overarching topic of this conference, AI may offer new possibilities. Tools such as Transkribus allow users to apply machine learning to filter specific elements from document sets. LLMs can then process this information into CSV files for GIS software. While not yet revolutionary, as these tools evolve, AI could become useful in extracting numerical evidence from textual sources. For geocoding of places, AI would greatly aid efficiency and relieve the researcher of tedious manual work. However, at this point, LLMs such as Claude AI and ChatGPT still hallucinate considerably.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#opportunities",
    "href": "submissions/450/index.html#opportunities",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Opportunities",
    "text": "Opportunities\nAI-assisted data extraction presents a gateway to think about opportunities. Researchers could focus more on experimenting with design and layering by automating time-consuming tasks. For example, mapping supports spatial thinking and perception by integrating the crucial ‘where’ element: Where are specific features located? How near or far is one place from another? Depending on what obstacles or facilitators are in place a park may be close in measured distance but far in terms of accessibility if there is no bridge or tunnel to e.g. cross a motorway, or water element. Therefore, how are different locations related? How can we perceive and understand distance?\nThe screenshot here shows a handful of ‘wheres’ (Figure 3). They reveal where the majority of parks are located, their proximity to neighborhoods, the types of surrounding communities, and their connections to amenities and public infrastructure. This approach enables comparisons across different scales. For instance, I can compare park distribution between Hamburg and Marseille and track their development over time.  These questions prompted by the use of GIS direct both user and observer back to the original sources for close reading. A GIS model can spark interest in a topic and motivate the researcher to dig deeper on what these layers mean and how they were created. Ideally GIS should be used as a starting point for in depth analysis. In the case of Marseille and Hamburg, the development of public urban green spaces was what inspired me to look more closely at the historical circumstances. Hamburg, for instance, has a long history of creating expansive green areas with the support of private patrons. Marseille does not have a comparable patronage system. Instead, municipal expropriation rendered private villas and their gardens public.\nGIS are a powerful tool that serve multiple functions in research (Wheatley and Gillings 2005, 8). They “can play a role in generating ideas and hypotheses at the beginning of a project” and serve as valuable instruments for analysis and evaluation (Brewer 2006, S36). By modeling research hypotheses and findings, e.g. maps can be used to effectively communicate to diverse audiences – from the general public to specialized groups such as urban planners and municipal governments, relevant to my field of historical urban planning research.\nA particularly compelling aspect of GIS is their ability to visually represent power relations (Figure 4). This feature bridges the gap between historical analysis and contemporary urban planning, making it an invaluable tool in understanding the evolution of urban spaces. The visualization of Marseille reveals that the majority of parks are located towards the center and south of the city and does not necessarily correspond to the population density. The south of Marseille is where villas abound and thus the upper and upper-middle class live. The majority of the HLM (housing at moderate rent) are located towards the north, where living conditions are condensed, and political representation is low. What is more, if I select the layers showing where most immigrants and workers live today, a lack of green spaces is visible (Figure 5) (Figure 6) (Figure 7).  Connecting this once more to close reading of the sources: when Mille Points Verts was launched, planners scavenged locations for green space creation. The HLM neighborhoods were marked as unsuitable for participation in this program: People living in social housing would “misuse” the parks by playing soccer on them or walking across the grass (Anonymous 1970). This shows complexity of space perception and power imbalance (Van Ruymbeke 2021, 7).\n\n\n\n\n\n\n\n\n\n(a) Zoom in on the harbor area where also the “pénétrante nord” is located (built in the late 1960s). Along this main road many parks were planned but not built.\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Population 2012 layer turned on (natural breaks). This visualization presents a caveat: the northern part does not appear densely populated. This is not the case as the northern neighborhoods are very hilly, thus HLM apartment blocks house a large number of people in a small space.\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zoom in on harbor area. Population 2012 (natural breaks).\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zoom in on harbor area. Immigrants 2012 (natural breaks).\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zoom in on harbor area. Workers 2012 (natural breaks).\n\n\n\n\n\nFigure 7",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#threats",
    "href": "submissions/450/index.html#threats",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Threats",
    "text": "Threats\nYet all these opportunities are ambiguous and “entrusting machines with the memory of human activity can be frightening”. The last element of the SWOT analysis, threats, rounds off these reflections. Although it is crucial to encourage critical thinking through the mapping of, for example, political representation and wealth distribution of a city it also shows my personal convictions. I wish to demonstrate which voices where not heard in the planning of these spaces, which people were not considered when decisions were made. I am biased when I start with the premise that there is inequality. The map, objective as it may seem, never is. The book How to Lie with Maps provocatively shows the power of maps to create a strong, and perhaps deceiving, narrative:\n“Map users generally are a trusting lot: they understand the need to distort geometry and suppress features, and they believe the cartographer really does know where to draw the line, figuratively as well as literally. […] Yet cartographers are not licensed, and many mapmakers competent in commercial art or the use of computer workstations have never studied cartography. Map users seldom, if ever, question these authorities, and they often fail to appreciate the map’s power as a tool of deliberate falsification or subtle propaganda” (Monmonier 1996, 1).\n\nPeople working with GIS can have all kinds of skill levels and interests. I, for example, am not a GIS specialist and relatively new to using the tool. Still, I can easily manipulate my model to paint various pictures, if I wish to do so. I can turn on different layers and focus on the number of immigrants per neighborhood, I can change the classification for the choropleth map and create entirely different impressions, or I can simply change the basemap and take away the context of terrain, transportation systems, etc. (cf. Figure 4). The quote speaks of an almost blind trust in maps, which shows once more that we must always be critical observers of the things we consume and historians should always want to be curios fact checkers.  A map is a series of decisions and it reflects the biography of both the maker and the observer. It is the responsibility of the historian working with GIS to be as transparent as possible regarding the choices made to display a historical development or state. It is the responsibility of the observer to use the map as a starting point for close reading, interpretation and analysis rather than the end point and a fact. We must remember “80% of GIS is about transforming, manipulating and managing spatial data”(C. E. Jones and Schiel 2022).",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/450/index.html#conclusion",
    "href": "submissions/450/index.html#conclusion",
    "title": "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion the use of GIS in historical research and analysis requires the researcher to stay true to the principles of the craft of the historian: source criticism, the ability to subsume information, create a strong narrative, document the process of source manipulation and proper source citation. As historians we should be aware of the power of storytelling – no matter which medium we use. An audience’s spatial understanding can be enhanced via GIS models, serving as a support system of sorts. All the more reason why GIS in historical analysis must be used and consumed critically and consciously. By embracing this complexity, we can use GIS for historical reflections, enhancing our understanding of spatial and temporal dynamics in historical contexts.",
    "crumbs": [
      "Abstracts",
      "Using GIS to Analyze the Development of Public Urban Green Spaces in Hamburg and Marseille (1945 - 1973)"
    ]
  },
  {
    "objectID": "submissions/429/index.html",
    "href": "submissions/429/index.html",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "",
    "text": "Part of the national Lab In Virtuo project (2021-2024), the Techn’hom Time Machine project, initiated in 2019 by the Belfort-Montbéliard University of Technology, aims to study and digitally restore the history of an industrial neighborhood, with teacher-researchers but also students as co-constructors (Gasnier 2014; 2020, 293). The project is thus located at the interface of pedagogy and research. The Techn’hom district was created after the Franco-Prussian War of 1870 with two companies from Alsace: the Société Alsacienne de Constructions Mécaniques, nowadays Alstom; and the Dollfus-Mieg et Compagnie (DMC) spinning mill, in operation from 1879 to 1959. The project aims to create a “Time Machine” of these industrial areas, beginning with the spinning mill. We seek to restore in four dimensions (including time) buildings, machines with their operation, but also document and model sociability and know-how, down to the gestures and feelings. The resulting “Sensory Realistic Intelligent Virtual Environment” should allow both researchers and general public to virtually discover places and “facts” taking place in the industry, but also interact with them or even make modifications.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#introduction",
    "href": "submissions/429/index.html#introduction",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "",
    "text": "Part of the national Lab In Virtuo project (2021-2024), the Techn’hom Time Machine project, initiated in 2019 by the Belfort-Montbéliard University of Technology, aims to study and digitally restore the history of an industrial neighborhood, with teacher-researchers but also students as co-constructors (Gasnier 2014; 2020, 293). The project is thus located at the interface of pedagogy and research. The Techn’hom district was created after the Franco-Prussian War of 1870 with two companies from Alsace: the Société Alsacienne de Constructions Mécaniques, nowadays Alstom; and the Dollfus-Mieg et Compagnie (DMC) spinning mill, in operation from 1879 to 1959. The project aims to create a “Time Machine” of these industrial areas, beginning with the spinning mill. We seek to restore in four dimensions (including time) buildings, machines with their operation, but also document and model sociability and know-how, down to the gestures and feelings. The resulting “Sensory Realistic Intelligent Virtual Environment” should allow both researchers and general public to virtually discover places and “facts” taking place in the industry, but also interact with them or even make modifications.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#study-and-training-areas",
    "href": "submissions/429/index.html#study-and-training-areas",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Study and training areas",
    "text": "Study and training areas\nThe project is carried out within a technology university and, as such, is designed to include the participation of engineering students. They can apply and develop skills previously covered in a more basic way in their curriculum. This constitute for students an investment in the acquisition of skills that can subsequently be reused in their professional lives as engineers. In the current state, four main axes exist concerning inclusion of students in the Techn’hom Time Machine project:\n\nModeling of industrial buildings on Revit;\nMachine modeling on Catia;\nKnowledge engineering with the construction of a data model, initially as a relational database, having evolved into an RDF base based on standard ontologies;\nIntegration of those elements in the same virtual environment on Unity. Historical sources are crucial in all axes since many artifacts no longer exist, have been heavily modified and/or are inaccessible. Modeling is based on handwritten or printed writings, plans, iconography, and surviving heritage. This imposes a disciplinary opening for engineering students, untrained in the manipulation and analysis of such sources, and who may feel distant from issues linked to human and social sciences.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#project-progress",
    "href": "submissions/429/index.html#project-progress",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Project progress",
    "text": "Project progress\nTo date, thirty two students were included in the project. Each of the four axes was allocated between four and twelve students depending on opportunities and needs. In addition to the scientific contribution, student reports make it possible to evaluate their point of view on this training, all critical perspective retained.\n\nModeling: the software question\nThis axis has currently involved twelve students, and has led to the complete or partial modeling of six machines. It implies to reverse engineering machines with very partial data, on software designed for rendering of much more recent mechanisms. Students are assigned to work on small projects whose results are not necessarily directly usable. This offers the advantage of an exploratory and critical approach, by having a student take over the project of a previous one. Students were thus responsible for creating the model, but also for defining the software used. The first machine modeled, a twisting machine, was the subject of two successive works, linked to a change in modeling software. The first student used Blender, directing his work “on the optimization of models rather than on precision” and “took the initiative to abandon coherence”, offering “parts very close to the base material from a visual point of view but absolutely not reliable from a measurement point of view” (Bogacz 2019, 11–12). The following year, a second group was tasked of restoring consistency in this model, but realized that their colleague’s choices prevented such an achievement: pieces were too inaccurate, and conversion to a kinematic CAD model was impossible (Castagno and Vigne 2020, 11, 13). They therefore remade the model on Catia, without realistic texture. The team of another machine proposed another solution: on Catia, they “‘imagined’ missing parts”, paying attention to their mechanical coherence, while using Keyshot to obtain a more visually attractive final result (Paulin and Chambon 2020, 15–16). This questioning also occurred with integration of buildings and machines on Unity: models produced by specialized software are each quite efficient, but too heavy and ill-optimized to be all integrated in the same simulation. Students working on this topic thus have to take and reduce models in order to optimize performance, losing a part of the precision (Bozane 2022, 4–6, 10). Freedom left to students in technical solutions thus made it possible, by authorizing research and free experimentation, to identify configurations most likely to meet the needs of the project as a whole.\n\n\nWhich data model?\nSimilarly, tests “distribution” between students provided insights as to the appropriate type of data model. The Techn’hom Time Machine project was initially supposed to rely on a “classic” relational database. The first student to work on setting up said database quickly realized that a historical database involves “a certain complexity in its design”, necessitating a table for abstract concepts “most difficult to define”, and a table for specifying types of links between actors, but without informing in advance all possible types of relationships (Garcia 2020, 20, 23). In short, the student realized that, for a system as complex as a human society, a relational database quickly shows its limits. In fact, even if this first student still managed to create a relational database, the next two underlined its complexity: “the number of tables in the database makes reading difficult” (Ruff 2022, 7), and it was difficult to “precisely complete [it]” (Marais 2020, 9, 16). A fourth student, tasked to take up the previous work to refine it and make a functional application, concluded with the support of teacher-researchers that this database simply did not allowed to describe precisely enough a historical reality, and pointed the need to use an RDF graph database (Echard 2023, 15–16, 21). This solution, actually adopted, therefore comes once again from a series of works allowing a self-critique of the entire project, helping to define effective solutions.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#reflective-feedback-from-students",
    "href": "submissions/429/index.html#reflective-feedback-from-students",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Reflective feedback from students",
    "text": "Reflective feedback from students\nBeyond these contributions to the scientific project, this program also aims to offer training to students. The point that emerges most clearly from students’ reports, before any technical consideration or skills acquisition, relates to discovery of human sciences and their methodologies.\n\nDiscovering human sciences\nAlmost all of the students emphasize an initial dismay when faced with historical sources, lacking quantity, precision and conciseness of the information expected in an engineering context. Apart from a few immediately relevant sources, the mass of additional documentation, necessary to understand machines operation and context, is much more confusing and time-consuming to analyze, while offering mediocre quality of information. Students have “access to a lot of documents but little precision” (Bogacz 2019, 6, 8), and historical documents often “do not provide as much information as [they] hoped” (Castagno and Vigne 2020, 4). Moreover, students note that, even with good sources, machines “remain much more complex” than diagrams, and no blueprints, which “does not allow the direct connection and understanding of each piece” (Paulin and Chambon 2020, 8). The same goes for buildings, with damaged or partial plans, forcing to “make measurements on the plan to approximate distances” (Le Guilly 2022, 9).\nDespite this initial blockage, students developed solutions - starting with awareness that historical models can never “exactly” reproduce past reality. The most important resource consisted of seeking by themselves complementary sources, like archive originals (Marchal 2021, 5), old films (Castagno and Vigne 2020, 4), or “observations made on site” for buildings (Le Guilly 2022, 10). For machines, two other valuable sources could be mobilized, via contacts obtained by supervising teacher-researchers: dialogue with former workers about machines functioning and details (Bogacz 2019, 7; Paulin and Chambon 2020, 9); and visits in still-working spinning mills. Those experiences allowed them, according to their feedback, to better understand machines, operations but also context, “allowing [them] to take a step back from the project” (Bogacz 2019, 7). On the contrary, students working in the midst of the Covid pandemic, regretted not having been able to have the same experience (Paulin and Chambon 2020, 20). Direct contact with historical elements also include an emotional aspect highlighted by the students: “It was both a very interesting and very pleasant moment. Being able to see with our own eyes the machine that we were trying to reproduce computationally was a very enriching experience”; “The fact of visualizing in real life a machine that we had been modeling for several months is truly incredible” (Bogacz 2019, 7, 16).\nThis need to delve into sources implied for students the discovery, through practice, of the ins and outs of human sciences research. Typically, with data modeling, working from real data brings a certain advantage: working from “concrete cases […] helped us to understand how to articulate [several] ontologies and thus develop a strategy to combine them effectively into a coherent whole” (Echard 2023, 23, 32). Likewise, for buildings, sources comparison led students to perceive inconsistencies, and thus “note the importance of reading all the archives and not just a few because errors may be present” (Pic 2020, 3–4). Some also emphasize “difficulty of exploiting numerous bibliographic resources” in terms of synthesis capacities and working time (Bogacz 2019, 6; Castagno and Vigne 2020, 15), but also the pleasure of “learning to read archives” (Paulin and Chambon 2020, 20). The novelty of the practice compared to classic engineering curriculum is well summed up by one of the teams: “This type of task requires patience and a methodology completely different from what we have habit of doing. The difficulty or even the impossibility of finding the desired information taught us to put ourselves in the shoes of a historian who must at certain times make hypotheses in order to continue his work.” (Castagno and Vigne 2020, 15).\n\n\nProject managing\nWhatever the students’ specific project, it generally appeared to be a first in their training, positioning them as researchers over several months. This induced a “complete autonomy” (Garcia 2020, 8) underlined by all reports, often before competence gains. One, those project was “the most significant project he had to carry out”, “learned the management” of his organization (Bogacz 2019, 16). Another “learned to manage a project in [his] free time” (Le Guilly 2022, 10), and a third “learned to work efficiently and manage projects independently” (Echard 2023, 9, 40). Faced with complex and non-linear projects, students emphasize the “need to do a lot of research to use the right method to work correctly”, and to propose solutions on their own (Marchal 2021, 9–10, 27). The gross volume of work is finally underlined, projects requiring “time to understand the documents, research into software functionalities as well as a considerable investment” (Pic 2020, 16). Participation in the project can appear as “a first professional experience (…) The experience gained during the internship is immense” (Garcia 2020, 39).\nBeyond each individual work, some students also develop reflection on the overall project. In particular, they suffered from a lack of communication with their predecessor on the same subject, “making the task more difficult”, leading to risk of “wasting (…) time understanding what the other had already understood” (Castagno and Vigne 2020, 15). This experience lead to an awareness of the importance of good communication or documentation. Students therefore suggested organizing “video conferences between old and new groups”, and that “each group [should] bring together important documents in a separate file” during project transitions. They applied the lesson to their own report, by “explaining as best as possible what [they] had understood”, with concrete recommendations (Castagno and Vigne 2020, 15–16).",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/429/index.html#conclusion",
    "href": "submissions/429/index.html#conclusion",
    "title": "Training engineering students through a digital humanities project: Techn’hom Time Machine",
    "section": "Conclusion",
    "text": "Conclusion\nStudents involvement in the Techn’hom Time Machine project leads to bidirectional enrichment. The project benefits from the possibility of distributed work and multiple proposal strengths, making it possible to test several options in parallel on a given subject. Students deepen their knowledge of diverse software, while introducing themselves to human sciences and project management. Gain in technical skills is often implied in reports, obviously being an integral part of expectations of any engineering school project. Acquisition of more fundamental knowledge can be identified, with discovery of some entirely new technologies. An interest in the historical dimension is also mentioned, as well as human contacts with researchers and workers. Finally, the very fact of participating in a digital humanities project, atypical in itself, appears as a source of satisfaction.",
    "crumbs": [
      "Abstracts",
      "Training engineering students through a digital humanities project: Techn’hom Time Machine"
    ]
  },
  {
    "objectID": "submissions/431/index.html",
    "href": "submissions/431/index.html",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "",
    "text": "The core data of RAG is based on the university registers. The registers usually contain the names and places of origin of the students as well as the date of enrolment. This data is enriched in the research database with biographical data on subjects studied, professional activities and written works. Since 2020, the RAG has been a sub-project of the umbrella project Repertorium Academicum (REPAC), which is being carried out at the Historical Institute of the University of Bern. See on the project and its developments: (Gubler, Hesse, and Schwinges 2022). Data skills in RAG can be divided into data collection, data entry and data analysis. Different data skills are required in the three areas, which have of course also changed over time as a result of digitalisation. While compiling and analysing data has been simplified by computer-aided processes, the precise recording of data in the database still requires in-depth historical knowledge and human intelligence.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#introduction",
    "href": "submissions/431/index.html#introduction",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "",
    "text": "The core data of RAG is based on the university registers. The registers usually contain the names and places of origin of the students as well as the date of enrolment. This data is enriched in the research database with biographical data on subjects studied, professional activities and written works. Since 2020, the RAG has been a sub-project of the umbrella project Repertorium Academicum (REPAC), which is being carried out at the Historical Institute of the University of Bern. See on the project and its developments: (Gubler, Hesse, and Schwinges 2022). Data skills in RAG can be divided into data collection, data entry and data analysis. Different data skills are required in the three areas, which have of course also changed over time as a result of digitalisation. While compiling and analysing data has been simplified by computer-aided processes, the precise recording of data in the database still requires in-depth historical knowledge and human intelligence.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#project-history",
    "href": "submissions/431/index.html#project-history",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Project history",
    "text": "Project history\nThe RAG started with a Microsoft Access database as a multi-user installation. In 2007, the switch was made to a client-server architecture, with MS Access continuing to serve as the front end and a Microsoft SQL server being added as the back end. This configuration had to be replaced in 2017 as regular software updates for the client and server had been neglected. As a result, it was no longer possible to update the MS Access client to the new architecture in good time and the server, which was running on the outdated MS SQL Server 2005 operating system, increasingly posed a security risk. In addition, publishing the data on the internet was only possible to a limited extent, as a fragmented export from the MS SQL server to a MySQL database with a PHP front end was required. In 2017, it was therefore decided to switch to a new system (Gubler 2020).\n\n\n\nFig. 1: Former frontend of the RAG project for data collection in MS Access 2003.\n\n\nOver one million data records on people, events, observations, locations, institutions, sources and literature were to be integrated in a database migration - a project that had previously been considered for years without success. After a evaluation of possible research environments, nodegoat was chosen (Bree and Kessels 2013). Nodegoat was a tip from a colleague who had attended a nodegoat workshop (Gubler 2021b). With nodegoat, the RAG was able to implement the desired functions immediately:\n\nLocation-independent data collection thanks to a web-based front end.\nData visualisations (maps, networks, time series) are integrated directly into nodegoat, which means that exporting to other software is not necessary, but possible.\nResearch data can be published directly from nodegoat without the need to export it to other software.\n\nFrom then on, the RAG research team worked with nodegoat in a live environment in which the data collected can be made available on the Internet immediately after a brief review. This facilitated the exchange with the research community and the interested public and significantly increased the visibility of the research project. The database migration to nodegoat meant that the biographical details of around 10,000 people could be published for the first time, which had previously not been possible due to difficulties in exporting data from the MS SQL server. On 1 January 2018, the research teams at the universities in Bern and Giessen then began collecting data in nodegoat, starting with extensive standardisation of the data. Thanks to a multi-change function in nodegoat, these standardisations could now be carried out efficiently by all users. Institutions where biographical events took place (e.g. universities, schools, cities, courts, churches, monasteries, courts) were newly introduced.\n\n\n\nFig. 2: Frontend of the RAG project for data collection in nodegoat.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#methodology",
    "href": "submissions/431/index.html#methodology",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Methodology",
    "text": "Methodology\nThese institutions were assigned to the events accordingly, which forms the basis for the project’s method of analysis: analysing the data according to the criteria ‘incoming’ and ‘outgoing’ (Gubler 2022). The key questions here are: Which people, ideas or knowledge entered an institution or space?\n\n\n\nFig. 3: Incoming: Places of origin of students at the University of Basel 1460-1550 with the large dot in the centre as the city of Basel., data: repac.ch, 07/2024.\n\n\nHow was this knowledge shared and passed on there? Spaces are considered both as geographical locations and as knowledge spaces within networks of scholars. In addition, the written works of scholars are taken into account in order to document their knowledge. The people themselves are seen as knowledge carriers who acquire knowledge and pass it on. Consequently, the people are linked to their knowledge in the database using approaches from the history of knowledge (Steckel 2015). The methodology described can therefore not only be used to research the circulation of knowledge between individuals and institutions, but also to digitally reconstruct spheres of influence and knowledge, for example by discipline: Spaces that were shaped by jurists, Physicians or theologians. The map shows places or regions where a particularly large number of Basel jurists were active. The second graphic shows the network of the same group with famous Bonifacius Amerbach as a strong link in the centre. The network is formed based on a Force-directed graph.\n\n\n\nFig. 4: Outgoing: Spheres of activity of jurists with a doctorate from the University of Basel 1460-1550., data: repac.ch, 07/2024.\n\n\n\n\n\nFig. 5: Network: Jurists with a doctorate from the University of Basel 1460-1550., data: repac.ch, 07/2024.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#data-literacy",
    "href": "submissions/431/index.html#data-literacy",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Data literacy",
    "text": "Data literacy\nStudents and researchers working on the RAG project can acquire important data skills. We can make a distinction, as said, between the skills required to collect, enter and analyse the biografical data. Key learning content related to the data entering process for students working in the RAG project are:\n\nBasics of data modelling\n\nBasic knowledge of the use of digital research tools and platforms. Students learn how to design and adapt data structures in order to systematically enter, manage and analyse historical information. They understand how to define entities (such as people, places, events) and their relationships.\n\nBasics of data collection\n\nThe collection of data in a historical project involves several steps and methods to ensure data consistency. In the project, students learn how to search and evaluate sources based on research questions and extract the relevant information. Both quantitative and qualitative approaches are considered in the methods of data collection. An SNSF Spark project provides an example of a quantitative approach on dynamic data ingestion of linked open data in one nodegoat environment (Gubler 2021a)\n\nData entry and management\n\nStudents acquire practical experience in entering and maintaining data within a digital research environment. Additionally, they learn to document workflows and data sources to ensure transparency and traceability. For effective data entry, both students and researchers must develop essential skills related to the extraction and evaluation of historical information.\n\nSource criticism and information extraction\n\nThe project’s most challenging task is extracting relevant biographical information from sources and literature and systematically recording and documenting it in the database according to project-specific guidelines. The goal is to achieve the highest possible standardization to ensure data quality and consistency. Specifically, students must select life events from approximately 900 biographical categories to accurately record an event. These categories are divided into three major blocks: 1) personal data (birth, death, social and geographical origin, etc.), 2) academic data (specializations, degrees), and 3) professional activities. These encompass all potential fields of activity in both ecclesiastical and secular administration in the late Middle Ages. Collecting data and accurately evaluating information from sources and research literature is a demanding task that requires a solid knowledge of history and Latin.\nKey learning content related to data analysis is:\n\nLearning how to query a database. The use of filters and search functions for targeted data analysis requires a solid understanding of the data model, the data collection methodology, and the available content. For an initial overview of the data and, if necessary, for in-depth analysis, AI tools for data analysis will also be used in the project in the future. Such tools can help with data retrieval, as the data can be queried using natural language prompts.\nGeographical and temporal visualisations\n\nUse of GIS functionalities to create and analyse geographical maps. Visualisation of historical data on time axes to show chronological processes and changes.\n\nNetwork analysis\n\nKnowing and applying methods for linking different data sets and for analysing networks and interactions between historical actors such as people, institutions, objects and others. The data can also be exported from nodegoat in order to evaluate it with other visualisation software, for example such as Gephi for network analyses. The graphic shows the general settings in nodegoat for network analyses.\n\n\n\nFig. 6: General settings for network analyses in nodegoat.\n\n\n\nInterpretation of the digital findings (patterns, developments)\n\nThe most important skill in the entire research process is, of course, the ability to interpret the results. The data is always interpreted against the background of the historical context. Without well-founded historical expertise, however, the data cannot provide in-depth insights for historical research, but at best enable superficial observations. It follows that when working with research data, a double source criticism must always take place: when obtaining the information from the sources (data collection) and when analysing the digital results obtained from the information (data interpretation).",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#digitisation",
    "href": "submissions/431/index.html#digitisation",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Digitisation",
    "text": "Digitisation\nHow have the described data competences changed since the start of the project in 2001? This question is linked to changes in the research infrastructure, the availability of digitised material (sources and literature) and with the question of how computer-aided automation, in particular, artificial intelligence have influenced and will influence the practices of data collection, entry and analysis in the project, expanding the epistemological framework? The most important factors in connection with digitalisation in general are:\n\nResources: The increasing availability of digitized texts, particularly through Google Books, has significantly transformed prosopographical and biographical research. Not only is a wealth of information more accessible today, but it can also be entered into databases more efficiently. Consequently, skills for digital research and information processing have had to be continuously adapted throughout the course of the project.\nTools: Since the start of the project, new software tools have significantly transformed the processes of collecting, extracting, entering, and analyzing information. The most substantial development has been in data analysis, which, thanks to advanced tools and user-friendly graphical interfaces, has become accessible to a wide range of researchers, no longer being limited to computer scientists. AI tools for data analysis also open up huge potential for data analysis. Large amounts of data can be analyzed in a short time using simple query languages. However, when using AI, the results must be examined even more critically than with conventional data analysis.\nData analysis: The visualization of research data in historical studies has seen significant advancements. For instance, data can now be displayed on historical maps, within networks, or in time series, and dynamically over time using a time slider in a research environment like nodegoat. This has accelerated data analysis: tasks like creating a map, which took weeks in the early years of the RAG project, now take only a few minutes.\nInterpretation of the data: The core method of historical scholarship, source criticism, has also evolved significantly. While it traditionally involved evaluating information from sources and literature, today it also requires the ability to analyze data visualizations and network representations derived from these sources. To adequately assess these digital findings, a thorough understanding of the data model, data context, and historical background is essential. Consequently, data analysis presents new challenges for historical research, necessitating advanced data competencies at multiple levels.\nCollaboration: Web-based research environments have made collaboration much easier and more transparent. Teams are now able to follow each other’s progress in real time, making the location of the work less important and communication smoother.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#human-and-artificial-intelligence",
    "href": "submissions/431/index.html#human-and-artificial-intelligence",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Human and artificial intelligence",
    "text": "Human and artificial intelligence\nRegarding data collection, entry, and analysis, artificial intelligence significantly impacts several, though not all, tasks within the RAG project.\n\nData collection: AI supports the rapid processing and pre-sorting of digital information used for data collection. For example, Transkribus is utilized to create OCR texts, which are then directly imported into nodegoat and matched with specific vocabularies using algorithms (Gubler 2023). This technology aids the RAG project by efficiently detecting references to students and scholars within large text corpora, significantly speeding up the identification and extraction process.\n\n\n\n\nFig. 7: Example settings for the algorithm for reconciling textual data in nodegoat.\n\n\n\nData entry: In this area, human intelligence remains crucial. In-depth specialist knowledge of the historical field under investigation is essential, particularly concerning the history of universities and knowledge in the European Middle Ages and the Renaissance. Due to the heterogeneous and often fragmented nature of the sources, AI cannot yet replicate this expertise. The nuanced understanding required to interpret historical events and their semantic levels still necessitates human insight.\nData analysis: While AI support for data entry is still limited, it is much greater for data analysis. The epistemological framework has expanded considerably not only in digital prosopography and digital biographical research, but in history in general. Exploratory data analysis in particular will become a key methodology in history through the application of AI.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/431/index.html#conclusion",
    "href": "submissions/431/index.html#conclusion",
    "title": "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)",
    "section": "Conclusion",
    "text": "Conclusion\nSince the 1990s, digital resources and tools have become increasingly prevalent in historical research. However, skills related to handling data remain underdeveloped in this field. This gap is not due to a lack of interest from students, but rather stems from a chronic lack of available training opportunities. This situation has gradually improved in recent years, with a growing number of courses and significant initiatives promoting digital history. Nevertheless, the responsibility now lies with academic chairs to take a more proactive role in integrating a sustainable range of digital courses into the general history curriculum. It is crucial that data literacy becomes a fundamental component of the training for history students, particularly considering their future career prospects and the increasingly complex task of evaluating information, including the critical use of artificial intelligence methods, tools and results. Especially with regard to the methodology of source criticism, which is now more important than ever in the evaluation of AI-generated results. In addition to formal teaching, more project-based learning should be offered to support students in acquiring digital skills.",
    "crumbs": [
      "Abstracts",
      "From manual work to artificial intelligence: developments in data literacy using the example of the Repertorium Academicum Germanicum (2001-2024)"
    ]
  },
  {
    "objectID": "submissions/405/index.html",
    "href": "submissions/405/index.html",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "",
    "text": "Data-driven approaches bring extensive opportunities for research to analyze large volumes of data, and gain new knowledge and insights. This is considered especially beneficial for implementation in the humanities and social sciences (Weichselbraun et al. 2021). Application of data-driven research methodologies in the field of history requires a sufficient source base, which should be accurate, transparently shaped and large enough for robust analysis (Braake et al. 2016). Web archives preserve valuable resources that can be drawn upon to analyze the development of the websites and even the whole domains through the decades, and provide access to them (Brügger 2018). At first glance, the volumes of data captured are impressive and suggest the opportunity for big data research practices. For example, the Web Crawls collection of the Internet Archive alone includes 80.2 PB of data (Internet Archive, n.d.c). At the same time, the web-archived collections expose a set of other characteristics relevant to big data and this can be challenging for their efficient use. Such features include, for instance, a high level of velocity, exhaustive in scope and diverse in variety (Kitchin 2014), which require addressing and resolving specific issues. This research focuses on museums’ presence on the web, describes opportunities for implementation of data-driven research, and identifies challenges faced by researchers. In particular, in the paper the opportunity to extract data, to investigate the complexity of structure of the archived websites, and to analyze the content are addressed. At the same time, the findings are relevant to other studies devoted to the use of the archived web in computational research in the humanities.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#introduction",
    "href": "submissions/405/index.html#introduction",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "",
    "text": "Data-driven approaches bring extensive opportunities for research to analyze large volumes of data, and gain new knowledge and insights. This is considered especially beneficial for implementation in the humanities and social sciences (Weichselbraun et al. 2021). Application of data-driven research methodologies in the field of history requires a sufficient source base, which should be accurate, transparently shaped and large enough for robust analysis (Braake et al. 2016). Web archives preserve valuable resources that can be drawn upon to analyze the development of the websites and even the whole domains through the decades, and provide access to them (Brügger 2018). At first glance, the volumes of data captured are impressive and suggest the opportunity for big data research practices. For example, the Web Crawls collection of the Internet Archive alone includes 80.2 PB of data (Internet Archive, n.d.c). At the same time, the web-archived collections expose a set of other characteristics relevant to big data and this can be challenging for their efficient use. Such features include, for instance, a high level of velocity, exhaustive in scope and diverse in variety (Kitchin 2014), which require addressing and resolving specific issues. This research focuses on museums’ presence on the web, describes opportunities for implementation of data-driven research, and identifies challenges faced by researchers. In particular, in the paper the opportunity to extract data, to investigate the complexity of structure of the archived websites, and to analyze the content are addressed. At the same time, the findings are relevant to other studies devoted to the use of the archived web in computational research in the humanities.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#data-driven-research-of-the-museums-web-presence",
    "href": "submissions/405/index.html#data-driven-research-of-the-museums-web-presence",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "Data-Driven Research of the Museums’ Web Presence",
    "text": "Data-Driven Research of the Museums’ Web Presence\nProliferation of digital technologies and the World Wide Web have profoundly impacted museums, transforming their functions and engagement practices. To comprehend these changes, a thorough examination of museum websites and their historical evolution is essential. The focus of this research is on the application of a data-driven approach to the history of the Metropolitan Museum of Art (MET) and the National Museum of Australia (NMA). The cases were selected from two publicly available web archives – the Internet Archive (Internet Archive, n.d.b) and the Australian Web Archive, Trove (Trove, n.d.a) – that are the oldest web archives which means they preserve the historical web from the same starting point, making it possible to compare their infrastructures and their use in research. When working with web archives, the main strategies to apply data-driven methods to research the websites’ history refer to obtaining data from the general pool or using special collections. In the first scenario, scholars obtain data captured during the general crawling process which is not specifically curated. In this case, there is a large chance that the website is not crawled systematically and in terms of the full depth of hierarchy of the pages. The second scenario assumes more rigorous preservation practices that result in a more comprehensive dataset. Analyzing the MET and the NMA, the researcher may use different approaches to obtaining data. Studying the MET’s history on the web, the researcher can search for data from the Internet Archive and also from the special collection devoted to the MET in the Archive-It project (Archive-It, n.d.b). However, the special collection was only initiated in 2019. To study the previous years the researcher needs to necessarily apply to the general pool of the Internet Archive. To investigate the web presence of the NMA scholars may mainly observe the data from the Internet Archive and from Trove as there is no special collection of the preserved web devoted to the NMA. Both web archives offer open APIs to obtain large datasets suitable for data-driven research (Internet Archive, n.d.a; Trove, n.d.b). However, even obtaining data is challenging. The statistics of the Internet Archive show that the preserved version of the MET museum website (The Metropolitan Museum of Art, n.d.b) was saved 20,519 times between November 11, 1996, and July 28, 2023, including 10,867,395 captures of text/HTML files, corresponding to 6,559,761 URLs (Wayback Machine, n.d.a). The statistics for the National Museum of Australia on the Internet Archive show 353,134 captures of text/HTML files, which relates to 175,999 URLs (Wayback Machine, n.d.b). At the same time, the attempt to download all the timestamps using the Wayback Machine Downloader returns only 1,342,067 files for the MET website. The same is relevant for the NMA website obtained from the Internet Archive. The stage of obtaining the dataset requires more attention to the APIs and gaining reliable data. Difficulties related to obtaining data and building datasets have been addressed partially by creating research infrastructures to work with web archived materials. The Internet Archive introduced several initiatives to collect, store and provide access to preserved materials and process data such as Archive-It (Archive-It, n.d.a) and ARCH (Archives Research Compute Hub (Archives Research Compute Hub (ARCH), n.d.)). The GLAM Workbench (GLAM Workbench, n.d.c) has been created to analyze materials from the Australian Web Archive, the Internet Archive and several other web archives. Initially focused on Australia and New Zealand digital platforms the GLAM Workbench suggests a range of solutions based on the use of Jupyter notebooks for exploration and usage of data from GLAM institutions including web archival data. These infrastructures support researchers in finding solutions of various problems in obtaining and processing data, opening them up to wide opportunities to explore the archived web. Regarding the topic of museums on the web, the GLAM Workbench is particularly valuable because some examples in the notebooks have already focused on the Australian web domain and the code from the notebooks can be easily transformed for addressing the topic related to the NMA’s web presence. Using these research infrastructures is beneficial also for solving some technical issues related to the limited capacity of personal computers to address large amount of data (Robertson 2022). Data-driven approaches require not only obtaining information in as complete as possible form but also assessment of the available data, which can be considered as a step in source criticism. Analysis of the distribution of data can be based on a URL analysis of data preserved on the web archives. The URL analysis serves as a necessary step in the source assessment because its study can reveal the temporal distribution of the data, identify the gaps, trace the regularity of crawling and updating the website, specify the distribution of the file formats, and identify other characteristics related to the complexity of the websites and their changes over time. URL-string after crawling becomes a part of the identification of information in the web archive, being enriched with a timestamp. URLs collected from the web archive provide a series of characteristics such as protocol, domain and subdomain names, path, timestamps, and parameters which are all valuable resources of information. Some of these parameters are more important for tracing the technical side of the website’s history. For example, tracing the protocols http and https give us an idea about accepting of security measures and technological upgrades. Some other characteristics provide valuable information to study the content of the website. Identification of the subdomains contributes to our understanding complexity and segmentation of the museum’s website and the presentation of information to the website’s visitors. Often the subdomains have been used for specific projects within the museum’s activities and can be studied separately from the ‘main’ website due to their own structure and content. Both of the considered case studies through their history included the subdomain structures and experimented over the years to find more suitable approaches to the complexity of the website. Subdomains could have a different design and non-overlapping content so that they can be located as a large web structure within the museums’ activities. The analysis of the subdomains facilitates reconstruction of their life cycle, sustainability and use in comparison with the main domain. Building the network of the domain and the subdomains serves as a way to identify the important webpages through the most connected nodes (web pages) and the edges (hyperlinks). In this regard, the main website performs as a metastructure that encompasses various substructures (such as subdomains). Therefore, data-driven approaches are helpful in the analysis of functional segmentation, autonomy and integration processes within such a museum’s web universe. The challenge in the identification of the subdomains refers not only to the inherent incompleteness of preserved data but also to the deficient methods of obtaining data from the web archives. Our experience shows that the API of the Internet Archive deriving the millions of URLs from the same domain returns errors and collapses the processing. Also, we are able to get some subdomains of the metmuseum.org website from the Archive-It platform (The Metropolitan Museum of Art Web Archive, 2024) but the MET preserves the data systematically only from 2019 and for this reason identification of the subdomains from the past perspective can be challenging and researchers need to seek better ways to achieve access this data. The GLAM Workbench has a notebook in relation to obtaining subdomains. The code can also be adjusted to any web domain for searching on the Internet Archive (and some other web archives). Regarding the research of the NMA and subdomains, the GLAM workbench suggests a highly powerful tool to consider the NMA’s website as a part of the gov.au domain (GLAM Workbench, n.d.a). The sub-subdomains of nma.gov.au website can be analyzed around the main museum’s website and at the same time the museum’s website can be considered in connection with other websites from the gov.au domain. Such an approach has a strong potential for discoveries related to positioning of the museums’ website along with the other 1825 third-level domains of the governmental segment, identifying the unique webpages and other characteristics. The archived web is a complex resource that encompasses a large amount of heterogeneous data (Brügger and Finnemann 2013). The single webpage may include various formats of information and analysing the whole website requires finding the appropriate methods. Deducing complexity and investigation data separated according to formats is a widely accepted method of analysis of the website’s content. Textual analysis is a subversion of such research on the websites when only the texts are taken for the study. Building a corpus of texts from the museum websites preserved on the Danish Web Archive gave insights into the development of the Danish museums on the web and the identification of the attributes specific to the museum clusters (Skov and Svarre 2024). Separating the content according to the formats and selecting the particular type of data for the analysis, may appear to be a simple task. However, building a corpus is a very complex task which requires defining appropriate approaches to how to obtain data and what type of data to include in the corpus. There are many pitfalls to consider: the transfer of dynamic web content to the static version on the web archive inescapably changes the nature of the data and requires decisions on how to shape the dataset for analysis (Brügger 2010). Moreover, not all the textual data represent the same level of data. Another issue is the multiplication of data when the same page has been crawled and preserved on the web archive several times. The Internet Archive and the GLAM Workbench suggest different solutions in this regard. The Internet Archive provides users with unique identifiers (‘digest’) of every captured URL. If the content on the same URL has been changed, the hash sum and subsequently the identifier will vary as well. It helps to treat the web pages differently if their content is diverse. At the same time, the significance of the changes cannot be assessed from a distance. The Glam Workbench suggests a code published on the Jupyter Notebook to harvest textual data from the required archived webpages (GLAM Workbench, n.d.b). At the same time, the obtaining of data is possible by the lists of the URLs, which aids in treating the large amount of webpages automatically. Textual data analysis is a well-established sphere of computational humanities. However, the complexity of the website is significantly greater than the text only. Analysis of images can be beneficial for many reasons. One such task is to reveal the selection processes in publishing images on the websites in general and in the digital collections in particular. Art museums had to identify the priorities in publishing pictures and develop specific strategies for that. We do not know much about selection processes, especially in the early years of the web, and how these solutions evolved due to the influences of political and cultural events, movements and actions. Data-driven research is able to identify and highlight these trends. To analyze the currently published collections some museums suggest open APIs for obtaining metadata about the museum objects. Both the MET and the NMA provide open access to their collections through the open API (The Metropolitan Museum of Art, n.d.a; The National Museum of Australia, n.d.). Access to the metadata of the publicly available objects is provided on the digital platforms. At the same time, the metadata is limited to information about the objects and does not include metadata regarding their web presence, including the date of the first publication online. In this regard, the timestamps from the web archives can be considered as a valuable resource to analyze the publishing processes from a historical perspective. At the same time, in the web archive the preserved pictures are disconnected from the metadata about the image and this gap requires finding specific solutions to connect the image and metadata to make the discoveries easier. Apart from the texts and images, the websites incorporate other formats of data and their use in the research is more problematic for analysis. The museums represented on the web multimedia content including videos, animation, conducted podcasts, etc. All of this and other content is valuable for our understanding of their evolution on the web. At the same time, these types of content are very challenging for web archiving (Müller-Budack et al. 2021), so the specific methodologies should be developed for their systematic preservation and then for the subsequent analysis, including data-driven practices.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/405/index.html#conclusion",
    "href": "submissions/405/index.html#conclusion",
    "title": "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries",
    "section": "Conclusion",
    "text": "Conclusion\nWeb archives provide wide opportunities for the implementation of data-driven research in the analysis of museums’ web presence. The archived websites require a thorough source criticism and evaluation of the available data for gaining new insights into the evolution of museums’ activities online. Studying the cases of the MET and the NMA is possible via a large amount of data preserved on the Internet Archive and Trove. However, the robust analysis is challenging due to various factors. Researchers need to investigate new ways to obtain the data from the web archives, identify incompleteness and biases, to evaluate data and diversity of the file formats, and to select the best approaches to address them. Analysis of web content remains challenging and requires the development of innovative solutions to exploit data-driven research. At the same time, some of the issues can be gradually resolved based on the developing tools and digital research infrastructures, first of all, ARCH and the GLAM Workbench. Ultimately, data-driven research on the museums’ web presence has a great potential for new discoveries but at the same time, it is a complex endeavor.",
    "crumbs": [
      "Abstracts",
      "Data-Driven Approaches to Studying the History of Museums on the Web: Challenges and Opportunities for New Discoveries"
    ]
  },
  {
    "objectID": "submissions/427/index.html",
    "href": "submissions/427/index.html",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "",
    "text": "Nowadays, software is a central component of every research project. Since the establishment of personal computers digital tools are used for a wide range of tasks, from simple text processing to machine assisted recognition in all sorts of historical documents. Research projects however, in particular those that produce digital scholarly editions, rarely rely just on existing tools, they often create new ones. Starting from the development or customization of own data formats ending with the implementation of often complex web applications for presentation, it is not uncommon for the tools developed in this context to be ‘quick hacks’ rather than well-designed software projects.1 In many cases, this is not a problem at all, because the duration of research projects in the humanities is often rather short (e.g. between three and six years). Software developed in such a short amount of time must first and foremost achieve the project’s goals, and therefore adaptation to other subjects or subsequent use is usually not intended. However, this becomes a problem if the corresponding research project is scheduled for a longer term, or if it is part of a series of projects depending on each other. In this case, quick solutions often become serious issues and are not really FAIR for either internal or external subsequent use. Not least for this reason, this phenomenon is the subject of discussion in the digital humanities community under the heading of research software engineering.2 This paper describes practical experiences from the perspective of a long-term editorial project and explores opportunities for sustainable development practices by utilizing modern methods that have long been established outside the academic world.\n\n\n\nThe Collection of Swiss Law Sources (SLS) is a 120 year old research project that publishes Swiss legal texts in German, French, Latin, Italian and Romansh from the 6th to the 18th century. The edited texts are published in a printed reference publication and in digital form.3 By the time of writing ten edition projects are currently carried out by 23 researchers in three languages throughout Switzerland: In French, volumes are to be published in the cantons of Geneva (1 vol.), Vaud (2 vols.) Neuchatel (1 vol.), Fribourg (1 vol.); in German Valais (1 vol.), Lucerne (2 vols.), Schaffhausen (2 vols.), St. Gallen (1 vol.), Graubünden (1 vol.) and in Italian Ticino (1 vol.). Further editions projects are planned or applied for, while the overall project is scheduled for another ~ 50 years. The entire technical infrastructure is provided and developed by the SLS core team, which consists of the project manager and two members of staff specializing in DH (the authors of this paper). This team is also responsible for coordinating the projects, processing the data, typesetting the printed volumes and digital publishing of the edited texts.\nIn this context, the development of new and the improvement of already existing software is not only a technical challenge, but also an organizational one. Existing applications must run continuously to provide the researchers with the tools they need for their daily work (and to grant the users of the digital edition access to all information), while new requirements must be met on an ongoing basis as each project deals with unique documents.\n\n\n\nAbout 15 years ago the Swiss Law Foundation, which stands behind the SLS, decided to retro-digitize the over hundred volumes published up to that point. Since then, the results of these initial digitization efforts have been presented in a web application which, as a ‘browsing machine’, makes the results of the many years of editing work, previously locked between two book covers, available to a broad public. This also marked the start of the project’s transition to a predominantly digital editing and working method. In these 15 years numerous (web) applications have been created: These include databases that collate information on historical entities (people, organizations, places and terms), a digital application that presents the transcriptions, now encoded in TEI-XML, in both a web and a print view and a lot of other tools used in the various tasks at hand. The ongoing nature of the project was one of the reasons why many of these applications were often ‘ad hoc solutions’ or proof of concepts that were neither designed for long-term operation nor for integration—i.e. collaboration—with other tools. As a result, a rather diverse ecosystem of different technologies has developed on the data side and on the processing and presentation side.4",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#introduction",
    "href": "submissions/427/index.html#introduction",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "",
    "text": "Nowadays, software is a central component of every research project. Since the establishment of personal computers digital tools are used for a wide range of tasks, from simple text processing to machine assisted recognition in all sorts of historical documents. Research projects however, in particular those that produce digital scholarly editions, rarely rely just on existing tools, they often create new ones. Starting from the development or customization of own data formats ending with the implementation of often complex web applications for presentation, it is not uncommon for the tools developed in this context to be ‘quick hacks’ rather than well-designed software projects.1 In many cases, this is not a problem at all, because the duration of research projects in the humanities is often rather short (e.g. between three and six years). Software developed in such a short amount of time must first and foremost achieve the project’s goals, and therefore adaptation to other subjects or subsequent use is usually not intended. However, this becomes a problem if the corresponding research project is scheduled for a longer term, or if it is part of a series of projects depending on each other. In this case, quick solutions often become serious issues and are not really FAIR for either internal or external subsequent use. Not least for this reason, this phenomenon is the subject of discussion in the digital humanities community under the heading of research software engineering.2 This paper describes practical experiences from the perspective of a long-term editorial project and explores opportunities for sustainable development practices by utilizing modern methods that have long been established outside the academic world.\n\n\n\nThe Collection of Swiss Law Sources (SLS) is a 120 year old research project that publishes Swiss legal texts in German, French, Latin, Italian and Romansh from the 6th to the 18th century. The edited texts are published in a printed reference publication and in digital form.3 By the time of writing ten edition projects are currently carried out by 23 researchers in three languages throughout Switzerland: In French, volumes are to be published in the cantons of Geneva (1 vol.), Vaud (2 vols.) Neuchatel (1 vol.), Fribourg (1 vol.); in German Valais (1 vol.), Lucerne (2 vols.), Schaffhausen (2 vols.), St. Gallen (1 vol.), Graubünden (1 vol.) and in Italian Ticino (1 vol.). Further editions projects are planned or applied for, while the overall project is scheduled for another ~ 50 years. The entire technical infrastructure is provided and developed by the SLS core team, which consists of the project manager and two members of staff specializing in DH (the authors of this paper). This team is also responsible for coordinating the projects, processing the data, typesetting the printed volumes and digital publishing of the edited texts.\nIn this context, the development of new and the improvement of already existing software is not only a technical challenge, but also an organizational one. Existing applications must run continuously to provide the researchers with the tools they need for their daily work (and to grant the users of the digital edition access to all information), while new requirements must be met on an ongoing basis as each project deals with unique documents.\n\n\n\nAbout 15 years ago the Swiss Law Foundation, which stands behind the SLS, decided to retro-digitize the over hundred volumes published up to that point. Since then, the results of these initial digitization efforts have been presented in a web application which, as a ‘browsing machine’, makes the results of the many years of editing work, previously locked between two book covers, available to a broad public. This also marked the start of the project’s transition to a predominantly digital editing and working method. In these 15 years numerous (web) applications have been created: These include databases that collate information on historical entities (people, organizations, places and terms), a digital application that presents the transcriptions, now encoded in TEI-XML, in both a web and a print view and a lot of other tools used in the various tasks at hand. The ongoing nature of the project was one of the reasons why many of these applications were often ‘ad hoc solutions’ or proof of concepts that were neither designed for long-term operation nor for integration—i.e. collaboration—with other tools. As a result, a rather diverse ecosystem of different technologies has developed on the data side and on the processing and presentation side.4",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#data-as-a-solid-ground-developing-an-xml-schema-for-a-scholarly-edition",
    "href": "submissions/427/index.html#data-as-a-solid-ground-developing-an-xml-schema-for-a-scholarly-edition",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "Data as a solid ground: developing an XML Schema for a scholarly edition",
    "text": "Data as a solid ground: developing an XML Schema for a scholarly edition\nThe foundation of a digital scholarly edition is undoubtedly the transcribed and annotated data, which usually is encoded in an XML format.5 All our newly edited texts are encoded in XML and as time permits all previously edited texts will be converted to this format. Therefore all further application layers, such as web presentation or printed output, have to be based on these XML files according to the single source principle. Over the last two decades, the guidelines of the Text Encoding Initiative (TEI)6 have established themselves as the de facto standard for this markup work. These guidelines are primarily a broad collection of suggestions rather than a clear set of rules, necessitating a precise formulation of philological concepts into a logical data model, specifically the creation of a TEI subset as an XML schema. The TEI itself offers a format called ODD (One Document Does it all) for creating an XML schema in a literate programming-fashion7, which itself is TEI-XML.8\nA schema’s main use case is validation, i.e. checking whether the XML data corresponds to certain structures and constraints. As a TEI subset it defines which components and elements provided by the TEI guidelines are used and how they are used, making it an important part of the editing concept itself. The validation against a schema ensures the consistency of the resulting data sets in an ongoing project and is necessary to continuously support and check the researchers during the transcription and annotation process. Therefore we regard an XML schema as a key software component, although the development of a schema is typically not understood as software development in the true sense of the word. This is probably one of the reasons why most of the modern engineering practices we want to demonstrate are not yet applied in this field (at least as far as we know).\n\nFour modern engineering practices and their application\nIn order to deal with a complex situation, as described above, the authors of this paper propose to make use of the following software engineering practices9:\n\nmodular software development\ntest driven development\nsemantic versioning\nsemiautomatic documentation\n\nThe development of the XML schema used in our project will be used as an example to show how these practices can be utilized for digital humanities projects. In the context of the ongoing reworking of the SLS application landscape, we developed a test based and modular workflow (see Figure 1) for the creation of a new schema, based on ODD-files as input.10\n\n\n\n\n\n\nFigure 1: Test and build pipeline of a modern schema development workflow\n\n\n\n\n\nModular software development\nIf you download a sample ODD file from the TEI homepage11 which contains all elements and components, such a file may be made up of 70000 lines of code. Our ODD file—which is just a limited subset—still contains way over 20000 lines of code. The first step to handle such a large and complex object is to split it into manageable pieces. For each TEI-element we need, we created a separate file containing the element’s specification. Common parts like attribute classes, data types or custom definitions that are used by multiple elements each went into their own files.\nA rather simple specification for the element &lt;pc/&gt; may look like this:\n&lt;elementSpec\n  xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:rng=\"http://relaxng.org/ns/structure/1.0\" ident=\"pc\" module=\"analysis\" mode=\"change\"&gt;\n  &lt;desc xml:lang=\"en\" versionDate=\"2024-04-30\"&gt;\n    Contains a punctuation mark, which is processed specially\n    considering linguistic regulations (for example, by adding a space).\n  &lt;/desc&gt;\n  &lt;classes mode=\"replace\"/&gt;\n  &lt;content&gt;\n    &lt;rng:data type=\"string\"&gt;\n      &lt;rng:param name=\"pattern\"&gt;[;:?!]&lt;/rng:param&gt;\n    &lt;/rng:data&gt;\n  &lt;/content&gt;\n  &lt;attList&gt;\n    &lt;attDef ident=\"force\" mode=\"delete\"/&gt;\n    &lt;attDef ident=\"unit\" mode=\"delete\"/&gt;\n    &lt;attDef ident=\"pre\" mode=\"delete\"/&gt;\n  &lt;/attList&gt;\n&lt;/elementSpec&gt;\nThis principle of atomicity enforces a clear structure, provides better maintainability in the future, made the files way more easy to grasp and to modify and had also the benefit of reducing redundancy, because shared parts were refactored and can be used throughout the schema while being defined in one place. The downside to this, of course, is the need to compile all those files into one ODD in a separate step. But this may be a small price to pay for the benefits.\n\n\nTest driven development (TDD)12\nThe second step was to define a set of tests for all element, attribute and datatype definitions.13 Each test set describes the expected behavior of a piece of the schema and consists of three components: a title of the test set, the markup being tested and the expected result, which can either be valid (True) or invalid (False). Each test set is executed and evaluated by a Python function which invokes an XML-Parser.\nThe following tests describe the contents and attributes of the element &lt;pc/&gt;.\n@pytest.mark.parametrize(\n    \"name, markup, result\",\n    [\n        (\n            \"valid-pc\",\n            \"&lt;pc&gt;;&lt;/pc&gt;\",\n            True,\n        ),\n        (\n            \"invalid-pc-with-wrong-char\",\n            \"&lt;pc&gt;-&lt;/pc&gt;\",\n            False,\n        ),\n        (\n            \"invalid-pc-with-attribute\",\n            \"&lt;pc unit='c'&gt;;&lt;/pc&gt;\",\n            False,\n        ),\n    ],\n)\ndef test_pc(\n    test_element_with_rng: RNG_test_function,\n    name: str,\n    markup: str,\n    result: bool,\n):\n    test_element_with_rng(\"pc\", name, markup, result, False)\nIf each specification is coupled with one or more tests, it is guaranteed that individual changes to the schema will not compromise the overall functionality and possible side-effects may be detected early on. Such test cases are abstract enough to enable representative testing of the software components to be developed, but at the same time concrete enough to make them readable for employees specializing in philology, thus they can be used as a means of communication between the digital humanist team and the philological or historical team. We can simply ask: Should this piece of XML be True or False?\n\n\nSemiautomatic documentation\nThe schema has to be documented for those who use it to encode the files as well as for those who use the files for any other purpose. We decided to generate as much of this documentation automatically as possible using markdown as a language and a site generator called MkDocs14. Our documentation website15 is constructed like this: A self written Python program reads all parts of the schema, converts them to markdown files and hands those to the MkDocs processor which returns a static HTML webpage that can easily be accessed and searched.\n\n\nSemantic versioning (SemVer and git)\nIt is obvious that each change to the schema not only affects the XML files to be validated16, but also changes the documentation. For this purpose every release of the schema is versioned with git and is reflected in a new corresponding build of the documentation site. All versions of the schema are named in accordance to the principles of semantic versioning17 so a user of any XML file that has to be validated against our schema can see which versions are available and is able to read a specific documentation for any schema version.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#a-brief-outlook",
    "href": "submissions/427/index.html#a-brief-outlook",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "A brief outlook",
    "text": "A brief outlook\nAlthough our journey of refactoring has just begun, we are already seeing the benefits of the principles we have applied. If the ground your are standing on is a solid one, you can build on it. Currently, we are working on a multilingual translation of our schema from German as the main language to English, French and Italian and hope to enrich the schema with extensive examples from actual XML files. Furthermore, we are rewriting the existing rendering-mechanisms (e.g. TEI to HTML), applying the same rules as described above. All in all, the work done and the cost we had to pay for is already paying off.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#references",
    "href": "submissions/427/index.html#references",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "References",
    "text": "References\n\n\nAlsaqqa, Samar, Samer Sawalha, and Heba Abdel-Nabi. 2020. “Agile Software Development: Methodologies and Trends.” International Journal of Interactive Mobile Technologies (iJIM) 14 (11): 246–70. https://doi.org/10.3991/ijim.v14i11.13269.\n\n\nBurghardt, Manuel, and Claudia Müller-Birn. 2019. “Software Engineering in Den Digital Humanities 2. Workshop Der Fachgruppe Informatik Und Digital Humanities (InfDH).” In 50 Jahre Gesellschaft Für Informatik - Informatik Für Gesellschaft Workshopbeiträge Der 49. Jahrestagung Der Gesellschaft Für Informatik: 23.-26.9.2019, Kassel, Deutschland, 75. Proceedings, volume 295. Bonn Gesellschaft für Informatik e.V. [2019].\n\n\nCarver, Jeffrey C., Nic Weber, Karthik Ram, Sandra Gesing, and Daniel S. Katz. 2022. “A Survey of the State of the Practice for Research Software in the United States.” PeerJ Computer Science 8 (May): e963. https://doi.org/10.7717/peerj-cs.963.\n\n\nChristie, Tom. 2024. “MkDocs. Project Documentation with Markdown.” 2024. https://www.mkdocs.org.\n\n\nHaaf, Susanne, and Christian Thomas. 2016. “Enabling the Encoding of Manuscripts Within the DTABf: Extension and Modularization of the Format.” Journal of the Text Encoding Initiative, December. https://doi.org/10.4000/jtei.1650.\n\n\nKnuth, Donald Ervin. 1992. Literate Programming. CSLI Lecture Notes, no. 27. Stanford, Calif.: Center for the Study of Language; Information.\n\n\nLaw Sources Foundation of the Swiss Lawyers Society. 2024a. “Collection of Swiss Law Sources Online. Editio.” 2024. https://editio.sls-online.ch.\n\n\n———. 2024b. “Transkriptionsrichtlinien Und Dokumentation. SSRQ Dokumentation.” 2024. https://schema.ssrq-sds-fds.ch/latest/.\n\n\nMartin, Robert C., ed. 2009. Clean Code: A Handbook of Agile Software Craftsmanship. Upper Saddle River, NJ: Prentice Hall.\n\n\nNeuber, Frederike. 2023. “Der Digitale Editionstext. Technologische Schichten, ‚Editorischer Kerntext‘ Und Datenzentrierte Rezeption.” In Der Text Und Seine (Re)produktion, edited by Niklas Fröhlich, Bastian Politycki, Dirk Schäfer, and Annkathrin Sonder, 55:69–84. Beihefte Zu Editio. Berlin/Boston.\n\n\nPolitycki, Bastian, Christian Sonder, and Pascale Sutter. 2024. “TEI-XML Schema Der Sammlung Schweizerischer Rechtsquellen.” https://doi.org/10.5281/zenodo.10625840.\n\n\nPorter, Dot. 2024. “What Is an Edition Anyway? My Keynote for the Digital Scholarly Editions as Interfaces Conference, University of Graz.” July 25, 2024. http://www.dotporterdigital.org/what-is-an-edition-anyway-my-keynote-for-the-digital-scholarly-editions-as-interfaces-conference-university-of-graz/.\n\n\nPreston-Werner, Tom. 2023. “Semantic Versioning 2.0.0.” 2023. https://semver.org.\n\n\nText Encoding Initiative. 2024a. “Guidelines. TEI: Text Encoding Initiative.” 2024. https://tei-c.org/release/doc/tei-p5-doc/en/html/index.html.\n\n\n———. 2024b. “Roma. TEI: Text Encoding Initiative.” 2024. https://roma.tei-c.org.\n\n\nZundert, Joris van, and Tara Andrews. 2018. “What Are You Trying to Say? The Interface as an Integral Element of Argument.” In Digital Scholarly Editions as Interfaces, 3–33. Norderstedt.",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/427/index.html#footnotes",
    "href": "submissions/427/index.html#footnotes",
    "title": "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCarver et al. recently demonstrated this with a survey, which shows that many researchers developing software have never received training in software development, and best practices are often ignored. See Carver et al. (2022).↩︎\nSee Manuel Burghardt and Claudia Müller-Birn organised a workshop specifically on this topic at the 50th Annual Conference of the German Informatics Society, see Burghardt and Müller-Birn (2019).↩︎\nSee Law Sources Foundation of the Swiss Lawyers Society (2024a) for the web presentation.↩︎\nThe edited texts themselves are available as PDF (the retro-digitized collection), TeX and FileMaker (transition phase) and TEI-XML (current projects). These are processed by scripts and applications in the programming languages Perl, OCaml, Python, JavaScript and XQuery. Relational as well as graph-based and document-orientated databases are used to store the entity data.↩︎\nThere have been various discussion what’s the key value of a digital scholarly digition. Maybe it’s the data (see Porter (2024)) or it’s the interface (see Zundert and Andrews (2018)). In the recent time it’s becoming more and more clear, it could be both. Therefore models have been developed, which understand scholarly editions as a stack of data, the processing applied to it and the resulting presentation (see Neuber (2023), p. 71).↩︎\nFor details see Text Encoding Initiative (2024a).↩︎\nThe term literate programming usually refers to a programming paradigm introduced by Donald E. Knuth. It describes an approach where programming is done in a human readable style at first. See Knuth (1992).↩︎\nThe ODD-format is used in various contexts, e.g. the German Textarchiv (DTA) uses ODD-files as a source for their TEI-subset DTABf. See Haaf and Thomas (2016).↩︎\nThis principles have been described in various books by many authors; one of the most famous is the book Clean code by Robert C. Martin (2009).↩︎\nThe source code of this pipeline as well as the ODD sources are open sourced and can be found in the corresponding GitHub-Repo as well as on Zenodo. See Politycki, Sonder, and Sutter (2024).↩︎\nThe starting point for the creation of ODD files is usually a tool called Roma. See Text Encoding Initiative (2024b).↩︎\nThe term TDD usually refers to Kent Beck, who reintroduced this idea in the early 2000s. It describes a programming paradigm where tests are written before the actual code. See Alsaqqa, Sawalha, and Abdel-Nabi (2020), p. 255.↩︎\nThese tests would normally be set up before the concrete description in the ODD-module is created, but we started with an already existing schema and decided to add the test later on.↩︎\nSee Christie (2024).↩︎\nSee Law Sources Foundation of the Swiss Lawyers Society (2024b).↩︎\nIt may sometimes be necessary to convert them with XSLT to be valid against the newer version of the schema.↩︎\nSee Preston-Werner (2023).↩︎",
    "crumbs": [
      "Abstracts",
      "On a solid ground. Building software for a 120-year-old research project applying modern engineering practices"
    ]
  },
  {
    "objectID": "submissions/462/index.html",
    "href": "submissions/462/index.html",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "",
    "text": "The role of real estate in a premodern urban economy is generally underrated due to a lack of specific research and the disparity of sources. While aspects such as the commodification of real estate and its role as collateral in credit contracts have been studied, combining different aspects from various sources into one perspective remains challenging. The Historical Land Register of Basel, initiated in 1895 and developed over several decades1, offers a potential solution. It involved thorough research in Basel’s city archives, creating approximately 120,000 file cards with almost verbatim transcriptions of sources, organized by house and chronologically sequenced. This comprehensive register combines information from various corpora, providing an unparalleled wealth of data for the era. To extract relevant information from these handwritten cards, we use machine learning methods such as named entity recognition (NER) and event extraction on texts generated by handwritten text recognition (HTR) of the scanned cards. As this is work in progress, we focus on specific aspects of real estate’s role in Basel’s economy, particularly interests. Medieval Basel had a dense network of religious institutions with seigneurial rights on many houses, additionally acting as lenders in annuities. Post-Reformation, these institutions’ records were well-preserved in the city archives, dominating the Historical Land Register. We contrast this with descriptions of houses in civil court records, which often list beneficiaries or note the absence of interests. We will then explore the frequency of interest mentions for houses in connection with seizure procedures in the civil court, typically due to non-payment of interests. When combining different sources, we must carefully select and interpret results, considering possible biases like gaps in tradition, changing writing habits, and the specific methods used by the Land Register’s creators. Additionally, machine learning errors can distort findings. By acknowledging these biases, we can draw more accurate conclusions about historical developments and changes.\nAfter a short introduction on methodology, the following research questions will be addressed in this extended abstract:\n\nWhat can we discover about the interest burden on real estate?\nDid a higher burden of interest lead to an increased number of seizure procedures?\nWho made use of seizure procedures and how does this use relate to interest claims?",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#introduction",
    "href": "submissions/462/index.html#introduction",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "",
    "text": "The role of real estate in a premodern urban economy is generally underrated due to a lack of specific research and the disparity of sources. While aspects such as the commodification of real estate and its role as collateral in credit contracts have been studied, combining different aspects from various sources into one perspective remains challenging. The Historical Land Register of Basel, initiated in 1895 and developed over several decades1, offers a potential solution. It involved thorough research in Basel’s city archives, creating approximately 120,000 file cards with almost verbatim transcriptions of sources, organized by house and chronologically sequenced. This comprehensive register combines information from various corpora, providing an unparalleled wealth of data for the era. To extract relevant information from these handwritten cards, we use machine learning methods such as named entity recognition (NER) and event extraction on texts generated by handwritten text recognition (HTR) of the scanned cards. As this is work in progress, we focus on specific aspects of real estate’s role in Basel’s economy, particularly interests. Medieval Basel had a dense network of religious institutions with seigneurial rights on many houses, additionally acting as lenders in annuities. Post-Reformation, these institutions’ records were well-preserved in the city archives, dominating the Historical Land Register. We contrast this with descriptions of houses in civil court records, which often list beneficiaries or note the absence of interests. We will then explore the frequency of interest mentions for houses in connection with seizure procedures in the civil court, typically due to non-payment of interests. When combining different sources, we must carefully select and interpret results, considering possible biases like gaps in tradition, changing writing habits, and the specific methods used by the Land Register’s creators. Additionally, machine learning errors can distort findings. By acknowledging these biases, we can draw more accurate conclusions about historical developments and changes.\nAfter a short introduction on methodology, the following research questions will be addressed in this extended abstract:\n\nWhat can we discover about the interest burden on real estate?\nDid a higher burden of interest lead to an increased number of seizure procedures?\nWho made use of seizure procedures and how does this use relate to interest claims?",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#information-extraction",
    "href": "submissions/462/index.html#information-extraction",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Information Extraction",
    "text": "Information Extraction\nTo facilitate the automated extraction of information, our team manually annotated 640 documents according to the BeNASch annotations guidelines.2 BeNASch is a nested annotation system which represents information about entities, relations and events. Annotation work was done using the INCEpTION platform (Klie et al. 2018).\n\nEntity Annotation\nEntities are classified into the categories PER (person), LOC (location), ORG (organization) or GPE (geo-political entity) and consist of an outer span and the head-element which marks the core. Entities are annotated in a nested manner, meaning another entity may be marked inside an entity. Entities may also contain Attributes and Descriptors, further describing them. The annotation of quantitative values, such as dates and money, is also part of the entity annotation process.\n\n\n\n\n\n\nFigure 1: Example annotation of a strongly simplified house description.\n\n\n\n\n\nEvent Annotation\nAn event is usually represented by a trigger phrase, one or multiple actors and objects (roles) and sometimes modifiers which add additional information (e.g. the information why an interest must be paid). For example, a document describing a seizure procedure will usually have a trigger phrase like “gefröhnt” and the roles of a claimant, the seized object and the reason for the seizure, an event describing the missed interests. Annotation may then look like the following example:\n&lt;claimant&gt; The administrator of the Klingenthal monastery &lt;/claimant&gt; has &lt;trigger&gt; seized &lt;/seized&gt; &lt;seized_obj&gt; Hans Müllers house at the Viehmarkt &lt;/seized_obj&gt; because of &lt;reason&gt; missed dues, as aforementioned monastery gets 3 fl. yearly interest from it &lt;/reason&gt;.\nEvents are not only linked to the entity annotations by their roles, but descriptors and entity mentions can also imply an event. For example, a descriptor with class dues also implies a dues/interest event.\n\n\nAutomated Extraction\nWe model both, entity and event extraction, in a two-step sequence tagging tasks. We utilize the FlairNLP framework (Akbik, Blythe, and Vollgraf 2018) to train our models, splitting our annotated data in an 80/10/10 split into training, validation and test data. For both steps we finetuned the german-language contextual character embedding model provided by the Flair framework (Akbik et al. 2019) using all available text from the Historical Land Records.\nIn the first step, we use a model to annotate all information that can be represented as spans of text which includes entity mentions, descriptors and values, but also event trigger phrases and modifiers. We use the process described in (Prada Ziegler 2024) to facilitate nested sequence tagging, but extend this method by adding a prefix and suffix to each sample to inform which annotation level we’re currently on. In the second step, we apply the role detection. For this task, we generate one sample for each trigger in our training set. The sample only includes the text of the immediate annotation level where the trigger is nested within. E.g. in the seizure example above, this would be the full document, as the trigger is found on document level, but the dues event (in the DESC.DUES) in the entity annotation example would only include the text of the descriptor. We again include a prefix and suffix to mark the annotation level and pretag the text by inserting the recognized spans from step 1 by putting a prefix and suffix around them. To ensure the system focuses the correct trigger during training and inference, we set the prefix and suffix annotation for all other triggers in the same sample to “INACTIVE”. This enables our system to learn which entities have roles corresponding to which trigger.3\nAs our annotated dataset is very small, the extraction performance for many classes cannot be properly evaluated. For the classes relevant to this study, we did additional evaluation based on the data outside our ground truth, scores will be reported whenever relevant in the following chapters.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#analysis",
    "href": "submissions/462/index.html#analysis",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Analysis",
    "text": "Analysis\n\nMeasuring interest burden on real estate\nThe obvious indicator to see what amount of interest a property is burdened with are description of interests. We may find these in different places: In the form of lists kept by different institutions which received interest, as part of the description of properties when they are mentioned in the documents, most often when a property is sold, and whenever an annuity is established. The lists of interest payments are problematic to use as they take the perspective of the beneficiary of the interest and do not mention if that property is also burdened with interest by other institutions or persons. The documents tracking lending of annuities would work, but their recognition by our automated system is still lacking. The descriptions of properties are thus a good choice, they are well detected by our automated annotation and contain information about all beneficiaries.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file\nfile_path = './data/3_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path)\n\n# Plot the 'Description of Dues' line\nplt.plot(data['decade'], data['interest descriptions as part of house descriptions'], label='interest descriptions as part of house descriptions')\n\n# Plot the 'Due-Events at Document Level' line\nplt.plot(data['decade'], data['document level interest events'], label='document level interest events')\n\n# Add labels and title\nplt.xlabel('Decade')\nplt.ylabel('Count')\nplt.legend()\n\n# Add grid for better readability\nplt.grid(True)\n\n# Save the plot as an image file\nplt.savefig(\"./images/fig_3.png\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Comparison between frequency of descriptions of interest as part of house descriptions and other contexts, such as institutional records recording received interest. Note that even though received interests remain steady, interest descriptions in house descriptions vanish with time. Results are bucketed by decade.\n\n\n\n\n\nWe consider three ways to measure the burden of interest. First, we could use the absolute number of descriptions, but this number is unreliable, as there is a trend to not describe the interest anymore in later documents (see Figure 2). Second, we could try to use the monetary values mentioned in the descriptions. Here we encounter multiple problems: Numbers suffer from more HTR errors than other parts of the documents and are thus less reliable in general. Additionally a number of different currencies are in use, which would need conversion to a single value, as well as payments in kind. Finally our automated system cannot differentiate between different reasons for interest at the moment, so we wouldn’t know if a value is paid per year or only in case of an exchange of property ownership (“zu erschatz”). We settled using the number of beneficiaries to determine the burden of rent. Any entity found in a house description is classified as a beneficiary (we evaluated this to be true in 98% of cases based on 100 samples, with the true false positives being caused by errors in the named entity recognition process). Figure 3 shows the absolute numbers of organizations and persons recognized as receivers of interest in house descriptions over time.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file\nfile_path = './data/3_4_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path)\n\n# Group by 'date' and 'entitytype' and count the number of occurrences\ngrouped_data = data.groupby(['date', 'entitytype']).size().unstack(fill_value=0)\n\n# Apply rolling window of 11 years (5 years before, the current year, and 5 years after)\nrolling_window = 11\ngrouped_data_rolling = grouped_data.rolling(window=rolling_window, min_periods=1, center=True).mean()\n\n# Plot the data\n#plt.figure(figsize=(10, 6))\n\n# Plot each entity type\nfor entity_type in ['per', 'org', 'keine']:\n    if entity_type in grouped_data_rolling.columns:\n        plt.plot(grouped_data_rolling.index, grouped_data_rolling[entity_type], label=entity_type if entity_type != \"keine\" else \"No entity\")\n\n# Add labels and title\nplt.xlabel('Year')\nplt.ylabel('Number of Entities')\nplt.legend(title='Entity Type')\n\n# Show the plot\nplt.grid(True)\nplt.savefig(\"./images/fig_4.png\")  # Save the plot as an image file\nplt.show()  # Display the plot\n\n\n\n\n\n\n\n\nFigure 3: Entities mentioned in descriptions of interests within house descriptions by class (5-year gliding window).\n\n\n\n\n\n\n\nHow seizures relate to burden of interest\nSince the 1420ies, seizure procedures obtained their own series in the city court records. These procedures could be applied in case of arrears in real-estate-related annuities or other interests, allowing the claimant to take the house into his possession after confirming his claim on three occasions. It meant no automatism of confiscation, but rather a gradually increased pressure on house owners to pay their due – actual transfer of ownership was much rarer than the high number of seizure procedures would suggest. Our model is very reliable when it comes to extracting seizure events. In a sample of 100 automatically identified seizures, none of them were false positives. We did not conduct a large enough evaluation to calculate how many seizures we miss currently, but from smaller checks, we’re confident this number is very low.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file while skipping the first row of headers\nfile_path = './data/4_5_viz.csv'  # Update this with the actual file path if necessary\ndata = pd.read_csv(file_path, skiprows=1, header=None)\n\n# Assign meaningful column names\ndata.columns = ['Decade_nice', 'Decade', 'no_of_seizures', 'no_of_docs_in_10_years', 'no_of_docs_with_desc_tax', 'no_of_entities_org_per', 'entities_desc_tax', 'entities_per_document', 'unused1', 'documents_per_decade', 'documents_with_desc_tax_per_decade', 'entities_per_decade', 'entities_desc_tax_2']\n\n# Plot the 'entities_desc_tax' line (column G, index 6)\nplt.plot(data['Decade'], data[\"entities_desc_tax\"], label='avg. entities in descriptions before seizure docs')\n\n# Plot the 'entities_desc_tax_2' line (column M, index 12)\nplt.plot(data['Decade'], data[\"entities_desc_tax_2\"], label='avg. entities in descriptions generally')\n\n# Add labels and title\nplt.xlabel('Decade')\nplt.legend()\n\n# Add grid for better readability\nplt.grid(True)\n\n# Save the plot as an image file\nplt.savefig(\"./images/fig_5.png\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Proportion of entities mentioned per interest description, comparison between houses in seizure procedures and all houses.\n\n\n\n\n\nSince seizure procedures were linked to interests, one could expect that interest descriptions would be more frequent in the years before the event. In fact, when comparing the 10 years before a seizure with all documents of the same decade, the relative number of entities mentioned in interest descriptions shows no difference. Thus, owing interests in itself was no reason for increased numbers of seizures. When looking at the number of entities mentioned in each interest description, one sees that in certain periods of time, this proportion was much higher for documents leading to seizure events than in the average document (see Figure 4). This is the case mainly for the time period between 1480 and 1540.\nIn the next part, we contrast the entities in the interest descriptions to the entities taking part in seizures as claimants. To distinguish organizations from persons as parties in seizure procedures, we must look closer at the claimants. Court records generally mention the representative who actually appeared in court. As actor in the event, this person is identified as claimant by our event recognition model. Thanks to our nested entity recognition, we can identify if there is an organizational affiliation with an accuracy of 83.4%. Most of the errors are based on the non-recognition of formulas like “innamen von” (representing) that should be possible to include in future algorithms. Figure 5 shows that surprisingly and in contrast to the interest descriptions presented above, persons were much more present as claimants than organizations, even if we account for a considerable proportion of unrecognized organizational affiliations. Thus, either there were people acting as claimants for institutions that are not recognisable in the text as such, or organizations made in fact fewer use of the seizure procedure. One possible explanation could be that seigneurial interests were generally very low, but accounted for a lot of organizations mentioned in interest descriptions. These low interests might not have presented a problem for house owners – or their non-payment did not justify going to court for the institutions. In order to clarify such questions, further research would be necessary.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file while skipping the first row of headers\nfile_path = './data/7_viz.csv'  # Update this with the actual file path if necessary\ndf = pd.read_csv(file_path)\n\nplt.plot(df['Decade'], df['none'], label='No claimant', color='green')\nplt.plot(df['Decade'], df['PER'], label='per', color='blue')\nplt.plot(df['Decade'], df['ORG'], label='org', color='orange')\n\n# Formatting\nplt.xlabel('Decade')\nplt.ylabel('Count')\nplt.legend(title='Type')\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Entity types of claimants.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#conclusions",
    "href": "submissions/462/index.html#conclusions",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Conclusions",
    "text": "Conclusions\nThe relationship between interest burden and seizures proves to be investigable for the first time with our data. However, it is a complex relationship. The probability of a seizure increases with rising interest burden only during certain periods when seizures were infrequent. The many institutional creditors are reflected in the interest descriptions but not in the number of seizures. Further investigations need to determine whether institutions were generally more patient, whether this was due to lower interest burden, or whether our models fail to adequately capture institutions. This requires unambiguous identification of institutions, which still needs to be carried out.\nOn a methodological level, it becomes clear that successful recognition of entities and events allows for a large number of research questions based on a combination of different aspects that are brought together in the Historical Land Register. However, the recognition of events is still a work in progress, due to the often small number of annotated events in the training material. It needs to be evaluated and developed based on specific samples. As this process progresses, the quality of the analyses will enable a long-term perspective on the economic activities involving Basel properties.",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#role-detection-column-format-example",
    "href": "submissions/462/index.html#role-detection-column-format-example",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Role Detection Column Format Example",
    "text": "Role Detection Column Format Example\n\n\n\n\n\n\n\n\n\nToken\nEntity Annotation\nRole Annotation\n\n\n\n\n[DESC]\nO\nO\n\n\n[B-DUE]\nB-tag:tr;t:due\nB-tag:tr;t:due\n\n\nzinst\nI-tag:tr;t:due\nO\n\n\n[E-DUE]\nI-tag:tr;t:due\nO\n\n\n[B-MONEY]\nB-tag:val;val:money\nB-tag:role;r:interest\n\n\n1sh\nI-tag:val;val:money\nO\n\n\n.\nI-tag:val;val:money\nO\n\n\n[E-MONEY]\nI-tag:val;val:money\nO\n\n\n[B-ORG]\nB-tag:ref;ent:org;sm:\nB-tag:role;r:beneficiary\n\n\nder\nI-tag:ref;ent:org;sm:\nO\n\n\nPresenz\nI-tag:ref;ent:org;sm:\nO\n\n\nuf\nI-tag:ref;ent:org;sm:\nO\n\n\nBurg\nI-tag:ref;ent:org;sm:\nO\n\n\n[E-ORG]\nI-tag:ref;ent:org;sm:\nO\n\n\n,\nO\nO\n\n\nsonstfrei\nO\nO\n\n\n[DESC]\nO\nO",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/462/index.html#footnotes",
    "href": "submissions/462/index.html#footnotes",
    "title": "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel’s historical land register, 1400-1700",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://dls.staatsarchiv.bs.ch/records/1016781.↩︎\nhttps://dhbern.github.io/BeNASch/↩︎\nSee the appendix for an example in column format.↩︎",
    "crumbs": [
      "Abstracts",
      "From record cards to the dynamics of real estate transactions: Working with automatically extracted information from Basel's historical land register, 1400-1700"
    ]
  },
  {
    "objectID": "submissions/447/index.html",
    "href": "submissions/447/index.html",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "",
    "text": "The movement of Open Science has grown in importance in the Humanities, advocating for better accessibility of scientific research, especially in the form of the publication of research data (UNESCO 2023). This has led funding agencies like SNSF, ANR, and Horizon Europe to ask research projects to publish their research data and metadata along the FAIR principles in public repositories (see for instance (ANR 2023; EC 2023; SNSF 2024). Such requirements are putting pressure on researchers, who need to learn and understand the principles and standards of FAIR data and its impact on research data, but also require them to acquire new methodologies and know-how, such as in data management and data science.\nAt the same time, this accessibility of an increasing volume of interoperable quality data and the new semantic methodologies might bring a change of paradigm in the Humanities by the way knowledge is produced (Beretta 2023; Feugère 2015). The utilization of Linked Open Data (LOD) grants scholars access to large volumes of interoperable and high-quality datasets, at a scale analogue methods cannot reach, fundamentally altering their approach to information. This enables scholars to pose novel research questions, marking a departure from traditional modes of inquiry and facilitating a broader range of analytical perspectives within academic discourse. Moreover, drawing upon semantic methodologies rooted in ontology engineering, scholars can effectively document the intricate complexities inherent of social and historical phenomena, enabling a nuanced representation essential to the Social Sciences and Humanities domains within their databases. This meticulous documentation not only reflects a sophisticated understanding of multifaceted realities but also empowers researchers to deepen the digital analysis of rich corpora.\nThe transition from analogical to digital research methodologies does not come without challenges for researchers, thus necessitating the development of new tools and research infrastructures to support them in this evolution. The demand arises for user-friendly tools that abstract the technical complexity, as well as project accompaniment organisations that can provide support in digital methodologies and strategies to help scholars to better manage their data for computational analysis and information sharing.\nThis is the goal of Geovistory. It is conceived as a virtual research and data publication environment designed to strengthen Open Research Data practices. Geovistory is developed for research projects in the Humanities and Social Sciences, whether in history, geography, literature or other related fields, according to the participatory method of “user experience design”. It supports researchers with simple and easy-to-use interfaces and allows them to make their research accessible in an attractive way to people interested in history.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#introduction",
    "href": "submissions/447/index.html#introduction",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "",
    "text": "The movement of Open Science has grown in importance in the Humanities, advocating for better accessibility of scientific research, especially in the form of the publication of research data (UNESCO 2023). This has led funding agencies like SNSF, ANR, and Horizon Europe to ask research projects to publish their research data and metadata along the FAIR principles in public repositories (see for instance (ANR 2023; EC 2023; SNSF 2024). Such requirements are putting pressure on researchers, who need to learn and understand the principles and standards of FAIR data and its impact on research data, but also require them to acquire new methodologies and know-how, such as in data management and data science.\nAt the same time, this accessibility of an increasing volume of interoperable quality data and the new semantic methodologies might bring a change of paradigm in the Humanities by the way knowledge is produced (Beretta 2023; Feugère 2015). The utilization of Linked Open Data (LOD) grants scholars access to large volumes of interoperable and high-quality datasets, at a scale analogue methods cannot reach, fundamentally altering their approach to information. This enables scholars to pose novel research questions, marking a departure from traditional modes of inquiry and facilitating a broader range of analytical perspectives within academic discourse. Moreover, drawing upon semantic methodologies rooted in ontology engineering, scholars can effectively document the intricate complexities inherent of social and historical phenomena, enabling a nuanced representation essential to the Social Sciences and Humanities domains within their databases. This meticulous documentation not only reflects a sophisticated understanding of multifaceted realities but also empowers researchers to deepen the digital analysis of rich corpora.\nThe transition from analogical to digital research methodologies does not come without challenges for researchers, thus necessitating the development of new tools and research infrastructures to support them in this evolution. The demand arises for user-friendly tools that abstract the technical complexity, as well as project accompaniment organisations that can provide support in digital methodologies and strategies to help scholars to better manage their data for computational analysis and information sharing.\nThis is the goal of Geovistory. It is conceived as a virtual research and data publication environment designed to strengthen Open Research Data practices. Geovistory is developed for research projects in the Humanities and Social Sciences, whether in history, geography, literature or other related fields, according to the participatory method of “user experience design”. It supports researchers with simple and easy-to-use interfaces and allows them to make their research accessible in an attractive way to people interested in history.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#geovistory-as-a-research-environment",
    "href": "submissions/447/index.html#geovistory-as-a-research-environment",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Geovistory as a Research Environment",
    "text": "Geovistory as a Research Environment\nGeovistory aims to be a comprehensive research environment that accompanies scholars throughout the whole research cycle. Geovistory includes: - The Geovistory Toolbox, which allows to manage and curate projects’ research data. The Toolbox is freely accessible for all individual projects. Each research project works on its own data perspective but at the same time directly contributes to a joint knowledge graph. - A joint Data repository that allows to connect and link the different research projects under a unique and modular ontology, thus creating a large Knowledge Graph. - The Geovistory Publication platform (http://geovistory.org), where data is published using the RDF framework and can be accessed via the community page or project-specific webpages and its graphical search tools or a SPARQL-endpoint. - An active Community to foster information and know-how exchange among the researchers, users and technological experts.\n\n\n\n\n\n\nFigure 1\n\n\n\nAs per current terms of service, all data produced in the information layer of Geovistory are licensed under creative commons BY-SA 4.0. Initiated by KleioLab GmbH, the different infrastructure components are currently being developed jointly by LARHRA and the University of Bern, while other actors are welcome to join the Geovistory vision.. All the web components and the publication platform have been made available as open source, as well as the toolbox. The LOD4HSS project (https://www.geovistory.org/lod4hss), co-funded by swissuniversities, structures these efforts and aims at creating a larger community of users and supporters of this vision.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#the-aim-of-breaking-information-silos",
    "href": "submissions/447/index.html#the-aim-of-breaking-information-silos",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "The aim of breaking information silos",
    "text": "The aim of breaking information silos\nThe goal of producing and publishing FAIR research data is to break the information silos that hinder the sharing and reusing of scientific data. However, achieving interoperability hinges on two critical components (Beretta 2024a): - Firstly, the unambiguous identification of real-world entities (e.g., persons, places, concepts) with unique identifiers (e.g., URIs in Linked Open Data) and the establishment of links between identical entities across different projects (e.g., ensuring that the entity “Paris” is identified by the same URI in all projects); - Secondly, the utilization of explicit ontologies that can be aligned across projects. Nevertheless, mapping between ontologies may prove challenging, or even unfeasible, particularly when divergent structural frameworks are employed (e.g., an event-centric ontology may have limited compatibility with an object-centric one).\nIn Geovistory, those challenges are addressed by producing a unique Knowledge Graph that integrates the various projects. This necessitates from each project the adherence to the Semantic Data for History and Social Sciences (SDHSS) ontology ecosystem. It includes a methodology of ontological foundational analysis, based on the principles of OntoClean, from the domain of semantic engineering (Guarino and Welty 2004), and the high-level conceptual categories of the DOLCE ontology (Borgo et al. 2022). This has been applied to the CIDOC CRM ontology, the ICOM standard for the Heritage domain, while extending it to include the social and mental realities crucial for documenting essential aspects of human history, like ownership, membership, collective beliefs, etc. (Beretta 2024b). On this basis, a standardised semantic methodology for the development of domain-oriented ontologies in different fields of the Humanities, such as archaeology, prosopography, and geography has been created.. The SDHSS ontology ecosystem provides adaptability to the specificities of the various research projects while ensuring full interoperability among them. It is collaboratively managed in the ontome.net (http://ontome.net) application, so that scholars and domain experts can participate in its development if interested.\nThis shared Knowledge Graph streamlines the entity creation process by enabling users to navigate the graph, identify existing objects, and reuse them in their project using the same URIs for entity identification. By leveraging a common ontology ecosystem, users can not only easily identify and reuse information pertaining to specific entities but also ensure seamless integration and interoperability across projects within the Geovistory platform.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#a-modular-system-for-managing-complex-hss-information",
    "href": "submissions/447/index.html#a-modular-system-for-managing-complex-hss-information",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "A modular system for managing complex HSS information",
    "text": "A modular system for managing complex HSS information\nScholars within the Humanities domain grapple with intricate information, significantly more complex when compared to other scientific disciplines. Historical sources, whether textual, oral, visual, or material, provide fragmented and biased glimpses into the past, necessitating contextualization and interpretation. Consequently, this dynamic can engender a considerable degree of information uncertainty and discordance that need to be meticulously documented. Any digital infrastructure or model employed must adeptly navigate this multifaceted information landscape and accommodate its inherent complexity.\nAn inherent strength of Geovistory lies in its handling of the challenges associated with scientific information in the Humanities and Social Sciences domain. Noteworthy among these challenges is the nuanced, context-sensitive nature of information and its relation with different research agendas, as well as the wide variations in meaning for the same terms and vocabulary complexities, competing views or gaps and fragmentation of available information. These complexities are deftly managed through the application of the SDHSS methodology, which tends to limit the number of classes and properties in the ontology ecosystem, while inviting projects to develop and share rich collections of controlled vocabularies of concepts that enrich the data model according to the different research agendas and perspectives.\nMoreover, the project-partition of the Knowledge Graph within Geovistory enables users to repurpose existing information while also accommodating contradictory data, particularly when discrepancies are identified by researchers. Each project graph is stored within a designated dataset, maintaining its individual identity within the overarching Knowledge Graph. This approach allows for the coexistence and contextualization of disparate interpretations of facts, enhancing the platform’s flexibility and adaptability to varying scholarly perspectives. It is the unique amalgamation of the Geovistory graph data model and its robust semantic enrichment capabilities that render it particularly compelling for research within the Humanities and Social Sciences.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#integrating-the-dh-ecosystem",
    "href": "submissions/447/index.html#integrating-the-dh-ecosystem",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Integrating the DH ecosystem",
    "text": "Integrating the DH ecosystem\nOperating within the framework of Linked Open Data principles entails establishing connections with disparate datasets housed in various open and online repositories or Knowledge Graphs, culminating in the creation of an inclusive and interconnected Web of Data—an accomplishment characterized as the fifth star of Tim Berners Lee’s Open Data (https://5stardata.info/en/). As datasets interlink, they collectively form the Linked Open Data Cloud (https://lod-cloud.net/), wherein predominant repositories such as Wikidata or DBpedia, alongside authority files such as VIAF or GND, assume pivotal roles as data hubs, enhancing the discoverability, contextualization, and citability of information.\nThe Geovistory ecosystem applies those principles, actively engaging with the Digital Humanities landscape. It is connected dynamically to the information systems of producers of authority records (such as IdRef, GND) and data repositories (such as Wikidata) in view of interconnecting bibliographic information systems and scale up to a large Knowledge Graph. Collaborative efforts include the establishment of a data exchange pipeline with the French Agence Bibliographique de l’Enseignement Supérieur (ABES), with ongoing initiatives to forge additional partnerships.\nMoreover, ensuring long-term preservation of research data remains imperative, with initiatives to archive completed projects in the Zenodo repository and explore potential collaborations with entities like DaSCH, OLOS, and Huma-Num for dynamic updates and data management, with preliminary engagements initiated with DaSCH.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/447/index.html#conclusions-and-future-perspectives",
    "href": "submissions/447/index.html#conclusions-and-future-perspectives",
    "title": "Geovistory, a LOD Research Infrastructure for Historical Sciences",
    "section": "Conclusions and future perspectives",
    "text": "Conclusions and future perspectives\nGeovistory has been designed as a comprehensive research environment tailored by and for historians and humanists to address their needs in generating and utilizing FAIR data, thereby streamlining the research digitization process. As the utilization of Geovistory proliferates across more projects, the Knowledge Graph grows with increasingly enriched information, rendering the overall environment more advantageous for scholars either by providing reusable datasets or by enriching imported data. In this regard, Geovistory can be compared as a Wikidata dedicated to research endeavors, with the difference that projects retain full control over their data without a loss of semantic coherence throughout the graph.\nThe forthcoming years mark a critical juncture for Geovistory, as the tools and infrastructures of the environment recently transitioned into the public domain. This needed change will ease collaboration with future public institutions within Europe, but a greater part of public fundings will be needed to ensure the sustainability of the ecosystem.\nNonetheless, the Digital Humanities ecosystem remains unstable, attributed to the lack of sustained funding for infrastructural initiatives by national funding agencies and the absence of cohesive coordination among institutions. To ameliorate this landscape, prioritizing the establishment of robust collaborations and partnerships among diverse tools and infrastructures in Switzerland and Europe is imperative. Leveraging the specialized expertise of each institution holds the promise of engendering a harmonized and synergistic, distributed environment conducive to scholarly pursuits.",
    "crumbs": [
      "Abstracts",
      "Geovistory, a LOD Research Infrastructure for Historical Sciences"
    ]
  },
  {
    "objectID": "submissions/457/index.html",
    "href": "submissions/457/index.html",
    "title": "Towards Computational Historiographical Modeling",
    "section": "",
    "text": "When we look for epistemological differences between “traditional” and digital history, corpora—stand out. Of course, historians have always created and studied collections of traces, in particular documents, but sometimes also other artifacts, and have built their narratives on the basis of these collections. This is a significant aspect of scholarship and in some sense constitutes the difference between historical and literary narratives: historical narratives are supposed to be grounded (in some way) in the historical facts represented by the respective corpus.\nNevertheless, the relation between such a corpus and the narrative is traditionally rather unclear. Not only is the corpus necessarily incomplete (and uncertain), but it’s typically only “virtual.” As Mayaffre (2006, 20) puts it, in the humanities corpora traditionally tend to be potentialities rather than realities: one could go and consult a certain document in some archive, but this may only be rarely done, and the corpus may thus have never been anything but an “intellectual object.” \nMachine-readable digital corpora—that is, what we mean by corpora today—have brought about major changes. Most of the time, it is their practical advantages that are highlighted: they are easier to store, they are (at least potentially) accessible from anywhere at any time, and they can be processed automatically. This, in turn, enables us to apply new types of analysis and thus to ask and study new research questions. What tends to be overlooked, though, is the epistemological impact of machine-readable corpora in history. The notion of corpus in digital history (and in digital humanities in general) is heavily influenced by the notion of corpus in computational linguistics: a large but finite collection of digital texts. Mayaffre (2006, 20) hints at the epistemological impact when he notes that, on the one hand, digitization dematerializes the text in that it is lifted from its previous support, but on the other hand, materializes the corpus more rigorously than before.\nThis is, of course, a precondition for more rigorous types of analysis, notably computational analyses, and—eventually—the construction of computational historical models. However, this raises a number of epistemological and methodological questions. In computational linguistics, a corpus is essentially considered a statistical sample of language. Historical corpora typically differ from linguistic corpora, both in its relation to the research objects, the research questions, and to the expected research findings. They also differ in the way they are constructed.\nWhile there is much discussion of individual methods and their appropriateness—and many common definitions as well as a large part of the criticism of DH are related to these methods—there is surprisingly little theoretical discussion of corpora. In a typical DH paper (or project proposal), just a few words are said about the corpus that was used, and most of it tends to concern its size and composition (n items of class X, m items of class Y, and so on) and the technical aspects of its construction (e.g., how it was scraped), if the authors did not use an existing corpus. The methods (algorithms, tools, etc.) used and the results achieved (and their interpretation and visualization) are typically discussed extensively, though.\nGiven the central role of corpora in digital history, I think we need to study them and the roles they play in order to avoid the production of research that is formally rigorous but historically meaningless (or even nonsensical).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#introduction",
    "href": "submissions/457/index.html#introduction",
    "title": "Towards Computational Historiographical Modeling",
    "section": "",
    "text": "When we look for epistemological differences between “traditional” and digital history, corpora—stand out. Of course, historians have always created and studied collections of traces, in particular documents, but sometimes also other artifacts, and have built their narratives on the basis of these collections. This is a significant aspect of scholarship and in some sense constitutes the difference between historical and literary narratives: historical narratives are supposed to be grounded (in some way) in the historical facts represented by the respective corpus.\nNevertheless, the relation between such a corpus and the narrative is traditionally rather unclear. Not only is the corpus necessarily incomplete (and uncertain), but it’s typically only “virtual.” As Mayaffre (2006, 20) puts it, in the humanities corpora traditionally tend to be potentialities rather than realities: one could go and consult a certain document in some archive, but this may only be rarely done, and the corpus may thus have never been anything but an “intellectual object.” \nMachine-readable digital corpora—that is, what we mean by corpora today—have brought about major changes. Most of the time, it is their practical advantages that are highlighted: they are easier to store, they are (at least potentially) accessible from anywhere at any time, and they can be processed automatically. This, in turn, enables us to apply new types of analysis and thus to ask and study new research questions. What tends to be overlooked, though, is the epistemological impact of machine-readable corpora in history. The notion of corpus in digital history (and in digital humanities in general) is heavily influenced by the notion of corpus in computational linguistics: a large but finite collection of digital texts. Mayaffre (2006, 20) hints at the epistemological impact when he notes that, on the one hand, digitization dematerializes the text in that it is lifted from its previous support, but on the other hand, materializes the corpus more rigorously than before.\nThis is, of course, a precondition for more rigorous types of analysis, notably computational analyses, and—eventually—the construction of computational historical models. However, this raises a number of epistemological and methodological questions. In computational linguistics, a corpus is essentially considered a statistical sample of language. Historical corpora typically differ from linguistic corpora, both in its relation to the research objects, the research questions, and to the expected research findings. They also differ in the way they are constructed.\nWhile there is much discussion of individual methods and their appropriateness—and many common definitions as well as a large part of the criticism of DH are related to these methods—there is surprisingly little theoretical discussion of corpora. In a typical DH paper (or project proposal), just a few words are said about the corpus that was used, and most of it tends to concern its size and composition (n items of class X, m items of class Y, and so on) and the technical aspects of its construction (e.g., how it was scraped), if the authors did not use an existing corpus. The methods (algorithms, tools, etc.) used and the results achieved (and their interpretation and visualization) are typically discussed extensively, though.\nGiven the central role of corpora in digital history, I think we need to study them and the roles they play in order to avoid the production of research that is formally rigorous but historically meaningless (or even nonsensical).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#corpora-as-models",
    "href": "submissions/457/index.html#corpora-as-models",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Corpora as Models",
    "text": "Corpora as Models\nAs Granger (1967) notes, the goal of any science (natural or other) is to build coherent and effective models of the phenomena they study.\nThus, and as I have argued before (Piotrowski 2019), a corpus should be considered a model in the sense of Leo Apostel, who asserted that “any subject using a system A that is neither directly nor indirectly interacting with a system B to obtain information about the system B, is using A as a model for B” (Apostel 1961, 36, emphasis in original). Creating a corpus thus means constructing a model, and modelers consequently have to answer questions such as: What is it that I am trying to model? In what respects is the model a reduction of it? And for whom and for what purpose am I creating the model?\nThese are not new questions: every time historians select sources, they construct models, even before any detailed analysis. However, machine-readable corpora are not only potentially much larger than any material collection of sources—which is already not inconsequential—but also have important epistemological consequences. The larger and the more “complete” a corpus is, the greater the danger to succumb to an “implicit essentialism” (Mothon 2010, 19) and to mistake the model for the original, a fallacy that can frequently be observed in the field of cultoromics (Michel et al. 2011), when arguments are being made on the basis of the Google Books Ngram Corpus.\nThe same then goes for any analysis of a corpus: if the corpus is “true,” so must be the results of the analysis; if there is no evidence of something in the corpus, it did not exist. This allure is even greater when the analysis is done automatically and in particular using opaque quantitative methods: as the computational analysis is assumed to be completely objective, there seems to be no reason to question the results—they merely need to be interpreted, which leads us to some kind of “digital positivism.” To rephrase Fustel de Coulanges (Monod 1889, 278), “Ne m’applaudissez pas, ce n’est pas moi qui vous parle ; ce sont les données qui parlent par mes courbes.”\nHowever, as Korzybski (1933, 58) famously remarked, “A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which accounts for its usefulness.” An analysis of a corpus will always yield results; the crucial question is whether these can tell us anything about the original phenomenon it aims to model. So, the crucial point is that corpora are not naturally occurring but intentionally constructed. A corpus is already a model and thus not epistemologically neutral. A good starting point for dealing with this seems to be Gaston Bachelard’s notion of phenomenotechnique (Bachelard 1968).",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#corpora-as-phenomenotechnical-devices",
    "href": "submissions/457/index.html#corpora-as-phenomenotechnical-devices",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Corpora as Phenomenotechnical Devices",
    "text": "Corpora as Phenomenotechnical Devices\nBachelard originally developed this notion, which treats scientific instruments as “materialized theories,” as a way to study the epistemology of modern physics, which goes far beyond what is directly observable. The humanities also and even primarily deal with phenomena that are not directly observable, but only through artifacts, in particular texts. They thus have also always constructed the objects of their studies through, for example, the categorization and selection of sources and the hermeneutic postulation and affirmation of phenomena.\nHowever, only the praxis has been codified to some extent as “best practices,” such as source criticism. Theories—or perhaps better: models and metamodels, as the term “theory” has a somewhat different meaning in the humanities than in the sciences—are not formalized and are only suggested by the (natural language) narrative. What history (and the humanities in general) traditionally do not have is something that corresponds to the scientific instrument.\nThis changes with digitalization and datafication: phenomena are now constructed and modeled through data and code, and (like in the sciences), the computational model takes on the role of the instrument and “sits in the center of the epistemic ensemble” (Rheinberger 2005, 320). Corpora are then, methodologically speaking, phenomenotechnical devices and form the basis and influence how we build, understand, and research higher-level concepts—which at the same time underly the construction of the corpus. In short: a corpus produces the phenomenon to be studied. As a model, it has Stachowiak’s three characteristics of models, the characteristic of mapping, the characteristic of shortening, and the characteristic of pragmatical model-function (Stachowiak 1973, 131–33). Note also that while a model does not have all properties of its corresponding original (the characteristic of shortening), it has abundant attributes (Stachowiak 1973, 155), i.e., attributes that are not present in the original.\nStatistics provide us with means to formally describe and analyze a specific subclass of models that are able to represent originals that have particular properties. However, the phenomena studied by the humanities generally do not have these properties, and we thus still lack adequate formal methods to describe them.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#conclusion",
    "href": "submissions/457/index.html#conclusion",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Conclusion",
    "text": "Conclusion\nI have tried to outline some of the background and the motivation for the project Towards Computational Historiographical Modeling: Corpora and Concepts, which is part of a larger research program.\nSo far, digital history (and digital humanities more generally) has largely contented itself with borrowing methods from other fields and has developed little methodology of its own. The focus on “methods and tools” represents a major obstacle towards the construction of computational models that could help us to obtain new insights into humanities research questions rather than just automate primarily quantitative processing—which is, without doubt, useful, but inherently limited, given that the research questions are ultimately qualitative.\nRegardless of the application domain, digital humanities research tends to rely heavily on corpora, i.e., curated collections of texts, images, music, or other types of data. However, both the epistemological foundations—the underlying concepts—and the epistemological implications have so far been largely ignored. I have proposed to consider corpora as phenomenotechnical devices (Bachelard 1968), like scientific instruments: corpora are, on the one hand, models of the phenomenon under study; on the other hand, the phenomenon is constructed through the corpus.\nWe therefore need to study corpora as models to answer questions such as: How do corpora model and produce phenomena? What are commonalities and differences between different types of corpora? How can corpora-as-models be formally described in order to take their properties into account for research that makes use of them?\nThe overall goal of the project is to contribute to theory formation in digital history and digital humanities, and to help us move from project-specific, often ad hoc, solutions to particular problems to a more general understanding of the issues at stake.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "submissions/457/index.html#acknowledgements",
    "href": "submissions/457/index.html#acknowledgements",
    "title": "Towards Computational Historiographical Modeling",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis research was supported by the Swiss National Science Foundation (SNSF) under grant no. 105211_204305.",
    "crumbs": [
      "Abstracts",
      "Towards Computational Historiographical Modeling"
    ]
  },
  {
    "objectID": "LICENSE-AGPL.html",
    "href": "LICENSE-AGPL.html",
    "title": "",
    "section": "",
    "text": "AboutLicense (Code) Code\n\n\n\n\n                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users’ Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others’ Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nRemote Network Interaction; Use with the GNU General Public License.\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0",
    "crumbs": [
      "Abstracts",
      "About",
      "License (Code)"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "AboutChangelog Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "",
    "section": "Unreleased",
    "text": "Unreleased\n\nFeatures\n\nInitial version",
    "crumbs": [
      "Abstracts",
      "About",
      "Changelog"
    ]
  },
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "",
    "section": "",
    "text": "AboutSecurity Code",
    "crumbs": [
      "Abstracts",
      "About",
      "Security"
    ]
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "",
    "section": "Reporting a Vulnerability",
    "text": "Reporting a Vulnerability\n\nTo report a security issue, please email digital-history-2024@unibas.ch with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Abstracts",
      "About",
      "Security"
    ]
  }
]